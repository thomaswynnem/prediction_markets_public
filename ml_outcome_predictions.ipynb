{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c7e931",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Predicting Market Outcomes: Utilizes Data Research and DataFrames to Cluster Data\n",
    "\n",
    "Utilized Traits:\n",
    "* Betting Data (Investment Amounts, Frequency, Wallets)\n",
    "* Outcome Data (Distribution, Winner)\n",
    "* Market Details (Category, Date, Time Length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827ed86",
   "metadata": {},
   "source": [
    "Open DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfca72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "marketOutcomes = pd.read_csv('data/silver/marketOutcomes.csv')\n",
    "markets = pd.read_csv('data/silver/markets_with_ai_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff24b425",
   "metadata": {},
   "source": [
    "Obtain Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a29224a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = markets['marketMakerAddress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9064c2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JCBJI6AJS9E71ZVH58E8IT9E8JZBCCRQEE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pprint\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "from datetime import datetime, timedelta\n",
    "POLYGON_API_KEY = os.getenv(\"POLYGONSCAN_API_KEY\")\n",
    "print(POLYGON_API_KEY)\n",
    "def time_decoder(address: str, iter: int = None, sz: str = None, hundredPlusBuys = None, lessThanHundredBuys = None): \n",
    "    dictForAddys = {}\n",
    "    if sz is not None:\n",
    "        if sz == 'large':\n",
    "            dictForAddys = hundredPlusBuys[iter]\n",
    "        elif sz == 'small':\n",
    "            dictForAddys = lessThanHundredBuys[iter]\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            dictForAddys = pd.read_csv(f'data/bronze/contract_buy_{address}.csv')\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "    try:\n",
    "        \n",
    "        people = [addy for addy in dictForAddys['buyer']]  \n",
    "        people = [a.lower() for a in people]\n",
    "        \n",
    "    except KeyError:\n",
    "        return None\n",
    "    url = (\n",
    "      f'https://api.polygonscan.com/api'\n",
    "      f'?module=account'\n",
    "      f'&action=tokentx'\n",
    "      f'&address={address}'\n",
    "      f'&startblock=0'\n",
    "      f'&endblock=99999999'\n",
    "      f'&apikey={POLYGON_API_KEY}'\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    transactionDict = response.json()\n",
    "    status = transactionDict.get(\"status\")\n",
    "    if status != '1':\n",
    "        print(f\"Error: {transactionDict.get('message')}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    timeDict = {}\n",
    "    for row in transactionDict.get('result', []):\n",
    "      \n",
    "      if row.get('from', '').lower() in people:\n",
    "            try:\n",
    "               timeDict[row.get('from','').lower()].append(row.get('timeStamp'))\n",
    "            except KeyError:\n",
    "               timeDict[row.get('from', '').lower()] = [row.get('timeStamp')]\n",
    "\n",
    "    from datetime import datetime\n",
    "    allTimes = [ts for ts_list in timeDict.values() for ts in ts_list]\n",
    "\n",
    "    \n",
    "    minTimestamp = min(datetime.fromtimestamp(int(x)) for x in allTimes)\n",
    "    for buyer, lst in timeDict.items():\n",
    "        lst_dt = sorted(datetime.fromtimestamp(int(x)) for x in lst)\n",
    "        timeDict[buyer] = [\n",
    "            int((ts - minTimestamp).total_seconds())   \n",
    "            for ts in lst_dt\n",
    "        ]\n",
    "    \n",
    "    buyers= dictForAddys['buyer'].str.lower()\n",
    "    dictForAddys['timeStampSinceFirst'] = [timeDict[addy].pop(0) if addy in timeDict and timeDict[addy] else None for addy in buyers]\n",
    "    dictForAddys.drop('timeStamp', axis=1, inplace=True)\n",
    "    dictForAddys.to_csv(f'data/bronze/contract_official_buys_{address}.csv')\n",
    "\n",
    "    return timeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fac2132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1 of 1138\n",
      "Turn 2 of 1138\n",
      "Turn 3 of 1138\n",
      "Turn 4 of 1138\n",
      "Turn 5 of 1138\n",
      "Turn 6 of 1138\n",
      "Turn 7 of 1138\n",
      "Turn 8 of 1138\n",
      "Turn 9 of 1138\n",
      "Turn 10 of 1138\n",
      "Turn 11 of 1138\n",
      "Turn 12 of 1138\n",
      "Turn 13 of 1138\n",
      "Turn 14 of 1138\n",
      "Turn 15 of 1138\n",
      "Turn 16 of 1138\n",
      "Turn 17 of 1138\n",
      "Turn 18 of 1138\n",
      "Turn 19 of 1138\n",
      "Turn 20 of 1138\n",
      "Turn 21 of 1138\n",
      "Turn 22 of 1138\n",
      "Turn 23 of 1138\n",
      "Turn 24 of 1138\n",
      "Turn 25 of 1138\n",
      "Turn 26 of 1138\n",
      "Turn 27 of 1138\n",
      "Turn 28 of 1138\n",
      "Turn 29 of 1138\n",
      "Turn 30 of 1138\n",
      "Turn 31 of 1138\n",
      "Turn 32 of 1138\n",
      "Turn 33 of 1138\n",
      "Turn 34 of 1138\n",
      "Turn 35 of 1138\n",
      "Turn 36 of 1138\n",
      "Turn 37 of 1138\n",
      "Turn 38 of 1138\n",
      "Turn 39 of 1138\n",
      "Turn 40 of 1138\n",
      "Turn 41 of 1138\n",
      "Turn 42 of 1138\n",
      "Turn 43 of 1138\n",
      "Turn 44 of 1138\n",
      "Turn 45 of 1138\n",
      "Turn 46 of 1138\n",
      "Turn 47 of 1138\n",
      "Turn 48 of 1138\n",
      "Turn 49 of 1138\n",
      "Turn 50 of 1138\n",
      "Turn 51 of 1138\n",
      "Turn 52 of 1138\n",
      "Turn 53 of 1138\n",
      "Turn 54 of 1138\n",
      "Turn 55 of 1138\n",
      "Turn 56 of 1138\n",
      "Turn 57 of 1138\n",
      "Turn 58 of 1138\n",
      "Turn 59 of 1138\n",
      "Turn 60 of 1138\n",
      "Turn 61 of 1138\n",
      "Turn 62 of 1138\n",
      "Turn 63 of 1138\n",
      "Turn 64 of 1138\n",
      "Turn 65 of 1138\n",
      "Turn 66 of 1138\n",
      "Turn 67 of 1138\n",
      "Turn 68 of 1138\n",
      "Turn 69 of 1138\n",
      "Turn 70 of 1138\n",
      "Turn 71 of 1138\n",
      "Turn 72 of 1138\n",
      "Turn 73 of 1138\n",
      "Turn 74 of 1138\n",
      "Turn 75 of 1138\n",
      "Turn 76 of 1138\n",
      "Turn 77 of 1138\n",
      "Turn 78 of 1138\n",
      "Turn 79 of 1138\n",
      "Turn 80 of 1138\n",
      "Turn 81 of 1138\n",
      "Turn 82 of 1138\n",
      "Turn 83 of 1138\n",
      "Turn 84 of 1138\n",
      "Turn 85 of 1138\n",
      "Turn 86 of 1138\n",
      "Turn 87 of 1138\n",
      "Turn 88 of 1138\n",
      "Turn 89 of 1138\n",
      "Turn 90 of 1138\n",
      "Turn 91 of 1138\n",
      "Turn 92 of 1138\n",
      "Turn 93 of 1138\n",
      "Turn 94 of 1138\n",
      "Turn 95 of 1138\n",
      "Turn 96 of 1138\n",
      "Turn 97 of 1138\n",
      "Turn 98 of 1138\n",
      "Turn 99 of 1138\n",
      "Turn 100 of 1138\n",
      "Turn 101 of 1138\n",
      "Turn 102 of 1138\n",
      "Turn 103 of 1138\n",
      "Turn 104 of 1138\n",
      "Turn 105 of 1138\n",
      "Turn 106 of 1138\n",
      "Turn 107 of 1138\n",
      "Turn 108 of 1138\n",
      "Turn 109 of 1138\n",
      "Turn 110 of 1138\n",
      "Turn 111 of 1138\n",
      "Turn 112 of 1138\n",
      "Turn 113 of 1138\n",
      "Turn 114 of 1138\n",
      "Turn 115 of 1138\n",
      "Turn 116 of 1138\n",
      "Turn 117 of 1138\n",
      "Turn 118 of 1138\n",
      "Turn 119 of 1138\n",
      "Turn 120 of 1138\n",
      "Turn 121 of 1138\n",
      "Turn 122 of 1138\n",
      "Turn 123 of 1138\n",
      "Turn 124 of 1138\n",
      "Turn 125 of 1138\n",
      "Turn 126 of 1138\n",
      "Turn 127 of 1138\n",
      "Turn 128 of 1138\n",
      "Turn 129 of 1138\n",
      "Turn 130 of 1138\n",
      "Turn 131 of 1138\n",
      "Turn 132 of 1138\n",
      "Turn 133 of 1138\n",
      "Turn 134 of 1138\n",
      "Turn 135 of 1138\n",
      "Turn 136 of 1138\n",
      "Turn 137 of 1138\n",
      "Turn 138 of 1138\n",
      "Turn 139 of 1138\n",
      "Turn 140 of 1138\n",
      "Turn 141 of 1138\n",
      "Turn 142 of 1138\n",
      "Turn 143 of 1138\n",
      "Turn 144 of 1138\n",
      "Turn 145 of 1138\n",
      "Turn 146 of 1138\n",
      "Turn 147 of 1138\n",
      "Turn 148 of 1138\n",
      "Turn 149 of 1138\n",
      "Turn 150 of 1138\n",
      "Turn 151 of 1138\n",
      "Turn 152 of 1138\n",
      "Turn 153 of 1138\n",
      "Turn 154 of 1138\n",
      "Turn 155 of 1138\n",
      "Turn 156 of 1138\n",
      "Turn 157 of 1138\n",
      "Turn 158 of 1138\n",
      "Turn 159 of 1138\n",
      "Turn 160 of 1138\n",
      "Turn 161 of 1138\n",
      "Turn 162 of 1138\n",
      "Turn 163 of 1138\n",
      "Turn 164 of 1138\n",
      "Turn 165 of 1138\n",
      "Turn 166 of 1138\n",
      "Turn 167 of 1138\n",
      "Turn 168 of 1138\n",
      "Turn 169 of 1138\n",
      "Turn 170 of 1138\n",
      "Turn 171 of 1138\n",
      "Turn 172 of 1138\n",
      "Turn 173 of 1138\n",
      "Turn 174 of 1138\n",
      "Turn 175 of 1138\n",
      "Turn 176 of 1138\n",
      "Turn 177 of 1138\n",
      "Turn 178 of 1138\n",
      "Turn 179 of 1138\n",
      "Turn 180 of 1138\n",
      "Turn 181 of 1138\n",
      "Turn 182 of 1138\n",
      "Turn 183 of 1138\n",
      "Turn 184 of 1138\n",
      "Turn 185 of 1138\n",
      "Turn 186 of 1138\n",
      "Turn 187 of 1138\n",
      "Turn 188 of 1138\n",
      "Turn 189 of 1138\n",
      "Turn 190 of 1138\n",
      "Turn 191 of 1138\n",
      "Turn 192 of 1138\n",
      "Turn 193 of 1138\n",
      "Turn 194 of 1138\n",
      "Turn 195 of 1138\n",
      "Turn 196 of 1138\n",
      "Turn 197 of 1138\n",
      "Turn 198 of 1138\n",
      "Turn 199 of 1138\n",
      "Turn 200 of 1138\n",
      "Turn 201 of 1138\n",
      "Turn 202 of 1138\n",
      "Turn 203 of 1138\n",
      "Turn 204 of 1138\n",
      "Turn 205 of 1138\n",
      "Turn 206 of 1138\n",
      "Turn 207 of 1138\n",
      "Turn 208 of 1138\n",
      "Turn 209 of 1138\n",
      "Turn 210 of 1138\n",
      "Turn 211 of 1138\n",
      "Turn 212 of 1138\n",
      "Turn 213 of 1138\n",
      "Turn 214 of 1138\n",
      "Turn 215 of 1138\n",
      "Turn 216 of 1138\n",
      "Turn 217 of 1138\n",
      "Turn 218 of 1138\n",
      "Turn 219 of 1138\n",
      "Turn 220 of 1138\n",
      "Turn 221 of 1138\n",
      "Turn 222 of 1138\n",
      "Turn 223 of 1138\n",
      "Turn 224 of 1138\n",
      "Turn 225 of 1138\n",
      "Turn 226 of 1138\n",
      "Turn 227 of 1138\n",
      "Turn 228 of 1138\n",
      "Turn 229 of 1138\n",
      "Turn 230 of 1138\n",
      "Turn 231 of 1138\n",
      "Turn 232 of 1138\n",
      "Turn 233 of 1138\n",
      "Turn 234 of 1138\n",
      "Turn 235 of 1138\n",
      "Turn 236 of 1138\n",
      "Turn 237 of 1138\n",
      "Turn 238 of 1138\n",
      "Turn 239 of 1138\n",
      "Turn 240 of 1138\n",
      "Turn 241 of 1138\n",
      "Turn 242 of 1138\n",
      "Turn 243 of 1138\n",
      "Turn 244 of 1138\n",
      "Turn 245 of 1138\n",
      "Turn 246 of 1138\n",
      "Turn 247 of 1138\n",
      "Turn 248 of 1138\n",
      "Turn 249 of 1138\n",
      "Turn 250 of 1138\n",
      "Turn 251 of 1138\n",
      "Turn 252 of 1138\n",
      "Turn 253 of 1138\n",
      "Turn 254 of 1138\n",
      "Turn 255 of 1138\n",
      "Turn 256 of 1138\n",
      "Turn 257 of 1138\n",
      "Turn 258 of 1138\n",
      "Turn 259 of 1138\n",
      "Turn 260 of 1138\n",
      "Turn 261 of 1138\n",
      "Turn 262 of 1138\n",
      "Turn 263 of 1138\n",
      "Turn 264 of 1138\n",
      "Turn 265 of 1138\n",
      "Turn 266 of 1138\n",
      "Turn 267 of 1138\n",
      "Turn 268 of 1138\n",
      "Turn 269 of 1138\n",
      "Turn 270 of 1138\n",
      "Turn 271 of 1138\n",
      "Turn 272 of 1138\n",
      "Turn 273 of 1138\n",
      "Turn 274 of 1138\n",
      "Turn 275 of 1138\n",
      "Turn 276 of 1138\n",
      "Turn 277 of 1138\n",
      "Turn 278 of 1138\n",
      "Turn 279 of 1138\n",
      "Turn 280 of 1138\n",
      "Turn 281 of 1138\n",
      "Turn 282 of 1138\n",
      "Turn 283 of 1138\n",
      "Turn 284 of 1138\n",
      "Turn 285 of 1138\n",
      "Turn 286 of 1138\n",
      "Turn 287 of 1138\n",
      "Turn 288 of 1138\n",
      "Turn 289 of 1138\n",
      "Turn 290 of 1138\n",
      "Turn 291 of 1138\n",
      "Turn 292 of 1138\n",
      "Turn 293 of 1138\n",
      "Turn 294 of 1138\n",
      "Turn 295 of 1138\n",
      "Turn 296 of 1138\n",
      "Turn 297 of 1138\n",
      "Turn 298 of 1138\n",
      "Turn 299 of 1138\n",
      "Turn 300 of 1138\n",
      "Turn 301 of 1138\n",
      "Turn 302 of 1138\n",
      "Turn 303 of 1138\n",
      "Turn 304 of 1138\n",
      "Turn 305 of 1138\n",
      "Turn 306 of 1138\n",
      "Turn 307 of 1138\n",
      "Turn 308 of 1138\n",
      "Turn 309 of 1138\n",
      "Turn 310 of 1138\n",
      "Turn 311 of 1138\n",
      "Turn 312 of 1138\n",
      "Turn 313 of 1138\n",
      "Turn 314 of 1138\n",
      "Turn 315 of 1138\n",
      "Turn 316 of 1138\n",
      "Turn 317 of 1138\n",
      "Turn 318 of 1138\n",
      "Turn 319 of 1138\n",
      "Turn 320 of 1138\n",
      "Turn 321 of 1138\n",
      "Turn 322 of 1138\n",
      "Turn 323 of 1138\n",
      "Turn 324 of 1138\n",
      "Turn 325 of 1138\n",
      "Turn 326 of 1138\n",
      "Turn 327 of 1138\n",
      "Turn 328 of 1138\n",
      "Turn 329 of 1138\n",
      "Turn 330 of 1138\n",
      "Turn 331 of 1138\n",
      "Turn 332 of 1138\n",
      "Turn 333 of 1138\n",
      "Turn 334 of 1138\n",
      "Turn 335 of 1138\n",
      "Turn 336 of 1138\n",
      "Turn 337 of 1138\n",
      "Turn 338 of 1138\n",
      "Turn 339 of 1138\n",
      "Turn 340 of 1138\n",
      "Turn 341 of 1138\n",
      "Turn 342 of 1138\n",
      "Turn 343 of 1138\n",
      "Turn 344 of 1138\n",
      "Turn 345 of 1138\n",
      "Turn 346 of 1138\n",
      "Turn 347 of 1138\n",
      "Turn 348 of 1138\n",
      "Turn 349 of 1138\n",
      "Turn 350 of 1138\n",
      "Turn 351 of 1138\n",
      "Turn 352 of 1138\n",
      "Turn 353 of 1138\n",
      "Turn 354 of 1138\n",
      "Turn 355 of 1138\n",
      "Turn 356 of 1138\n",
      "Turn 357 of 1138\n",
      "Turn 358 of 1138\n",
      "Turn 359 of 1138\n",
      "Turn 360 of 1138\n",
      "Turn 361 of 1138\n",
      "Turn 362 of 1138\n",
      "Turn 363 of 1138\n",
      "Turn 364 of 1138\n",
      "Turn 365 of 1138\n",
      "Turn 366 of 1138\n",
      "Turn 367 of 1138\n",
      "Turn 368 of 1138\n",
      "Turn 369 of 1138\n",
      "Turn 370 of 1138\n",
      "Turn 371 of 1138\n",
      "Turn 372 of 1138\n",
      "Turn 373 of 1138\n",
      "Turn 374 of 1138\n",
      "Turn 375 of 1138\n",
      "Turn 376 of 1138\n",
      "Turn 377 of 1138\n",
      "Turn 378 of 1138\n",
      "Turn 379 of 1138\n",
      "Turn 380 of 1138\n",
      "Turn 381 of 1138\n",
      "Turn 382 of 1138\n",
      "Turn 383 of 1138\n",
      "Turn 384 of 1138\n",
      "Turn 385 of 1138\n",
      "Turn 386 of 1138\n",
      "Turn 387 of 1138\n",
      "Turn 388 of 1138\n",
      "Turn 389 of 1138\n",
      "Turn 390 of 1138\n",
      "Turn 391 of 1138\n",
      "Turn 392 of 1138\n",
      "Turn 393 of 1138\n",
      "Turn 394 of 1138\n",
      "Turn 395 of 1138\n",
      "Turn 396 of 1138\n",
      "Turn 397 of 1138\n",
      "Turn 398 of 1138\n",
      "Turn 399 of 1138\n",
      "Turn 400 of 1138\n",
      "Turn 401 of 1138\n",
      "Turn 402 of 1138\n",
      "Turn 403 of 1138\n",
      "Turn 404 of 1138\n",
      "Turn 405 of 1138\n",
      "Turn 406 of 1138\n",
      "Turn 407 of 1138\n",
      "Turn 408 of 1138\n",
      "Turn 409 of 1138\n",
      "Turn 410 of 1138\n",
      "Turn 411 of 1138\n",
      "Turn 412 of 1138\n",
      "Turn 413 of 1138\n",
      "Turn 414 of 1138\n",
      "Turn 415 of 1138\n",
      "Turn 416 of 1138\n",
      "Turn 417 of 1138\n",
      "Turn 418 of 1138\n",
      "Turn 419 of 1138\n",
      "Turn 420 of 1138\n",
      "Turn 421 of 1138\n",
      "Turn 422 of 1138\n",
      "Turn 423 of 1138\n",
      "Turn 424 of 1138\n",
      "Turn 425 of 1138\n",
      "Turn 426 of 1138\n",
      "Turn 427 of 1138\n",
      "Turn 428 of 1138\n",
      "Turn 429 of 1138\n",
      "Turn 430 of 1138\n",
      "Turn 431 of 1138\n",
      "Turn 432 of 1138\n",
      "Turn 433 of 1138\n",
      "Turn 434 of 1138\n",
      "Turn 435 of 1138\n",
      "Turn 436 of 1138\n",
      "Turn 437 of 1138\n",
      "Turn 438 of 1138\n",
      "Turn 439 of 1138\n",
      "Turn 440 of 1138\n",
      "Turn 441 of 1138\n",
      "Turn 442 of 1138\n",
      "Turn 443 of 1138\n",
      "Turn 444 of 1138\n",
      "Turn 445 of 1138\n",
      "Turn 446 of 1138\n",
      "Turn 447 of 1138\n",
      "Turn 448 of 1138\n",
      "Turn 449 of 1138\n",
      "Turn 450 of 1138\n",
      "Turn 451 of 1138\n",
      "Turn 452 of 1138\n",
      "Turn 453 of 1138\n",
      "Turn 454 of 1138\n",
      "Turn 455 of 1138\n",
      "Turn 456 of 1138\n",
      "Turn 457 of 1138\n",
      "Turn 458 of 1138\n",
      "Turn 459 of 1138\n",
      "Turn 460 of 1138\n",
      "Turn 461 of 1138\n",
      "Turn 462 of 1138\n",
      "Turn 463 of 1138\n",
      "Turn 464 of 1138\n",
      "Turn 465 of 1138\n",
      "Turn 466 of 1138\n",
      "Turn 467 of 1138\n",
      "Turn 468 of 1138\n",
      "Turn 469 of 1138\n",
      "Turn 470 of 1138\n",
      "Turn 471 of 1138\n",
      "Turn 472 of 1138\n",
      "Turn 473 of 1138\n",
      "Turn 474 of 1138\n",
      "Turn 475 of 1138\n",
      "Turn 476 of 1138\n",
      "Turn 477 of 1138\n",
      "Turn 478 of 1138\n",
      "Turn 479 of 1138\n",
      "Turn 480 of 1138\n",
      "Turn 481 of 1138\n",
      "Turn 482 of 1138\n",
      "Turn 483 of 1138\n",
      "Turn 484 of 1138\n",
      "Turn 485 of 1138\n",
      "Turn 486 of 1138\n",
      "Turn 487 of 1138\n",
      "Turn 488 of 1138\n",
      "Turn 489 of 1138\n",
      "Turn 490 of 1138\n",
      "Turn 491 of 1138\n",
      "Turn 492 of 1138\n",
      "Turn 493 of 1138\n",
      "Turn 494 of 1138\n",
      "Turn 495 of 1138\n",
      "Turn 496 of 1138\n",
      "Turn 497 of 1138\n",
      "Turn 498 of 1138\n",
      "Turn 499 of 1138\n",
      "Turn 500 of 1138\n",
      "Turn 501 of 1138\n",
      "Turn 502 of 1138\n",
      "Turn 503 of 1138\n",
      "Turn 504 of 1138\n",
      "Turn 505 of 1138\n",
      "Turn 506 of 1138\n",
      "Turn 507 of 1138\n",
      "Turn 508 of 1138\n",
      "Turn 509 of 1138\n",
      "Turn 510 of 1138\n",
      "Turn 511 of 1138\n",
      "Turn 512 of 1138\n",
      "Turn 513 of 1138\n",
      "Turn 514 of 1138\n",
      "Turn 515 of 1138\n",
      "Turn 516 of 1138\n",
      "Turn 517 of 1138\n",
      "Turn 518 of 1138\n",
      "Turn 519 of 1138\n",
      "Turn 520 of 1138\n",
      "Turn 521 of 1138\n",
      "Turn 522 of 1138\n",
      "Turn 523 of 1138\n",
      "Turn 524 of 1138\n",
      "Turn 525 of 1138\n",
      "Turn 526 of 1138\n",
      "Turn 527 of 1138\n",
      "Turn 528 of 1138\n",
      "Turn 529 of 1138\n",
      "Turn 530 of 1138\n",
      "Turn 531 of 1138\n",
      "Turn 532 of 1138\n",
      "Turn 533 of 1138\n",
      "Turn 534 of 1138\n",
      "Turn 535 of 1138\n",
      "Turn 536 of 1138\n",
      "Turn 537 of 1138\n",
      "Turn 538 of 1138\n",
      "Turn 539 of 1138\n",
      "Turn 540 of 1138\n",
      "Turn 541 of 1138\n",
      "Turn 542 of 1138\n",
      "Turn 543 of 1138\n",
      "Turn 544 of 1138\n",
      "Turn 545 of 1138\n",
      "Turn 546 of 1138\n",
      "Turn 547 of 1138\n",
      "Turn 548 of 1138\n",
      "Turn 549 of 1138\n",
      "Turn 550 of 1138\n",
      "Turn 551 of 1138\n",
      "Turn 552 of 1138\n",
      "Turn 553 of 1138\n",
      "Turn 554 of 1138\n",
      "Turn 555 of 1138\n",
      "Turn 556 of 1138\n",
      "Turn 557 of 1138\n",
      "Turn 558 of 1138\n",
      "Turn 559 of 1138\n",
      "Turn 560 of 1138\n",
      "Turn 561 of 1138\n",
      "Turn 562 of 1138\n",
      "Turn 563 of 1138\n",
      "Turn 564 of 1138\n",
      "Turn 565 of 1138\n",
      "Turn 566 of 1138\n",
      "Turn 567 of 1138\n",
      "Turn 568 of 1138\n",
      "Turn 569 of 1138\n",
      "Turn 570 of 1138\n",
      "Turn 571 of 1138\n",
      "Turn 572 of 1138\n",
      "Turn 573 of 1138\n",
      "Turn 574 of 1138\n",
      "Turn 575 of 1138\n",
      "Turn 576 of 1138\n",
      "Turn 577 of 1138\n",
      "Turn 578 of 1138\n",
      "Turn 579 of 1138\n",
      "Turn 580 of 1138\n",
      "Turn 581 of 1138\n",
      "Turn 582 of 1138\n",
      "Turn 583 of 1138\n",
      "Turn 584 of 1138\n",
      "Turn 585 of 1138\n",
      "Turn 586 of 1138\n",
      "Turn 587 of 1138\n",
      "Turn 588 of 1138\n",
      "Turn 589 of 1138\n",
      "Turn 590 of 1138\n",
      "Turn 591 of 1138\n",
      "Turn 592 of 1138\n",
      "Turn 593 of 1138\n",
      "Turn 594 of 1138\n",
      "Turn 595 of 1138\n",
      "Turn 596 of 1138\n",
      "Turn 597 of 1138\n",
      "Turn 598 of 1138\n",
      "Turn 599 of 1138\n",
      "Turn 600 of 1138\n",
      "Turn 601 of 1138\n",
      "Turn 602 of 1138\n",
      "Turn 603 of 1138\n",
      "Turn 604 of 1138\n",
      "Turn 605 of 1138\n",
      "Turn 606 of 1138\n",
      "Turn 607 of 1138\n",
      "Turn 608 of 1138\n",
      "Turn 609 of 1138\n",
      "Turn 610 of 1138\n",
      "Turn 611 of 1138\n",
      "Turn 612 of 1138\n",
      "Turn 613 of 1138\n",
      "Turn 614 of 1138\n",
      "Turn 615 of 1138\n",
      "Turn 616 of 1138\n",
      "Turn 617 of 1138\n",
      "Turn 618 of 1138\n",
      "Turn 619 of 1138\n",
      "Turn 620 of 1138\n",
      "Turn 621 of 1138\n",
      "Turn 622 of 1138\n",
      "Turn 623 of 1138\n",
      "Turn 624 of 1138\n",
      "Turn 625 of 1138\n",
      "Turn 626 of 1138\n",
      "Turn 627 of 1138\n",
      "Turn 628 of 1138\n",
      "Turn 629 of 1138\n",
      "Turn 630 of 1138\n",
      "Turn 631 of 1138\n",
      "Turn 632 of 1138\n",
      "Turn 633 of 1138\n",
      "Turn 634 of 1138\n",
      "Turn 635 of 1138\n",
      "Turn 636 of 1138\n",
      "Turn 637 of 1138\n",
      "Turn 638 of 1138\n",
      "Turn 639 of 1138\n",
      "Turn 640 of 1138\n",
      "Turn 641 of 1138\n",
      "Turn 642 of 1138\n",
      "Turn 643 of 1138\n",
      "Turn 644 of 1138\n",
      "Turn 645 of 1138\n",
      "Turn 646 of 1138\n",
      "Turn 647 of 1138\n",
      "Turn 648 of 1138\n",
      "Turn 649 of 1138\n",
      "Turn 650 of 1138\n",
      "Turn 651 of 1138\n",
      "Turn 652 of 1138\n",
      "Turn 653 of 1138\n",
      "Turn 654 of 1138\n",
      "Turn 655 of 1138\n",
      "Turn 656 of 1138\n",
      "Turn 657 of 1138\n",
      "Turn 658 of 1138\n",
      "Turn 659 of 1138\n",
      "Turn 660 of 1138\n",
      "Turn 661 of 1138\n",
      "Turn 662 of 1138\n",
      "Turn 663 of 1138\n",
      "Turn 664 of 1138\n",
      "Turn 665 of 1138\n",
      "Turn 666 of 1138\n",
      "Turn 667 of 1138\n",
      "Turn 668 of 1138\n",
      "Turn 669 of 1138\n",
      "Turn 670 of 1138\n",
      "Turn 671 of 1138\n",
      "Turn 672 of 1138\n",
      "Turn 673 of 1138\n",
      "Turn 674 of 1138\n",
      "Turn 675 of 1138\n",
      "Turn 676 of 1138\n",
      "Turn 677 of 1138\n",
      "Turn 678 of 1138\n",
      "Turn 679 of 1138\n",
      "Turn 680 of 1138\n",
      "Turn 681 of 1138\n",
      "Turn 682 of 1138\n",
      "Turn 683 of 1138\n",
      "Turn 684 of 1138\n",
      "Turn 685 of 1138\n",
      "Turn 686 of 1138\n",
      "Turn 687 of 1138\n",
      "Turn 688 of 1138\n",
      "Turn 689 of 1138\n",
      "Turn 690 of 1138\n",
      "Turn 691 of 1138\n",
      "Turn 692 of 1138\n",
      "Turn 693 of 1138\n",
      "Turn 694 of 1138\n",
      "Turn 695 of 1138\n",
      "Turn 696 of 1138\n",
      "Turn 697 of 1138\n",
      "Turn 698 of 1138\n",
      "Turn 699 of 1138\n",
      "Turn 700 of 1138\n",
      "Turn 701 of 1138\n",
      "Turn 702 of 1138\n",
      "Turn 703 of 1138\n",
      "Turn 704 of 1138\n",
      "Turn 705 of 1138\n",
      "Turn 706 of 1138\n",
      "Turn 707 of 1138\n",
      "Turn 708 of 1138\n",
      "Turn 709 of 1138\n",
      "Turn 710 of 1138\n",
      "Turn 711 of 1138\n",
      "Turn 712 of 1138\n",
      "Turn 713 of 1138\n",
      "Turn 714 of 1138\n",
      "Turn 715 of 1138\n",
      "Turn 716 of 1138\n",
      "Turn 717 of 1138\n",
      "Turn 718 of 1138\n",
      "Turn 719 of 1138\n",
      "Turn 720 of 1138\n",
      "Turn 721 of 1138\n",
      "Turn 722 of 1138\n",
      "Turn 723 of 1138\n",
      "Turn 724 of 1138\n",
      "Turn 725 of 1138\n",
      "Turn 726 of 1138\n",
      "Turn 727 of 1138\n",
      "Turn 728 of 1138\n",
      "Turn 729 of 1138\n",
      "Turn 730 of 1138\n",
      "Turn 731 of 1138\n",
      "Turn 732 of 1138\n",
      "Turn 733 of 1138\n",
      "Turn 734 of 1138\n",
      "Turn 735 of 1138\n",
      "Turn 736 of 1138\n",
      "Turn 737 of 1138\n",
      "Turn 738 of 1138\n",
      "Turn 739 of 1138\n",
      "Turn 740 of 1138\n",
      "Turn 741 of 1138\n",
      "Turn 742 of 1138\n",
      "Turn 743 of 1138\n",
      "Turn 744 of 1138\n",
      "Turn 745 of 1138\n",
      "Turn 746 of 1138\n",
      "Turn 747 of 1138\n",
      "Turn 748 of 1138\n",
      "Turn 749 of 1138\n",
      "Turn 750 of 1138\n",
      "Turn 751 of 1138\n",
      "Turn 752 of 1138\n",
      "Turn 753 of 1138\n",
      "Turn 754 of 1138\n",
      "Turn 755 of 1138\n",
      "Turn 756 of 1138\n",
      "Turn 757 of 1138\n",
      "Turn 758 of 1138\n",
      "Turn 759 of 1138\n",
      "Turn 760 of 1138\n",
      "Turn 761 of 1138\n",
      "Turn 762 of 1138\n",
      "Turn 763 of 1138\n",
      "Turn 764 of 1138\n",
      "Turn 765 of 1138\n",
      "Turn 766 of 1138\n",
      "Turn 767 of 1138\n",
      "Turn 768 of 1138\n",
      "Turn 769 of 1138\n",
      "Turn 770 of 1138\n",
      "Turn 771 of 1138\n",
      "Turn 772 of 1138\n",
      "Turn 773 of 1138\n",
      "Turn 774 of 1138\n",
      "Turn 775 of 1138\n",
      "Turn 776 of 1138\n",
      "Turn 777 of 1138\n",
      "Turn 778 of 1138\n",
      "Turn 779 of 1138\n",
      "Turn 780 of 1138\n",
      "Turn 781 of 1138\n",
      "Turn 782 of 1138\n",
      "Turn 783 of 1138\n",
      "Turn 784 of 1138\n",
      "Turn 785 of 1138\n",
      "Turn 786 of 1138\n",
      "Turn 787 of 1138\n",
      "Turn 788 of 1138\n",
      "Turn 789 of 1138\n",
      "Turn 790 of 1138\n",
      "Turn 791 of 1138\n",
      "Turn 792 of 1138\n",
      "Turn 793 of 1138\n",
      "Turn 794 of 1138\n",
      "Turn 795 of 1138\n",
      "Turn 796 of 1138\n",
      "Turn 797 of 1138\n",
      "Turn 798 of 1138\n",
      "Turn 799 of 1138\n",
      "Turn 800 of 1138\n",
      "Turn 801 of 1138\n",
      "Turn 802 of 1138\n",
      "Turn 803 of 1138\n",
      "Turn 804 of 1138\n",
      "Turn 805 of 1138\n",
      "Turn 806 of 1138\n",
      "Turn 807 of 1138\n",
      "Turn 808 of 1138\n",
      "Turn 809 of 1138\n",
      "Turn 810 of 1138\n",
      "Turn 811 of 1138\n",
      "Turn 812 of 1138\n",
      "Turn 813 of 1138\n",
      "Turn 814 of 1138\n",
      "Turn 815 of 1138\n",
      "Turn 816 of 1138\n",
      "Turn 817 of 1138\n",
      "Turn 818 of 1138\n",
      "Turn 819 of 1138\n",
      "Turn 820 of 1138\n",
      "Turn 821 of 1138\n",
      "Turn 822 of 1138\n",
      "Turn 823 of 1138\n",
      "Turn 824 of 1138\n",
      "Turn 825 of 1138\n",
      "Turn 826 of 1138\n",
      "Turn 827 of 1138\n",
      "Turn 828 of 1138\n",
      "Turn 829 of 1138\n",
      "Turn 830 of 1138\n",
      "Turn 831 of 1138\n",
      "Turn 832 of 1138\n",
      "Turn 833 of 1138\n",
      "Turn 834 of 1138\n",
      "Turn 835 of 1138\n",
      "Turn 836 of 1138\n",
      "Turn 837 of 1138\n",
      "Turn 838 of 1138\n",
      "Turn 839 of 1138\n",
      "Turn 840 of 1138\n",
      "Turn 841 of 1138\n",
      "Turn 842 of 1138\n",
      "Turn 843 of 1138\n",
      "Turn 844 of 1138\n",
      "Turn 845 of 1138\n",
      "Turn 846 of 1138\n",
      "Turn 847 of 1138\n",
      "Turn 848 of 1138\n",
      "Turn 849 of 1138\n",
      "Turn 850 of 1138\n",
      "Turn 851 of 1138\n",
      "Turn 852 of 1138\n",
      "Turn 853 of 1138\n",
      "Turn 854 of 1138\n",
      "Turn 855 of 1138\n",
      "Turn 856 of 1138\n",
      "Turn 857 of 1138\n",
      "Turn 858 of 1138\n",
      "Turn 859 of 1138\n",
      "Turn 860 of 1138\n",
      "Turn 861 of 1138\n",
      "Turn 862 of 1138\n",
      "Turn 863 of 1138\n",
      "Turn 864 of 1138\n",
      "Turn 865 of 1138\n",
      "Turn 866 of 1138\n",
      "Turn 867 of 1138\n",
      "Turn 868 of 1138\n",
      "Turn 869 of 1138\n",
      "Turn 870 of 1138\n",
      "Turn 871 of 1138\n",
      "Turn 872 of 1138\n",
      "Turn 873 of 1138\n",
      "Turn 874 of 1138\n",
      "Turn 875 of 1138\n",
      "Turn 876 of 1138\n",
      "Turn 877 of 1138\n",
      "Turn 878 of 1138\n",
      "Turn 879 of 1138\n",
      "Turn 880 of 1138\n",
      "Turn 881 of 1138\n",
      "Turn 882 of 1138\n",
      "Turn 883 of 1138\n",
      "Turn 884 of 1138\n",
      "Turn 885 of 1138\n",
      "Turn 886 of 1138\n",
      "Turn 887 of 1138\n",
      "Turn 888 of 1138\n",
      "Turn 889 of 1138\n",
      "Turn 890 of 1138\n",
      "Turn 891 of 1138\n",
      "Turn 892 of 1138\n",
      "Turn 893 of 1138\n",
      "Turn 894 of 1138\n",
      "Turn 895 of 1138\n",
      "Turn 896 of 1138\n",
      "Turn 897 of 1138\n",
      "Turn 898 of 1138\n",
      "Turn 899 of 1138\n",
      "Turn 900 of 1138\n",
      "Turn 901 of 1138\n",
      "Turn 902 of 1138\n",
      "Turn 903 of 1138\n",
      "Turn 904 of 1138\n",
      "Turn 905 of 1138\n",
      "Turn 906 of 1138\n",
      "Turn 907 of 1138\n",
      "Turn 908 of 1138\n",
      "Turn 909 of 1138\n",
      "Turn 910 of 1138\n",
      "Turn 911 of 1138\n",
      "Turn 912 of 1138\n",
      "Turn 913 of 1138\n",
      "Turn 914 of 1138\n",
      "Turn 915 of 1138\n",
      "Turn 916 of 1138\n",
      "Turn 917 of 1138\n",
      "Turn 918 of 1138\n",
      "Turn 919 of 1138\n",
      "Turn 920 of 1138\n",
      "Turn 921 of 1138\n",
      "Turn 922 of 1138\n",
      "Turn 923 of 1138\n",
      "Turn 924 of 1138\n",
      "Turn 925 of 1138\n",
      "Turn 926 of 1138\n",
      "Turn 927 of 1138\n",
      "Turn 928 of 1138\n",
      "Turn 929 of 1138\n",
      "Turn 930 of 1138\n",
      "Turn 931 of 1138\n",
      "Turn 932 of 1138\n",
      "Turn 933 of 1138\n",
      "Turn 934 of 1138\n",
      "Turn 935 of 1138\n",
      "Turn 936 of 1138\n",
      "Turn 937 of 1138\n",
      "Turn 938 of 1138\n",
      "Turn 939 of 1138\n",
      "Turn 940 of 1138\n",
      "Turn 941 of 1138\n",
      "Turn 942 of 1138\n",
      "Turn 943 of 1138\n",
      "Turn 944 of 1138\n",
      "Turn 945 of 1138\n",
      "Turn 946 of 1138\n",
      "Turn 947 of 1138\n",
      "Turn 948 of 1138\n",
      "Turn 949 of 1138\n",
      "Turn 950 of 1138\n",
      "Turn 951 of 1138\n",
      "Turn 952 of 1138\n",
      "Turn 953 of 1138\n",
      "Turn 954 of 1138\n",
      "Turn 955 of 1138\n",
      "Turn 956 of 1138\n",
      "Turn 957 of 1138\n",
      "Turn 958 of 1138\n",
      "Turn 959 of 1138\n",
      "Turn 960 of 1138\n",
      "Turn 961 of 1138\n",
      "Turn 962 of 1138\n",
      "Turn 963 of 1138\n",
      "Turn 964 of 1138\n",
      "Turn 965 of 1138\n",
      "Turn 966 of 1138\n",
      "Turn 967 of 1138\n",
      "Turn 968 of 1138\n",
      "Turn 969 of 1138\n",
      "Turn 970 of 1138\n",
      "Turn 971 of 1138\n",
      "Turn 972 of 1138\n",
      "Turn 973 of 1138\n",
      "Turn 974 of 1138\n",
      "Turn 975 of 1138\n",
      "Turn 976 of 1138\n",
      "Turn 977 of 1138\n",
      "Turn 978 of 1138\n",
      "Turn 979 of 1138\n",
      "Turn 980 of 1138\n",
      "Turn 981 of 1138\n",
      "Turn 982 of 1138\n",
      "Turn 983 of 1138\n",
      "Turn 984 of 1138\n",
      "Turn 985 of 1138\n",
      "Turn 986 of 1138\n",
      "Turn 987 of 1138\n",
      "Turn 988 of 1138\n",
      "Turn 989 of 1138\n",
      "Turn 990 of 1138\n",
      "Turn 991 of 1138\n",
      "Turn 992 of 1138\n",
      "Turn 993 of 1138\n",
      "Turn 994 of 1138\n",
      "Turn 995 of 1138\n",
      "Turn 996 of 1138\n",
      "Turn 997 of 1138\n",
      "Turn 998 of 1138\n",
      "Turn 999 of 1138\n",
      "Turn 1000 of 1138\n",
      "Turn 1001 of 1138\n",
      "Turn 1002 of 1138\n",
      "Turn 1003 of 1138\n",
      "Turn 1004 of 1138\n",
      "Turn 1005 of 1138\n",
      "Turn 1006 of 1138\n",
      "Turn 1007 of 1138\n",
      "Turn 1008 of 1138\n",
      "Turn 1009 of 1138\n",
      "Turn 1010 of 1138\n",
      "Turn 1011 of 1138\n",
      "Turn 1012 of 1138\n",
      "Turn 1013 of 1138\n",
      "Turn 1014 of 1138\n",
      "Turn 1015 of 1138\n",
      "Turn 1016 of 1138\n",
      "Turn 1017 of 1138\n",
      "Turn 1018 of 1138\n",
      "Turn 1019 of 1138\n",
      "Turn 1020 of 1138\n",
      "Turn 1021 of 1138\n",
      "Turn 1022 of 1138\n",
      "Turn 1023 of 1138\n",
      "Turn 1024 of 1138\n",
      "Turn 1025 of 1138\n",
      "Turn 1026 of 1138\n",
      "Turn 1027 of 1138\n",
      "Turn 1028 of 1138\n",
      "Turn 1029 of 1138\n",
      "Turn 1030 of 1138\n",
      "Turn 1031 of 1138\n",
      "Turn 1032 of 1138\n",
      "Turn 1033 of 1138\n",
      "Turn 1034 of 1138\n",
      "Turn 1035 of 1138\n",
      "Turn 1036 of 1138\n",
      "Turn 1037 of 1138\n",
      "Turn 1038 of 1138\n",
      "Turn 1039 of 1138\n",
      "Turn 1040 of 1138\n",
      "Turn 1041 of 1138\n",
      "Turn 1042 of 1138\n",
      "Turn 1043 of 1138\n",
      "Turn 1044 of 1138\n",
      "Turn 1045 of 1138\n",
      "Turn 1046 of 1138\n",
      "Turn 1047 of 1138\n",
      "Turn 1048 of 1138\n",
      "Turn 1049 of 1138\n",
      "Turn 1050 of 1138\n",
      "Turn 1051 of 1138\n",
      "Turn 1052 of 1138\n",
      "Turn 1053 of 1138\n",
      "Turn 1054 of 1138\n",
      "Turn 1055 of 1138\n",
      "Turn 1056 of 1138\n",
      "Turn 1057 of 1138\n",
      "Turn 1058 of 1138\n",
      "Turn 1059 of 1138\n",
      "Turn 1060 of 1138\n",
      "Turn 1061 of 1138\n",
      "Turn 1062 of 1138\n",
      "Turn 1063 of 1138\n",
      "Turn 1064 of 1138\n",
      "Turn 1065 of 1138\n",
      "Turn 1066 of 1138\n",
      "Turn 1067 of 1138\n",
      "Turn 1068 of 1138\n",
      "Turn 1069 of 1138\n",
      "Turn 1070 of 1138\n",
      "Turn 1071 of 1138\n",
      "Turn 1072 of 1138\n",
      "Turn 1073 of 1138\n",
      "Turn 1074 of 1138\n",
      "Turn 1075 of 1138\n",
      "Turn 1076 of 1138\n",
      "Turn 1077 of 1138\n",
      "Turn 1078 of 1138\n",
      "Turn 1079 of 1138\n",
      "Turn 1080 of 1138\n",
      "Turn 1081 of 1138\n",
      "Turn 1082 of 1138\n",
      "Turn 1083 of 1138\n",
      "Turn 1084 of 1138\n",
      "Turn 1085 of 1138\n",
      "Turn 1086 of 1138\n",
      "Turn 1087 of 1138\n",
      "Turn 1088 of 1138\n",
      "Turn 1089 of 1138\n",
      "Turn 1090 of 1138\n",
      "Turn 1091 of 1138\n",
      "Turn 1092 of 1138\n",
      "Turn 1093 of 1138\n",
      "Turn 1094 of 1138\n",
      "Turn 1095 of 1138\n",
      "Turn 1096 of 1138\n",
      "Turn 1097 of 1138\n",
      "Turn 1098 of 1138\n",
      "Turn 1099 of 1138\n",
      "Turn 1100 of 1138\n",
      "Turn 1101 of 1138\n",
      "Turn 1102 of 1138\n",
      "Turn 1103 of 1138\n",
      "Turn 1104 of 1138\n",
      "Turn 1105 of 1138\n",
      "Turn 1106 of 1138\n",
      "Turn 1107 of 1138\n",
      "Turn 1108 of 1138\n",
      "Turn 1109 of 1138\n",
      "Turn 1110 of 1138\n",
      "Turn 1111 of 1138\n",
      "Turn 1112 of 1138\n",
      "Turn 1113 of 1138\n",
      "Turn 1114 of 1138\n",
      "Turn 1115 of 1138\n",
      "Turn 1116 of 1138\n",
      "Turn 1117 of 1138\n",
      "Turn 1118 of 1138\n",
      "Turn 1119 of 1138\n",
      "Turn 1120 of 1138\n",
      "Turn 1121 of 1138\n",
      "Turn 1122 of 1138\n",
      "Turn 1123 of 1138\n",
      "Turn 1124 of 1138\n",
      "Turn 1125 of 1138\n",
      "Turn 1126 of 1138\n",
      "Turn 1127 of 1138\n",
      "Turn 1128 of 1138\n",
      "Turn 1129 of 1138\n",
      "Turn 1130 of 1138\n",
      "Turn 1131 of 1138\n",
      "Turn 1132 of 1138\n",
      "Turn 1133 of 1138\n",
      "Turn 1134 of 1138\n",
      "Turn 1135 of 1138\n",
      "Turn 1136 of 1138\n",
      "Turn 1137 of 1138\n",
      "Turn 1138 of 1138\n"
     ]
    }
   ],
   "source": [
    "timerStamper = []\n",
    "turn = 0\n",
    "for index, market in markets.iterrows():\n",
    "    turn += 1\n",
    "    print(f\"Turn {turn} of {len(markets)}\")\n",
    "    try:\n",
    "        timeStamp  = time_decoder(market[\"marketMakerAddress\"])\n",
    "    except:\n",
    "        continue    \n",
    "    print(\"Yahoo News\")\n",
    "    try:\n",
    "        buyScans = pd.read_csv(f'data/bronze/contract_official_buys_{market[\"marketMakerAddress\"]}.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 1\")\n",
    "        continue\n",
    "    try:\n",
    "        buyScans75Index = max(buyScans['timeStampSinceFirst'].tolist())*.50\n",
    "        buyScans75 = buyScans[buyScans['timeStampSinceFirst'] < buyScans75Index]\n",
    "        \n",
    "    except:\n",
    "        print(f\"Error: Unicorn\")\n",
    "        continue\n",
    "    try:\n",
    "        filtered = marketOutcomes.loc[marketOutcomes[\"marketMakerAddress\"] == market[\"marketMakerAddress\"], \"index\"]\n",
    "        marketOutcomeIndex = filtered.iloc[0]\n",
    "    except:\n",
    "        print(f\"Error: 2\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        buysIndex0 = buyScans75[buyScans75['outcomeIndex'] == 0]['investmentAmount']\n",
    "        buysIndex1 = buyScans75[buyScans75['outcomeIndex'] == 1]['investmentAmount']\n",
    "    except:\n",
    "        print(f\"Error: 3\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        total = sum(buyScans75['investmentAmount'].tolist())\n",
    "        bigBets = buyScans75[buyScans75['investmentAmount'] > .05*total]\n",
    "        buyScansVC = bigBets['outcomeIndex'].value_counts()\n",
    "        whale0 = buyScansVC.get(0, 0)\n",
    "        whale1 = buyScansVC.get(1, 0)\n",
    "    except:\n",
    "        print(\"Error: 5\")\n",
    "        continue\n",
    "    \n",
    "    try: \n",
    "        sz = len(buyScans75)\n",
    "        finalRatioVC = buyScans75['outcomeIndex'].value_counts()\n",
    "        count0 = finalRatioVC.get(0, 0)\n",
    "        count1 = finalRatioVC.get(1, 0)\n",
    "        ratio = count0 / (count0 + count1)\n",
    "    except:\n",
    "        print(\"Error: 6\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        timerStamper.append({'Market': market['marketMakerAddress'], 'Category': market['category'], 'TimeStamps': timeStamp, 'BuysIndex0': buysIndex0.tolist(), 'BuysIndex1': buysIndex1.tolist(), 'OutcomeIndex': marketOutcomeIndex, 'Whale0': whale0, 'Whale1': whale1, 'FinalRatio': ratio})\n",
    "        print(\"Added\")\n",
    "    except:\n",
    "        print(\"Timer Failure\")\n",
    "        \n",
    "    print(\"Success\")\n",
    "timerStamper = pd.DataFrame(timerStamper)\n",
    "timerStamper.to_csv('data/silver/timerStamper2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c9fd1",
   "metadata": {},
   "source": [
    "Creating Official ML Model Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49174a78",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m timerStamper \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/silver/timerStamper1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m      5\u001b[0m badIndeces \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\PredMarkEnv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\PredMarkEnv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\PredMarkEnv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\PredMarkEnv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\PredMarkEnv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "timerStamper = pd.read_csv(\"data/silver/timerStamper2.csv\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "badIndeces = []\n",
    "encoder = LabelEncoder()\n",
    "timerStamper['CategoryEncoded'] = encoder.fit_transform(timerStamper['Category'])\n",
    "from datetime import timedelta\n",
    "import ast\n",
    "dataSector = []\n",
    "for iter, line in timerStamper.iterrows():\n",
    "    buysIndex0 = ast.literal_eval(line['BuysIndex0'])\n",
    "    buysIndex1 = ast.literal_eval(line['BuysIndex1'])\n",
    "    try:\n",
    "        timeDict = ast.literal_eval(line['TimeStamps'])\n",
    "        allDeltas = []\n",
    "        for deltas in timeDict.values():\n",
    "            allDeltas.extend(deltas)\n",
    "        maxDelta = max(allDeltas)\n",
    "    except:\n",
    "        badIndeces.append(iter)\n",
    "        print(\"Error: 7\")\n",
    "        continue\n",
    "    # Total length of time of the market\n",
    "    if None in [line['CategoryEncoded'], maxDelta , sum(buysIndex0), sum(buysIndex1), line['Whale0'], line['Whale1'], line['FinalRatio']] or (len(buysIndex0) + len(buysIndex1)) < 10:\n",
    "        badIndeces.append(iter)\n",
    "        continue\n",
    "    dataSector.append([[line['CategoryEncoded'], maxDelta ,len(buysIndex0), sum(buysIndex0), len(buysIndex1), sum(buysIndex1), line['Whale0'], line['Whale1'], line['FinalRatio']], line['OutcomeIndex']])\n",
    "    print(line['OutcomeIndex'])\n",
    "    \n",
    "timerStamper.drop(badIndeces, inplace=True)\n",
    "timerStamper.reset_index(drop=True, inplace=True)\n",
    "timerStamper.to_csv('data/silver/timerStamper2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c3758",
   "metadata": {},
   "source": [
    "Splitting Data into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_39572\\3047300000.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.features = torch.tensor(features, dtype=torch.float32)\n",
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_39572\\3047300000.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.float32) # Use float for binary labels\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32) # Use float for binary labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "totalData = len(dataSector)\n",
    "\n",
    "trainCap = totalData*4 // 5\n",
    "\n",
    "trainDataInputs = [x[0] for x in dataSector[:trainCap]]\n",
    "trainResults = [x[1] for x in dataSector[:trainCap]]\n",
    "\n",
    "testDataInputs = [x[0] for x in dataSector[trainCap:]]\n",
    "testResults = [x[1] for x in dataSector[trainCap:]]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = np.array(trainDataInputs)\n",
    "X_test = np.array(testDataInputs)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(trainResults, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(testResults, dtype=torch.float32)\n",
    "\n",
    "trainData = BinaryDataset(X_train_tensor, y_train_tensor)\n",
    "testData = BinaryDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 10\n",
    "shuffle = True\n",
    "dataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=shuffle)\n",
    "dataLoaderTest = DataLoader(testData, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd821c6e",
   "metadata": {},
   "source": [
    "Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fff352",
   "metadata": {},
   "source": [
    "Model Architecture taken from here : https://medium.com/data-science/pytorch-tabular-binary-classification-a0368da5bb89\n",
    "\n",
    "Key Point: Relu serves to fid complex relationships between variables\n",
    "Key Point: Normalizing to keep everything within 0 to 1 scope and not put too much weight on anything\n",
    "Key Point: Linear Layers Attempt to make reason out of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10096d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class MarketPredictor(nn.Module):\n",
    "    \"Initializes multi-class classification model\"\n",
    "    def __init__(self, input_features=9, output_features=1, hidden_units=16):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_features, hidden_units) \n",
    "        self.layer_2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.layer_out = nn.Linear(hidden_units, output_features) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_units)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_units)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "MarketPredictorModel = MarketPredictor(input_features=9,\n",
    "                    output_features=1,\n",
    "                    hidden_units=16)\n",
    "\n",
    "\n",
    "class MarketPredictorLinear(nn.Module):\n",
    "    \"Simplified linear model for binary classification\"\n",
    "    def __init__(self, input_features=9, output_features=1, hidden_units = 8):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_features, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units,output_features),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "MarketPredictorLinearModel = MarketPredictorLinear(input_features=9,\n",
    "                    output_features=1,\n",
    "                    hidden_units = 8)\n",
    "lossFn = nn.BCEWithLogitsLoss()\n",
    "optimizerReg = torch.optim.Adam(MarketPredictorModel.parameters(), lr=0.001)\n",
    "optimizerLin = torch.optim.Adam(MarketPredictorLinearModel.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0bf5d",
   "metadata": {},
   "source": [
    "A way to Analyze Accuracy (Literally just a percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c26015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryAccuracy(actualOutcomes, predProbs, threshold=0.5):\n",
    "    preds = (predProbs > threshold).float()\n",
    "    correct = (preds == actualOutcomes).float().sum()\n",
    "    acc = correct / actualOutcomes.shape[0]\n",
    "    return acc * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da43031",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7bce3",
   "metadata": {},
   "source": [
    "Both training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd84c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "'''\n",
    "def train(epochs: int):\n",
    "  rates = []\n",
    "  maxAcc = 0\n",
    "  for epoch in range(epochs):\n",
    "    MarketPredictorModel.train()\n",
    "    trainIndex = random.randint(736)\n",
    "    binaryPredictions = MarketPredictorModel(trainData[trainIndex]).squeeze()\n",
    "    loss = lossFn(binaryPredictions, trainResults)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    probs1 = torch.sigmoid(binaryPredictions)\n",
    "    accuracy = binaryAccuracy(trainResults, probs1)\n",
    "    MarketPredictorModel.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "      indexTest = random.randint(100)\n",
    "      testPredictions = MarketPredictorModel(testData[indexTest]).squeeze()\n",
    "      lossTest = lossFn(testPredictions, testResults[indexTest])\n",
    "      probs = torch.sigmoid(testPredictions)\n",
    "      accTest = binaryAccuracy(testResults, probs)\n",
    "      \n",
    "      \n",
    "      print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Acc: {accuracy:.2f}% | Test Loss: {lossTest:.5f} | Test Acc: {accTest:.4f}\")\n",
    "      maxAcc = max(maxAcc, accTest)\n",
    "      rates.append(accTest)\n",
    "      \n",
    "  import statistics\n",
    "  return maxAcc, statistics.mean([r.item() for r in rates])\n",
    "'''\n",
    "\n",
    "\n",
    "import statistics as stat\n",
    "def train_one_epoch(epoch_index, model, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "\n",
    "    for i, data in enumerate(dataLoader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = lossFn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        outputsSig = torch.sigmoid(outputs)\n",
    "\n",
    "        \n",
    "def evaluate_full_test_set(test_loader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs).squeeze()\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            mask = (preds >= 0.6) | (preds <= 0.4)\n",
    "            preds = preds[mask]\n",
    "            labels = labels[mask]\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                continue\n",
    "\n",
    "            preds = preds > 0.5\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100 * correct / total\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351432d2",
   "metadata": {},
   "source": [
    "Training Data to be 85% accuracte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee40ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 50\n",
      "Epoch 0 Accuracy: 85.09316770186335\n",
      "Epoch 1 of 50\n",
      "Epoch 1 Accuracy: 85.45454545454545\n",
      "Epoch 2 of 50\n",
      "Epoch 2 Accuracy: 84.33734939759036\n",
      "Epoch 3 of 50\n",
      "Epoch 3 Accuracy: 84.84848484848484\n",
      "Epoch 4 of 50\n",
      "Epoch 4 Accuracy: 82.94117647058823\n",
      "Epoch 5 of 50\n",
      "Epoch 5 Accuracy: 86.95652173913044\n",
      "Epoch 6 of 50\n",
      "Epoch 6 Accuracy: 85.44303797468355\n",
      "Epoch 7 of 50\n",
      "Epoch 7 Accuracy: 81.64556962025317\n",
      "Epoch 8 of 50\n",
      "Epoch 8 Accuracy: 85.09316770186335\n",
      "Epoch 9 of 50\n",
      "Epoch 9 Accuracy: 85.71428571428571\n",
      "Epoch 10 of 50\n",
      "Epoch 10 Accuracy: 84.93975903614458\n",
      "Epoch 11 of 50\n",
      "Epoch 11 Accuracy: 85.80246913580247\n",
      "Epoch 12 of 50\n",
      "Epoch 12 Accuracy: 83.13253012048193\n",
      "Epoch 13 of 50\n",
      "Epoch 13 Accuracy: 85.625\n",
      "Epoch 14 of 50\n",
      "Epoch 14 Accuracy: 86.62420382165605\n",
      "Epoch 15 of 50\n",
      "Epoch 15 Accuracy: 85.09316770186335\n",
      "Epoch 16 of 50\n",
      "Epoch 16 Accuracy: 85.71428571428571\n",
      "Epoch 17 of 50\n",
      "Epoch 17 Accuracy: 83.22981366459628\n",
      "Epoch 18 of 50\n",
      "Epoch 18 Accuracy: 84.71337579617834\n",
      "Epoch 19 of 50\n",
      "Epoch 19 Accuracy: 84.472049689441\n",
      "Epoch 20 of 50\n",
      "Epoch 20 Accuracy: 85.0\n",
      "Epoch 21 of 50\n",
      "Epoch 21 Accuracy: 84.04907975460122\n",
      "Epoch 22 of 50\n",
      "Epoch 22 Accuracy: 86.36363636363636\n",
      "Epoch 23 of 50\n",
      "Epoch 23 Accuracy: 84.4311377245509\n",
      "Epoch 24 of 50\n",
      "Epoch 24 Accuracy: 83.76623376623377\n",
      "Epoch 25 of 50\n",
      "Epoch 25 Accuracy: 84.61538461538461\n",
      "Epoch 26 of 50\n",
      "Epoch 26 Accuracy: 84.61538461538461\n",
      "Epoch 27 of 50\n",
      "Epoch 27 Accuracy: 84.61538461538461\n",
      "Epoch 28 of 50\n",
      "Epoch 28 Accuracy: 81.92771084337349\n",
      "Epoch 29 of 50\n",
      "Epoch 29 Accuracy: 83.75\n",
      "Epoch 30 of 50\n",
      "Epoch 30 Accuracy: 84.81012658227849\n",
      "Epoch 31 of 50\n",
      "Epoch 31 Accuracy: 85.0\n",
      "Epoch 32 of 50\n",
      "Epoch 32 Accuracy: 84.84848484848484\n",
      "Epoch 33 of 50\n",
      "Epoch 33 Accuracy: 85.06493506493507\n",
      "Epoch 34 of 50\n",
      "Epoch 34 Accuracy: 85.625\n",
      "Epoch 35 of 50\n",
      "Epoch 35 Accuracy: 85.18518518518519\n",
      "Epoch 36 of 50\n",
      "Epoch 36 Accuracy: 84.5679012345679\n",
      "Epoch 37 of 50\n",
      "Epoch 37 Accuracy: 87.5\n",
      "Epoch 38 of 50\n",
      "Epoch 38 Accuracy: 85.2760736196319\n",
      "Epoch 39 of 50\n",
      "Epoch 39 Accuracy: 84.07643312101911\n",
      "Epoch 40 of 50\n",
      "Epoch 40 Accuracy: 84.14634146341463\n",
      "Epoch 41 of 50\n",
      "Epoch 41 Accuracy: 84.5679012345679\n",
      "Epoch 42 of 50\n",
      "Epoch 42 Accuracy: 86.07594936708861\n",
      "Epoch 43 of 50\n",
      "Epoch 43 Accuracy: 84.52380952380952\n",
      "Epoch 44 of 50\n",
      "Epoch 44 Accuracy: 86.25\n",
      "Epoch 45 of 50\n",
      "Epoch 45 Accuracy: 82.5\n",
      "Epoch 46 of 50\n",
      "Epoch 46 Accuracy: 85.97560975609755\n",
      "Epoch 47 of 50\n",
      "Epoch 47 Accuracy: 81.81818181818181\n",
      "Epoch 48 of 50\n",
      "Epoch 48 Accuracy: 83.22981366459628\n",
      "Epoch 49 of 50\n",
      "Epoch 49 Accuracy: 80.24691358024691\n"
     ]
    }
   ],
   "source": [
    "maxAcc = 0\n",
    "mean = 0\n",
    "epochs =50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch} of {epochs}\")\n",
    "    train_one_epoch(epoch, MarketPredictorModel, optimizerReg)\n",
    "    avg  =evaluate_full_test_set(dataLoaderTest, MarketPredictorModel)\n",
    "    print(f\"Epoch {epoch} Accuracy: {avg}\")\n",
    "    \n",
    "torch.save(MarketPredictorModel.state_dict(), \"MarketPredictor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239df6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 250\n",
      "Epoch 0 Accuracy: 83.72093023255815\n",
      "Epoch 1 of 250\n",
      "Epoch 1 Accuracy: 83.81502890173411\n",
      "Epoch 2 of 250\n",
      "Epoch 2 Accuracy: 83.62573099415205\n",
      "Epoch 3 of 250\n",
      "Epoch 3 Accuracy: 83.33333333333333\n",
      "Epoch 4 of 250\n",
      "Epoch 4 Accuracy: 84.11764705882354\n",
      "Epoch 5 of 250\n",
      "Epoch 5 Accuracy: 84.11764705882354\n",
      "Epoch 6 of 250\n",
      "Epoch 6 Accuracy: 83.81502890173411\n",
      "Epoch 7 of 250\n",
      "Epoch 7 Accuracy: 83.42857142857143\n",
      "Epoch 8 of 250\n",
      "Epoch 8 Accuracy: 83.81502890173411\n",
      "Epoch 9 of 250\n",
      "Epoch 9 Accuracy: 83.42857142857143\n",
      "Epoch 10 of 250\n",
      "Epoch 10 Accuracy: 83.62573099415205\n",
      "Epoch 11 of 250\n",
      "Epoch 11 Accuracy: 82.95454545454545\n",
      "Epoch 12 of 250\n",
      "Epoch 12 Accuracy: 83.33333333333333\n",
      "Epoch 13 of 250\n",
      "Epoch 13 Accuracy: 83.42857142857143\n",
      "Epoch 14 of 250\n",
      "Epoch 14 Accuracy: 83.9080459770115\n",
      "Epoch 15 of 250\n",
      "Epoch 15 Accuracy: 82.95454545454545\n",
      "Epoch 16 of 250\n",
      "Epoch 16 Accuracy: 82.48587570621469\n",
      "Epoch 17 of 250\n",
      "Epoch 17 Accuracy: 83.72093023255815\n",
      "Epoch 18 of 250\n",
      "Epoch 18 Accuracy: 83.72093023255815\n",
      "Epoch 19 of 250\n",
      "Epoch 19 Accuracy: 83.33333333333333\n",
      "Epoch 20 of 250\n",
      "Epoch 20 Accuracy: 83.81502890173411\n",
      "Epoch 21 of 250\n",
      "Epoch 21 Accuracy: 83.72093023255815\n",
      "Epoch 22 of 250\n",
      "Epoch 22 Accuracy: 83.33333333333333\n",
      "Epoch 23 of 250\n",
      "Epoch 23 Accuracy: 83.72093023255815\n",
      "Epoch 24 of 250\n",
      "Epoch 24 Accuracy: 83.72093023255815\n",
      "Epoch 25 of 250\n",
      "Epoch 25 Accuracy: 83.81502890173411\n",
      "Epoch 26 of 250\n",
      "Epoch 26 Accuracy: 82.48587570621469\n",
      "Epoch 27 of 250\n",
      "Epoch 27 Accuracy: 83.33333333333333\n",
      "Epoch 28 of 250\n",
      "Epoch 28 Accuracy: 82.95454545454545\n",
      "Epoch 29 of 250\n",
      "Epoch 29 Accuracy: 83.42857142857143\n",
      "Epoch 30 of 250\n",
      "Epoch 30 Accuracy: 83.23699421965318\n",
      "Epoch 31 of 250\n",
      "Epoch 31 Accuracy: 83.33333333333333\n",
      "Epoch 32 of 250\n",
      "Epoch 32 Accuracy: 82.48587570621469\n",
      "Epoch 33 of 250\n",
      "Epoch 33 Accuracy: 83.33333333333333\n",
      "Epoch 34 of 250\n",
      "Epoch 34 Accuracy: 82.95454545454545\n",
      "Epoch 35 of 250\n",
      "Epoch 35 Accuracy: 83.33333333333333\n",
      "Epoch 36 of 250\n",
      "Epoch 36 Accuracy: 82.02247191011236\n",
      "Epoch 37 of 250\n",
      "Epoch 37 Accuracy: 82.85714285714286\n",
      "Epoch 38 of 250\n",
      "Epoch 38 Accuracy: 83.14606741573034\n",
      "Epoch 39 of 250\n",
      "Epoch 39 Accuracy: 83.33333333333333\n",
      "Epoch 40 of 250\n",
      "Epoch 40 Accuracy: 83.72093023255815\n",
      "Epoch 41 of 250\n",
      "Epoch 41 Accuracy: 84.30232558139535\n",
      "Epoch 42 of 250\n",
      "Epoch 42 Accuracy: 83.33333333333333\n",
      "Epoch 43 of 250\n",
      "Epoch 43 Accuracy: 83.33333333333333\n",
      "Epoch 44 of 250\n",
      "Epoch 44 Accuracy: 82.85714285714286\n",
      "Epoch 45 of 250\n",
      "Epoch 45 Accuracy: 82.95454545454545\n",
      "Epoch 46 of 250\n",
      "Epoch 46 Accuracy: 82.38636363636364\n",
      "Epoch 47 of 250\n",
      "Epoch 47 Accuracy: 83.81502890173411\n",
      "Epoch 48 of 250\n",
      "Epoch 48 Accuracy: 82.95454545454545\n",
      "Epoch 49 of 250\n",
      "Epoch 49 Accuracy: 82.95454545454545\n",
      "Epoch 50 of 250\n",
      "Epoch 50 Accuracy: 84.21052631578948\n",
      "Epoch 51 of 250\n",
      "Epoch 51 Accuracy: 83.81502890173411\n",
      "Epoch 52 of 250\n",
      "Epoch 52 Accuracy: 82.95454545454545\n",
      "Epoch 53 of 250\n",
      "Epoch 53 Accuracy: 83.33333333333333\n",
      "Epoch 54 of 250\n",
      "Epoch 54 Accuracy: 82.95454545454545\n",
      "Epoch 55 of 250\n",
      "Epoch 55 Accuracy: 82.38636363636364\n",
      "Epoch 56 of 250\n",
      "Epoch 56 Accuracy: 82.85714285714286\n",
      "Epoch 57 of 250\n",
      "Epoch 57 Accuracy: 82.48587570621469\n",
      "Epoch 58 of 250\n",
      "Epoch 58 Accuracy: 83.42857142857143\n",
      "Epoch 59 of 250\n",
      "Epoch 59 Accuracy: 82.48587570621469\n",
      "Epoch 60 of 250\n",
      "Epoch 60 Accuracy: 83.52272727272727\n",
      "Epoch 61 of 250\n",
      "Epoch 61 Accuracy: 83.33333333333333\n",
      "Epoch 62 of 250\n",
      "Epoch 62 Accuracy: 84.30232558139535\n",
      "Epoch 63 of 250\n",
      "Epoch 63 Accuracy: 83.33333333333333\n",
      "Epoch 64 of 250\n",
      "Epoch 64 Accuracy: 83.72093023255815\n",
      "Epoch 65 of 250\n",
      "Epoch 65 Accuracy: 83.05084745762711\n",
      "Epoch 66 of 250\n",
      "Epoch 66 Accuracy: 83.33333333333333\n",
      "Epoch 67 of 250\n",
      "Epoch 67 Accuracy: 83.72093023255815\n",
      "Epoch 68 of 250\n",
      "Epoch 68 Accuracy: 84.21052631578948\n",
      "Epoch 69 of 250\n",
      "Epoch 69 Accuracy: 83.81502890173411\n",
      "Epoch 70 of 250\n",
      "Epoch 70 Accuracy: 83.23699421965318\n",
      "Epoch 71 of 250\n",
      "Epoch 71 Accuracy: 83.05084745762711\n",
      "Epoch 72 of 250\n",
      "Epoch 72 Accuracy: 83.05084745762711\n",
      "Epoch 73 of 250\n",
      "Epoch 73 Accuracy: 84.21052631578948\n",
      "Epoch 74 of 250\n",
      "Epoch 74 Accuracy: 83.72093023255815\n",
      "Epoch 75 of 250\n",
      "Epoch 75 Accuracy: 83.33333333333333\n",
      "Epoch 76 of 250\n",
      "Epoch 76 Accuracy: 82.77777777777777\n",
      "Epoch 77 of 250\n",
      "Epoch 77 Accuracy: 83.72093023255815\n",
      "Epoch 78 of 250\n",
      "Epoch 78 Accuracy: 83.33333333333333\n",
      "Epoch 79 of 250\n",
      "Epoch 79 Accuracy: 84.21052631578948\n",
      "Epoch 80 of 250\n",
      "Epoch 80 Accuracy: 83.42857142857143\n",
      "Epoch 81 of 250\n",
      "Epoch 81 Accuracy: 83.14606741573034\n",
      "Epoch 82 of 250\n",
      "Epoch 82 Accuracy: 83.05084745762711\n",
      "Epoch 83 of 250\n",
      "Epoch 83 Accuracy: 83.72093023255815\n",
      "Epoch 84 of 250\n",
      "Epoch 84 Accuracy: 82.95454545454545\n",
      "Epoch 85 of 250\n",
      "Epoch 85 Accuracy: 83.52272727272727\n",
      "Epoch 86 of 250\n",
      "Epoch 86 Accuracy: 83.42857142857143\n",
      "Epoch 87 of 250\n",
      "Epoch 87 Accuracy: 82.85714285714286\n",
      "Epoch 88 of 250\n",
      "Epoch 88 Accuracy: 83.81502890173411\n",
      "Epoch 89 of 250\n",
      "Epoch 89 Accuracy: 83.81502890173411\n",
      "Epoch 90 of 250\n",
      "Epoch 90 Accuracy: 83.05084745762711\n",
      "Epoch 91 of 250\n",
      "Epoch 91 Accuracy: 82.77777777777777\n",
      "Epoch 92 of 250\n",
      "Epoch 92 Accuracy: 83.81502890173411\n",
      "Epoch 93 of 250\n",
      "Epoch 93 Accuracy: 83.42857142857143\n",
      "Epoch 94 of 250\n",
      "Epoch 94 Accuracy: 83.42857142857143\n",
      "Epoch 95 of 250\n",
      "Epoch 95 Accuracy: 83.81502890173411\n",
      "Epoch 96 of 250\n",
      "Epoch 96 Accuracy: 84.21052631578948\n",
      "Epoch 97 of 250\n",
      "Epoch 97 Accuracy: 83.14606741573034\n",
      "Epoch 98 of 250\n",
      "Epoch 98 Accuracy: 82.77777777777777\n",
      "Epoch 99 of 250\n",
      "Epoch 99 Accuracy: 83.05084745762711\n",
      "Epoch 100 of 250\n",
      "Epoch 100 Accuracy: 82.58426966292134\n",
      "Epoch 101 of 250\n",
      "Epoch 101 Accuracy: 83.52272727272727\n",
      "Epoch 102 of 250\n",
      "Epoch 102 Accuracy: 82.95454545454545\n",
      "Epoch 103 of 250\n",
      "Epoch 103 Accuracy: 83.05084745762711\n",
      "Epoch 104 of 250\n",
      "Epoch 104 Accuracy: 84.0\n",
      "Epoch 105 of 250\n",
      "Epoch 105 Accuracy: 82.75862068965517\n",
      "Epoch 106 of 250\n",
      "Epoch 106 Accuracy: 82.95454545454545\n",
      "Epoch 107 of 250\n",
      "Epoch 107 Accuracy: 83.05084745762711\n",
      "Epoch 108 of 250\n",
      "Epoch 108 Accuracy: 83.33333333333333\n",
      "Epoch 109 of 250\n",
      "Epoch 109 Accuracy: 83.52272727272727\n",
      "Epoch 110 of 250\n",
      "Epoch 110 Accuracy: 83.42857142857143\n",
      "Epoch 111 of 250\n",
      "Epoch 111 Accuracy: 83.05084745762711\n",
      "Epoch 112 of 250\n",
      "Epoch 112 Accuracy: 83.42857142857143\n",
      "Epoch 113 of 250\n",
      "Epoch 113 Accuracy: 83.42857142857143\n",
      "Epoch 114 of 250\n",
      "Epoch 114 Accuracy: 83.9080459770115\n",
      "Epoch 115 of 250\n",
      "Epoch 115 Accuracy: 84.0\n",
      "Epoch 116 of 250\n",
      "Epoch 116 Accuracy: 83.52272727272727\n",
      "Epoch 117 of 250\n",
      "Epoch 117 Accuracy: 83.05084745762711\n",
      "Epoch 118 of 250\n",
      "Epoch 118 Accuracy: 83.81502890173411\n",
      "Epoch 119 of 250\n",
      "Epoch 119 Accuracy: 82.95454545454545\n",
      "Epoch 120 of 250\n",
      "Epoch 120 Accuracy: 83.42857142857143\n",
      "Epoch 121 of 250\n",
      "Epoch 121 Accuracy: 83.42857142857143\n",
      "Epoch 122 of 250\n",
      "Epoch 122 Accuracy: 83.9080459770115\n",
      "Epoch 123 of 250\n",
      "Epoch 123 Accuracy: 83.81502890173411\n",
      "Epoch 124 of 250\n",
      "Epoch 124 Accuracy: 83.52272727272727\n",
      "Epoch 125 of 250\n",
      "Epoch 125 Accuracy: 84.0\n",
      "Epoch 126 of 250\n",
      "Epoch 126 Accuracy: 83.42857142857143\n",
      "Epoch 127 of 250\n",
      "Epoch 127 Accuracy: 83.05084745762711\n",
      "Epoch 128 of 250\n",
      "Epoch 128 Accuracy: 84.0\n",
      "Epoch 129 of 250\n",
      "Epoch 129 Accuracy: 84.0909090909091\n",
      "Epoch 130 of 250\n",
      "Epoch 130 Accuracy: 83.9080459770115\n",
      "Epoch 131 of 250\n",
      "Epoch 131 Accuracy: 84.0\n",
      "Epoch 132 of 250\n",
      "Epoch 132 Accuracy: 83.9080459770115\n",
      "Epoch 133 of 250\n",
      "Epoch 133 Accuracy: 83.9080459770115\n",
      "Epoch 134 of 250\n",
      "Epoch 134 Accuracy: 83.42857142857143\n",
      "Epoch 135 of 250\n",
      "Epoch 135 Accuracy: 82.95454545454545\n",
      "Epoch 136 of 250\n",
      "Epoch 136 Accuracy: 83.05084745762711\n",
      "Epoch 137 of 250\n",
      "Epoch 137 Accuracy: 82.95454545454545\n",
      "Epoch 138 of 250\n",
      "Epoch 138 Accuracy: 81.35593220338983\n",
      "Epoch 139 of 250\n",
      "Epoch 139 Accuracy: 82.95454545454545\n",
      "Epoch 140 of 250\n",
      "Epoch 140 Accuracy: 83.9080459770115\n",
      "Epoch 141 of 250\n",
      "Epoch 141 Accuracy: 82.48587570621469\n",
      "Epoch 142 of 250\n",
      "Epoch 142 Accuracy: 83.42857142857143\n",
      "Epoch 143 of 250\n",
      "Epoch 143 Accuracy: 82.95454545454545\n",
      "Epoch 144 of 250\n",
      "Epoch 144 Accuracy: 82.85714285714286\n",
      "Epoch 145 of 250\n",
      "Epoch 145 Accuracy: 84.0\n",
      "Epoch 146 of 250\n",
      "Epoch 146 Accuracy: 83.52272727272727\n",
      "Epoch 147 of 250\n",
      "Epoch 147 Accuracy: 82.58426966292134\n",
      "Epoch 148 of 250\n",
      "Epoch 148 Accuracy: 82.95454545454545\n",
      "Epoch 149 of 250\n",
      "Epoch 149 Accuracy: 84.0\n",
      "Epoch 150 of 250\n",
      "Epoch 150 Accuracy: 83.05084745762711\n",
      "Epoch 151 of 250\n",
      "Epoch 151 Accuracy: 82.48587570621469\n",
      "Epoch 152 of 250\n",
      "Epoch 152 Accuracy: 82.38636363636364\n",
      "Epoch 153 of 250\n",
      "Epoch 153 Accuracy: 83.42857142857143\n",
      "Epoch 154 of 250\n",
      "Epoch 154 Accuracy: 83.42857142857143\n",
      "Epoch 155 of 250\n",
      "Epoch 155 Accuracy: 83.05084745762711\n",
      "Epoch 156 of 250\n",
      "Epoch 156 Accuracy: 82.48587570621469\n",
      "Epoch 157 of 250\n",
      "Epoch 157 Accuracy: 83.9080459770115\n",
      "Epoch 158 of 250\n",
      "Epoch 158 Accuracy: 82.95454545454545\n",
      "Epoch 159 of 250\n",
      "Epoch 159 Accuracy: 83.52272727272727\n",
      "Epoch 160 of 250\n",
      "Epoch 160 Accuracy: 82.38636363636364\n",
      "Epoch 161 of 250\n",
      "Epoch 161 Accuracy: 83.9080459770115\n",
      "Epoch 162 of 250\n",
      "Epoch 162 Accuracy: 83.42857142857143\n",
      "Epoch 163 of 250\n",
      "Epoch 163 Accuracy: 83.42857142857143\n",
      "Epoch 164 of 250\n",
      "Epoch 164 Accuracy: 83.42857142857143\n",
      "Epoch 165 of 250\n",
      "Epoch 165 Accuracy: 83.42857142857143\n",
      "Epoch 166 of 250\n",
      "Epoch 166 Accuracy: 81.92090395480226\n",
      "Epoch 167 of 250\n",
      "Epoch 167 Accuracy: 82.95454545454545\n",
      "Epoch 168 of 250\n",
      "Epoch 168 Accuracy: 82.85714285714286\n",
      "Epoch 169 of 250\n",
      "Epoch 169 Accuracy: 83.42857142857143\n",
      "Epoch 170 of 250\n",
      "Epoch 170 Accuracy: 82.58426966292134\n",
      "Epoch 171 of 250\n",
      "Epoch 171 Accuracy: 83.42857142857143\n",
      "Epoch 172 of 250\n",
      "Epoch 172 Accuracy: 83.9080459770115\n",
      "Epoch 173 of 250\n",
      "Epoch 173 Accuracy: 83.42857142857143\n",
      "Epoch 174 of 250\n",
      "Epoch 174 Accuracy: 83.42857142857143\n",
      "Epoch 175 of 250\n",
      "Epoch 175 Accuracy: 83.42857142857143\n",
      "Epoch 176 of 250\n",
      "Epoch 176 Accuracy: 83.05084745762711\n",
      "Epoch 177 of 250\n",
      "Epoch 177 Accuracy: 83.42857142857143\n",
      "Epoch 178 of 250\n",
      "Epoch 178 Accuracy: 83.42857142857143\n",
      "Epoch 179 of 250\n",
      "Epoch 179 Accuracy: 82.02247191011236\n",
      "Epoch 180 of 250\n",
      "Epoch 180 Accuracy: 83.05084745762711\n",
      "Epoch 181 of 250\n",
      "Epoch 181 Accuracy: 82.38636363636364\n",
      "Epoch 182 of 250\n",
      "Epoch 182 Accuracy: 82.95454545454545\n",
      "Epoch 183 of 250\n",
      "Epoch 183 Accuracy: 83.42857142857143\n",
      "Epoch 184 of 250\n",
      "Epoch 184 Accuracy: 82.95454545454545\n",
      "Epoch 185 of 250\n",
      "Epoch 185 Accuracy: 83.52272727272727\n",
      "Epoch 186 of 250\n",
      "Epoch 186 Accuracy: 83.52272727272727\n",
      "Epoch 187 of 250\n",
      "Epoch 187 Accuracy: 82.58426966292134\n",
      "Epoch 188 of 250\n",
      "Epoch 188 Accuracy: 82.48587570621469\n",
      "Epoch 189 of 250\n",
      "Epoch 189 Accuracy: 83.9080459770115\n",
      "Epoch 190 of 250\n",
      "Epoch 190 Accuracy: 82.95454545454545\n",
      "Epoch 191 of 250\n",
      "Epoch 191 Accuracy: 82.95454545454545\n",
      "Epoch 192 of 250\n",
      "Epoch 192 Accuracy: 82.48587570621469\n",
      "Epoch 193 of 250\n",
      "Epoch 193 Accuracy: 83.9080459770115\n",
      "Epoch 194 of 250\n",
      "Epoch 194 Accuracy: 82.95454545454545\n",
      "Epoch 195 of 250\n",
      "Epoch 195 Accuracy: 83.05084745762711\n",
      "Epoch 196 of 250\n",
      "Epoch 196 Accuracy: 82.95454545454545\n",
      "Epoch 197 of 250\n",
      "Epoch 197 Accuracy: 81.35593220338983\n",
      "Epoch 198 of 250\n",
      "Epoch 198 Accuracy: 83.52272727272727\n",
      "Epoch 199 of 250\n",
      "Epoch 199 Accuracy: 83.05084745762711\n",
      "Epoch 200 of 250\n",
      "Epoch 200 Accuracy: 83.52272727272727\n",
      "Epoch 201 of 250\n",
      "Epoch 201 Accuracy: 82.58426966292134\n",
      "Epoch 202 of 250\n",
      "Epoch 202 Accuracy: 83.05084745762711\n",
      "Epoch 203 of 250\n",
      "Epoch 203 Accuracy: 82.48587570621469\n",
      "Epoch 204 of 250\n",
      "Epoch 204 Accuracy: 83.52272727272727\n",
      "Epoch 205 of 250\n",
      "Epoch 205 Accuracy: 82.48587570621469\n",
      "Epoch 206 of 250\n",
      "Epoch 206 Accuracy: 82.95454545454545\n",
      "Epoch 207 of 250\n",
      "Epoch 207 Accuracy: 82.02247191011236\n",
      "Epoch 208 of 250\n",
      "Epoch 208 Accuracy: 83.52272727272727\n",
      "Epoch 209 of 250\n",
      "Epoch 209 Accuracy: 82.95454545454545\n",
      "Epoch 210 of 250\n",
      "Epoch 210 Accuracy: 83.05084745762711\n",
      "Epoch 211 of 250\n",
      "Epoch 211 Accuracy: 82.95454545454545\n",
      "Epoch 212 of 250\n",
      "Epoch 212 Accuracy: 82.02247191011236\n",
      "Epoch 213 of 250\n",
      "Epoch 213 Accuracy: 82.95454545454545\n",
      "Epoch 214 of 250\n",
      "Epoch 214 Accuracy: 83.42857142857143\n",
      "Epoch 215 of 250\n",
      "Epoch 215 Accuracy: 82.48587570621469\n",
      "Epoch 216 of 250\n",
      "Epoch 216 Accuracy: 82.48587570621469\n",
      "Epoch 217 of 250\n",
      "Epoch 217 Accuracy: 82.02247191011236\n",
      "Epoch 218 of 250\n",
      "Epoch 218 Accuracy: 83.42857142857143\n",
      "Epoch 219 of 250\n",
      "Epoch 219 Accuracy: 83.42857142857143\n",
      "Epoch 220 of 250\n",
      "Epoch 220 Accuracy: 82.58426966292134\n",
      "Epoch 221 of 250\n",
      "Epoch 221 Accuracy: 82.95454545454545\n",
      "Epoch 222 of 250\n",
      "Epoch 222 Accuracy: 81.46067415730337\n",
      "Epoch 223 of 250\n",
      "Epoch 223 Accuracy: 82.48587570621469\n",
      "Epoch 224 of 250\n",
      "Epoch 224 Accuracy: 83.42857142857143\n",
      "Epoch 225 of 250\n",
      "Epoch 225 Accuracy: 82.58426966292134\n",
      "Epoch 226 of 250\n",
      "Epoch 226 Accuracy: 82.95454545454545\n",
      "Epoch 227 of 250\n",
      "Epoch 227 Accuracy: 83.33333333333333\n",
      "Epoch 228 of 250\n",
      "Epoch 228 Accuracy: 82.48587570621469\n",
      "Epoch 229 of 250\n",
      "Epoch 229 Accuracy: 81.81818181818181\n",
      "Epoch 230 of 250\n",
      "Epoch 230 Accuracy: 83.42857142857143\n",
      "Epoch 231 of 250\n",
      "Epoch 231 Accuracy: 82.95454545454545\n",
      "Epoch 232 of 250\n",
      "Epoch 232 Accuracy: 82.48587570621469\n",
      "Epoch 233 of 250\n",
      "Epoch 233 Accuracy: 82.02247191011236\n",
      "Epoch 234 of 250\n",
      "Epoch 234 Accuracy: 81.56424581005587\n",
      "Epoch 235 of 250\n",
      "Epoch 235 Accuracy: 82.95454545454545\n",
      "Epoch 236 of 250\n",
      "Epoch 236 Accuracy: 82.48587570621469\n",
      "Epoch 237 of 250\n",
      "Epoch 237 Accuracy: 83.42857142857143\n",
      "Epoch 238 of 250\n",
      "Epoch 238 Accuracy: 82.95454545454545\n",
      "Epoch 239 of 250\n",
      "Epoch 239 Accuracy: 82.38636363636364\n",
      "Epoch 240 of 250\n",
      "Epoch 240 Accuracy: 83.42857142857143\n",
      "Epoch 241 of 250\n",
      "Epoch 241 Accuracy: 82.02247191011236\n",
      "Epoch 242 of 250\n",
      "Epoch 242 Accuracy: 82.95454545454545\n",
      "Epoch 243 of 250\n",
      "Epoch 243 Accuracy: 82.48587570621469\n",
      "Epoch 244 of 250\n",
      "Epoch 244 Accuracy: 82.95454545454545\n",
      "Epoch 245 of 250\n",
      "Epoch 245 Accuracy: 82.95454545454545\n",
      "Epoch 246 of 250\n",
      "Epoch 246 Accuracy: 81.46067415730337\n",
      "Epoch 247 of 250\n",
      "Epoch 247 Accuracy: 82.48587570621469\n",
      "Epoch 248 of 250\n",
      "Epoch 248 Accuracy: 82.95454545454545\n",
      "Epoch 249 of 250\n",
      "Epoch 249 Accuracy: 82.48587570621469\n"
     ]
    }
   ],
   "source": [
    "maxAcc = 0\n",
    "mean = 0\n",
    "epochs =250\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch} of {epochs}\")\n",
    "    train_one_epoch(epoch,MarketPredictorLinearModel, optimizerLin)\n",
    "    avg  =evaluate_full_test_set(dataLoaderTest, MarketPredictorLinearModel)\n",
    "    print(f\"Epoch {epoch} Accuracy: {avg}\")\n",
    "    \n",
    "torch.save(MarketPredictorLinearModel.state_dict(), \"MarketPredictorLinear.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d8c91",
   "metadata": {},
   "source": [
    "maxAcc = 0\n",
    "mean = 0\n",
    "epochs =200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch} of {epochs}\")\n",
    "    train_one_epoch(epoch, None)\n",
    "    avg  =evaluate_full_test_set(dataLoaderTest)\n",
    "    print(f\"Epoch {epoch} Accuracy: {avg}\")\n",
    "    \n",
    "torch.save(MarketPredictorLinearModel.state_dict(), \"MarketPredictor.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb88c8e",
   "metadata": {},
   "source": [
    "Analyzing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: layer_1.weight | Shape: torch.Size([16, 9])\n",
      "tensor([[-0.1923,  0.3078,  0.2185, -0.0988,  0.1501, -0.0824, -0.1221,  0.0333,\n",
      "          0.2074],\n",
      "        [ 0.1415, -0.3362, -0.1522,  0.3083, -0.1600, -0.0953,  0.0395, -0.1482,\n",
      "          0.2699],\n",
      "        [-0.1917, -0.0101,  0.0960, -0.0212, -0.1038, -0.5314,  0.0663, -0.1873,\n",
      "         -0.2414],\n",
      "        [-0.1422, -0.3109, -0.0198,  0.0209, -0.0121,  0.0452, -0.0365, -0.0746,\n",
      "         -0.3496],\n",
      "        [ 0.1439, -0.0418, -0.0870,  0.4045, -0.3434, -0.1869,  0.3126, -0.1851,\n",
      "         -0.2007],\n",
      "        [-0.1773,  0.0789, -0.0155, -0.1804,  0.1447,  0.1824, -0.2092,  0.3803,\n",
      "          0.1395],\n",
      "        [ 0.1451, -0.0202, -0.2106,  0.0789,  0.2402, -0.4106, -0.3676,  0.0506,\n",
      "         -0.0495],\n",
      "        [ 0.2139, -0.0560,  0.0871, -0.7219, -0.2309, -0.0699, -0.1513,  0.0564,\n",
      "         -0.0535],\n",
      "        [ 0.2844, -0.4973, -0.0417,  0.0165,  0.0638,  0.1323,  0.1819,  0.1029,\n",
      "         -0.1318],\n",
      "        [ 0.2268, -0.1178,  0.4475, -0.4354, -0.0009,  0.1515, -0.3065, -0.3971,\n",
      "         -0.2650],\n",
      "        [-0.0336, -0.2559, -0.2170,  0.4044,  0.2062, -0.2185,  0.1357,  0.0738,\n",
      "          0.0464],\n",
      "        [ 0.2981, -0.2230,  0.0885,  0.1578,  0.4592,  0.0388, -0.0906, -0.1006,\n",
      "         -0.1946],\n",
      "        [ 0.0802,  0.1285,  0.0126, -0.0923, -0.1211, -0.1488, -0.0973,  0.0151,\n",
      "         -0.0801],\n",
      "        [-0.4319, -0.1775, -0.0620, -0.0631,  0.0711,  0.1546, -0.2758,  0.3533,\n",
      "         -0.1327],\n",
      "        [-0.3280, -0.0377,  0.0696, -0.4238, -0.3171,  0.0038,  0.1139,  0.1518,\n",
      "         -0.3243],\n",
      "        [-0.3590,  0.2564,  0.0454,  0.1381, -0.1208,  0.2240, -0.0182,  0.3020,\n",
      "          0.2005]])\n",
      "----------------------------------------\n",
      "Layer: layer_1.bias | Shape: torch.Size([16])\n",
      "tensor([ 0.0165,  0.1649, -0.0690,  0.1924, -0.1522,  0.0521,  0.1379,  0.0596,\n",
      "         0.1030,  0.1053,  0.2568,  0.2004, -0.4283,  0.1611, -0.3442, -0.0626])\n",
      "----------------------------------------\n",
      "Layer: layer_2.weight | Shape: torch.Size([16, 16])\n",
      "tensor([[ 5.0452e-02, -1.8597e-01,  1.3691e-01,  1.3377e-03,  1.7458e-01,\n",
      "         -4.2191e-01,  1.7434e-01,  7.5091e-02, -5.8627e-02,  1.2078e-02,\n",
      "         -6.9783e-02,  8.4674e-02,  1.6094e-01, -1.0329e-01, -1.4236e-01,\n",
      "         -6.8605e-02],\n",
      "        [-1.6643e-01, -1.2745e-02, -2.0128e-01,  2.0925e-01,  1.0715e-01,\n",
      "         -4.7937e-02, -3.5362e-02,  5.0311e-02, -2.2497e-01,  9.5690e-02,\n",
      "         -3.0508e-01, -2.1692e-01, -1.7298e-02, -3.7491e-02, -1.3470e-02,\n",
      "          2.2563e-01],\n",
      "        [-7.8413e-02, -1.3598e-01, -2.0633e-01,  4.5493e-02, -5.6280e-02,\n",
      "          7.7000e-02, -6.2524e-02,  5.8779e-02, -7.4683e-02, -1.9259e-01,\n",
      "         -2.8590e-01,  3.9224e-02,  8.4265e-02,  5.8955e-02, -8.5581e-02,\n",
      "         -2.9561e-01],\n",
      "        [ 5.4913e-02, -9.2207e-02,  2.5867e-02, -2.0608e-01,  6.6703e-02,\n",
      "          1.2853e-01, -3.9661e-01, -2.4951e-01, -1.1454e-02, -9.6110e-02,\n",
      "          2.2177e-01, -1.8775e-02,  2.7668e-01, -1.9015e-01,  8.1811e-02,\n",
      "          1.8036e-01],\n",
      "        [-1.1182e-01, -2.6787e-01,  2.5669e-01, -1.8790e-02, -8.3042e-02,\n",
      "         -4.4953e-02, -7.2230e-02, -4.5619e-01, -1.1384e-01,  5.2310e-03,\n",
      "         -1.1078e-01,  2.5750e-01, -7.4426e-03, -9.7099e-02, -5.5903e-02,\n",
      "         -1.4127e-02],\n",
      "        [ 2.2699e-03,  2.6371e-01,  2.3597e-02,  8.9375e-02,  1.0928e-01,\n",
      "          1.4976e-01,  6.1837e-03,  1.0638e-01, -1.8631e-01,  1.1913e-01,\n",
      "         -4.6479e-01, -2.2586e-02, -8.4049e-02,  6.8913e-02,  1.4382e-01,\n",
      "         -4.0694e-03],\n",
      "        [ 1.0813e-01, -1.1818e-01,  8.5358e-02,  1.0770e-02,  2.1091e-01,\n",
      "         -3.9411e-01,  9.5132e-02,  5.9291e-02, -1.7590e-01, -1.2559e-01,\n",
      "          1.0864e-01,  2.3385e-02, -4.7847e-02,  1.0909e-01, -2.0856e-01,\n",
      "          1.6238e-01],\n",
      "        [ 6.1144e-02,  5.4083e-03,  1.8971e-01, -2.6535e-01,  4.3470e-02,\n",
      "         -1.2432e-01,  1.4331e-01, -1.1896e-02,  3.3096e-01,  3.4577e-02,\n",
      "         -1.5206e-01, -4.5591e-02,  5.0098e-02, -1.1501e-01, -3.3902e-01,\n",
      "          7.0236e-02],\n",
      "        [ 3.4321e-02,  4.2934e-03,  6.3222e-02,  2.8903e-02, -5.7150e-01,\n",
      "         -1.3139e-02, -2.5669e-01,  3.1441e-01, -7.0666e-02,  1.9602e-02,\n",
      "         -5.7648e-02,  5.3305e-02, -3.1862e-02,  2.6485e-02,  2.2466e-02,\n",
      "         -6.7760e-02],\n",
      "        [ 8.7781e-02,  1.4056e-01, -4.9532e-01,  9.7629e-02, -1.8529e-01,\n",
      "          6.6168e-02, -4.5433e-02,  2.0162e-01, -8.3940e-02,  2.6380e-01,\n",
      "          1.3719e-01, -1.8774e-02,  5.5131e-02, -1.6616e-02, -1.0776e-02,\n",
      "         -4.6507e-04],\n",
      "        [ 2.2793e-01, -3.7952e-02,  1.9448e-01, -4.9705e-02, -2.4543e-01,\n",
      "          9.6068e-02,  7.5909e-05,  2.7205e-01,  8.0783e-02,  9.8876e-02,\n",
      "          1.5246e-01, -2.2627e-01, -1.6162e-01,  2.8148e-01,  4.5317e-02,\n",
      "         -9.7113e-02],\n",
      "        [ 8.8127e-02,  1.6590e-02,  1.7751e-01, -1.2907e-01,  2.4613e-02,\n",
      "         -8.6974e-02,  3.2625e-01, -4.5467e-02, -2.3585e-01,  1.0139e-01,\n",
      "          2.7292e-01, -9.5105e-02,  4.1547e-02, -1.8134e-02,  4.6812e-03,\n",
      "          7.8521e-02],\n",
      "        [ 1.8073e-01, -1.1074e-01, -4.6688e-01,  2.5427e-01, -7.9879e-02,\n",
      "          2.8084e-02, -8.4136e-02,  1.8805e-01,  4.1567e-03,  9.0554e-02,\n",
      "          1.3608e-01, -3.1007e-01, -6.1217e-02,  3.5966e-02, -8.6165e-02,\n",
      "         -4.9919e-02],\n",
      "        [-8.8459e-02,  2.2448e-01,  4.7948e-02, -1.5086e-01,  2.4374e-01,\n",
      "         -3.7085e-01, -7.0873e-02,  1.3964e-01,  1.6318e-01, -1.8352e-01,\n",
      "          8.9018e-02, -1.7991e-01, -1.7043e-01, -1.6717e-01,  2.0628e-01,\n",
      "         -3.9354e-02],\n",
      "        [ 1.2504e-01, -8.9803e-03, -2.4607e-01,  7.1897e-02,  5.6128e-02,\n",
      "          7.0676e-02,  7.0378e-02,  2.4800e-01, -3.6057e-02,  1.0065e-01,\n",
      "         -1.2498e-01,  1.2594e-01,  2.6026e-01,  2.3893e-01,  1.2588e-01,\n",
      "         -1.2671e-01],\n",
      "        [-8.6749e-02, -1.0025e-01,  3.1870e-05, -2.5001e-01, -4.8332e-02,\n",
      "         -4.7173e-01,  2.8498e-03,  1.1457e-02, -1.5551e-02,  1.3564e-01,\n",
      "          5.4789e-02,  1.7478e-01, -3.7809e-01, -8.9809e-02,  1.3001e-01,\n",
      "         -8.2958e-02]])\n",
      "----------------------------------------\n",
      "Layer: layer_2.bias | Shape: torch.Size([16])\n",
      "tensor([ 0.0714, -0.1392, -0.2775, -0.1715, -0.3551, -0.1873,  0.5299, -0.1784,\n",
      "         0.0316, -0.0460,  0.2361, -0.0071,  0.0940,  0.0093, -0.3178,  0.1317])\n",
      "----------------------------------------\n",
      "Layer: layer_out.weight | Shape: torch.Size([1, 16])\n",
      "tensor([[-0.2423,  0.2815,  0.1827, -0.1325, -0.2389,  0.2803, -0.2689, -0.2750,\n",
      "          0.3245,  0.2371,  0.2789, -0.2853,  0.2846, -0.1345,  0.2522, -0.1422]])\n",
      "----------------------------------------\n",
      "Layer: layer_out.bias | Shape: torch.Size([1])\n",
      "tensor([0.1877])\n",
      "----------------------------------------\n",
      "Layer: batchnorm1.weight | Shape: torch.Size([16])\n",
      "tensor([0.8907, 0.9398, 1.0940, 0.8519, 1.0913, 1.1307, 0.9678, 1.1542, 1.0185,\n",
      "        0.8614, 1.0681, 1.0593, 0.8920, 0.8816, 0.9185, 1.0214])\n",
      "----------------------------------------\n",
      "Layer: batchnorm1.bias | Shape: torch.Size([16])\n",
      "tensor([ 0.0193, -0.0689,  0.1581, -0.0019,  0.1635, -0.1608,  0.0078,  0.0524,\n",
      "        -0.0465,  0.0170,  0.1829, -0.0780, -0.1055, -0.0799, -0.0682,  0.0913])\n",
      "----------------------------------------\n",
      "Layer: batchnorm2.weight | Shape: torch.Size([16])\n",
      "tensor([0.9914, 1.0764, 0.9411, 0.9416, 1.2559, 1.2160, 1.1426, 1.1167, 1.3267,\n",
      "        1.0892, 1.0852, 1.0962, 1.2558, 0.9824, 1.1523, 1.0177])\n",
      "----------------------------------------\n",
      "Layer: batchnorm2.bias | Shape: torch.Size([16])\n",
      "tensor([-0.1294,  0.1387,  0.1309, -0.1183, -0.0673,  0.1137, -0.1158, -0.1520,\n",
      "         0.1731,  0.1471,  0.1156, -0.1487,  0.1857, -0.1249,  0.1399, -0.1321])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, param in MarketPredictorModel.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Shape: {param.shape}\")\n",
    "        print(param.data)\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132010cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 568 | Total: 699 | Accuracy: 81.25894134477825%\n"
     ]
    }
   ],
   "source": [
    "timerStamper = pd.read_csv(\"data/silver/timerStamper1.csv\")\n",
    "\n",
    "correct = 0\n",
    "total  =0\n",
    "\n",
    "for market in timerStamper['Market'].tolist():\n",
    "    try:\n",
    "        buyScans = pd.read_csv(f'data/bronze/contract_official_buys_{market}.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 1\")\n",
    "        continue\n",
    "\n",
    "    buyScans75Index = max(buyScans['timeStampSinceFirst'].tolist())*.50\n",
    "    buyScans75 = buyScans[buyScans['timeStampSinceFirst'] < buyScans75Index]\n",
    "\n",
    "\n",
    "    buysIndex0 = buyScans75[buyScans75['outcomeIndex'] == 0]['investmentAmount']\n",
    "    buysIndex1 = buyScans75[buyScans75['outcomeIndex'] == 1]['investmentAmount']\n",
    "\n",
    "    \n",
    "    prediction = sum(buysIndex0)/(sum(buysIndex0) + sum(buysIndex1))\n",
    "    if prediction > .4 and prediction < .6:\n",
    "        continue\n",
    "    prediction = 0 if prediction >= 0.5 else 1\n",
    "    outcomeIndex = timerStamper[timerStamper['Market'] == market]['OutcomeIndex'].tolist()[0]\n",
    "    \n",
    "    if prediction == outcomeIndex:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    \n",
    "print(f\"Correct: {correct} | Total: {total} | Accuracy: {correct/total*100}%\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix shape: (8, 9)\n",
      "Feature 0: Importance Score = 0.3743\n",
      "Feature 1: Importance Score = 0.2869\n",
      "Feature 2: Importance Score = 0.3565\n",
      "Feature 3: Importance Score = 0.4540\n",
      "Feature 4: Importance Score = 0.3791\n",
      "Feature 5: Importance Score = 0.4188\n",
      "Feature 6: Importance Score = 0.4286\n",
      "Feature 7: Importance Score = 0.3365\n",
      "Feature 8: Importance Score = 0.3569\n"
     ]
    }
   ],
   "source": [
    "# Assuming your model is called `MarketPredictorLinearModel`\n",
    "input_layer_weights = MarketPredictorLinearModel.model[0].weight.detach().numpy()\n",
    "\n",
    "# Shape: [hidden_units, input_features]\n",
    "print(\"Weight matrix shape:\", input_layer_weights.shape)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "feature_influence = np.mean(np.abs(input_layer_weights), axis=0)\n",
    "\n",
    "for i, influence in enumerate(feature_influence):\n",
    "    print(f\"Feature {i}: Importance Score = {influence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PredMarkEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

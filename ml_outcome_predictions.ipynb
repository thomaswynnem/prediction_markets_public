{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c7e931",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Predicting Market Outcomes: Utilizes Data Research and DataFrames to Cluster Data\n",
    "\n",
    "Utilized Traits:\n",
    "* Betting Data (Investment Amounts, Frequency, Wallets)\n",
    "* Outcome Data (Distribution, Winner)\n",
    "* Market Details (Category, Date, Time Length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827ed86",
   "metadata": {},
   "source": [
    "Open DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dfca72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "marketOutcomes = pd.read_csv('data/silver/marketOutcomes.csv')\n",
    "markets = pd.read_csv('data/silver/markets_with_ai_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff24b425",
   "metadata": {},
   "source": [
    "Obtain Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a29224a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = markets['marketMakerAddress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9064c2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JCBJI6AJS9E71ZVH58E8IT9E8JZBCCRQEE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pprint\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "from datetime import datetime, timedelta\n",
    "POLYGON_API_KEY = os.getenv(\"POLYGONSCAN_API_KEY\")\n",
    "print(POLYGON_API_KEY)\n",
    "def time_decoder(address: str, iter: int = None, sz: str = None, hundredPlusBuys = None, lessThanHundredBuys = None): \n",
    "    dictForAddys = {}\n",
    "    if sz is not None:\n",
    "        if sz == 'large':\n",
    "            dictForAddys = hundredPlusBuys[iter]\n",
    "        elif sz == 'small':\n",
    "            dictForAddys = lessThanHundredBuys[iter]\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            dictForAddys = pd.read_csv(f'data/bronze/contract_buy_{address}.csv')\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "    try:\n",
    "        \n",
    "        people = [addy for addy in dictForAddys['buyer']]  \n",
    "        people = [a.lower() for a in people]\n",
    "        \n",
    "    except KeyError:\n",
    "        return None\n",
    "    url = (\n",
    "      f'https://api.polygonscan.com/api'\n",
    "      f'?module=account'\n",
    "      f'&action=tokentx'\n",
    "      f'&address={address}'\n",
    "      f'&startblock=0'\n",
    "      f'&endblock=99999999'\n",
    "      f'&apikey={POLYGON_API_KEY}'\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    transactionDict = response.json()\n",
    "    status = transactionDict.get(\"status\")\n",
    "    if status != '1':\n",
    "        print(f\"Error: {transactionDict.get('message')}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    timeDict = {}\n",
    "    for row in transactionDict.get('result', []):\n",
    "      \n",
    "      if row.get('from', '').lower() in people:\n",
    "            try:\n",
    "               timeDict[row.get('from','').lower()].append(row.get('timeStamp'))\n",
    "            except KeyError:\n",
    "               timeDict[row.get('from', '').lower()] = [row.get('timeStamp')]\n",
    "\n",
    "    from datetime import datetime\n",
    "    allTimes = [ts for ts_list in timeDict.values() for ts in ts_list]\n",
    "\n",
    "    \n",
    "    minTimestamp = min(datetime.fromtimestamp(int(x)) for x in allTimes)\n",
    "    for buyer, lst in timeDict.items():\n",
    "        lst_dt = sorted(datetime.fromtimestamp(int(x)) for x in lst)\n",
    "        timeDict[buyer] = [\n",
    "            int((ts - minTimestamp).total_seconds())   \n",
    "            for ts in lst_dt\n",
    "        ]\n",
    "    \n",
    "    buyers= dictForAddys['buyer'].str.lower()\n",
    "    dictForAddys['timeStampSinceFirst'] = [timeDict[addy].pop(0) if addy in timeDict and timeDict[addy] else None for addy in buyers]\n",
    "    dictForAddys.drop('timeStamp', axis=1, inplace=True)\n",
    "    dictForAddys.to_csv(f'data/bronze/contract_official_buys_{address}.csv')\n",
    "\n",
    "    return timeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac2132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 2 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 3 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 4 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 5 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 6 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 7 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 8 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 9 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 10 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 11 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 12 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 13 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 14 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 15 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 16 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 17 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 18 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 19 of 1138\n",
      "Turn 20 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 21 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 22 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 23 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 24 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 25 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 26 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 27 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 28 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 29 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 30 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 31 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 32 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 33 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 34 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 35 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 36 of 1138\n",
      "Yahoo News\n",
      "Added\n",
      "Success\n",
      "Turn 37 of 1138\n"
     ]
    }
   ],
   "source": [
    "timerStamper = []\n",
    "turn = 0\n",
    "for index, market in markets.iterrows():\n",
    "    turn += 1\n",
    "    print(f\"Turn {turn} of {len(markets)}\")\n",
    "    try:\n",
    "        timeStamp  = time_decoder(market[\"marketMakerAddress\"])\n",
    "    except:\n",
    "        continue    \n",
    "    print(\"Yahoo News\")\n",
    "    try:\n",
    "        buyScans = pd.read_csv(f'data/bronze/contract_official_buys_{market[\"marketMakerAddress\"]}.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 1\")\n",
    "        continue\n",
    "    try:\n",
    "        buyScans75Index = max(buyScans['timeStampSinceFirst'].tolist())*.75\n",
    "        buyScans75 = buyScans[buyScans['timeStampSinceFirst'] < buyScans75Index]\n",
    "        \n",
    "    except:\n",
    "        print(f\"Error: Unicorn\")\n",
    "        continue\n",
    "    try:\n",
    "        filtered = marketOutcomes.loc[marketOutcomes[\"marketMakerAddress\"] == market[\"marketMakerAddress\"], \"index\"]\n",
    "        marketOutcomeIndex = filtered.iloc[0]\n",
    "    except:\n",
    "        print(f\"Error: 2\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        buysIndex0 = buyScans75[buyScans75['outcomeIndex'] == 0]['investmentAmount']\n",
    "        buysIndex1 = buyScans75[buyScans75['outcomeIndex'] == 1]['investmentAmount']\n",
    "    except:\n",
    "        print(f\"Error: 3\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        total = sum(buyScans75['investmentAmount'].tolist())\n",
    "        buyScans75[buyScans75['investmentAmount'] > .05*total]\n",
    "        buyScansVC = buyScans75['outcomeIndex'].value_counts()\n",
    "        whale0 = buyScansVC.get(0, 0)\n",
    "        whale1 = buyScansVC.get(1, 0)\n",
    "    except:\n",
    "        print(\"Error: 5\")\n",
    "        continue\n",
    "    \n",
    "    try: \n",
    "        sz = len(buyScans75)\n",
    "        finalRatioVC = buyScans75['outcomeIndex'].value_counts()\n",
    "        count0 = finalRatioVC.get(0, 0)\n",
    "        count1 = finalRatioVC.get(1, 0)\n",
    "        ratio = count0 / (count0 + count1)\n",
    "    except:\n",
    "        print(\"Error: 6\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        timerStamper.append({'Market': market['marketMakerAddress'], 'Category': market['category'], 'TimeStamps': timeStamp, 'BuysIndex0': buysIndex0.tolist(), 'BuysIndex1': buysIndex1.tolist(), 'OutcomeIndex': marketOutcomeIndex, 'Whale0': whale0, 'Whale1': whale1, 'FinalRatio': ratio})\n",
    "        print(\"Added\")\n",
    "    except:\n",
    "        print(\"Timer Failure\")\n",
    "        \n",
    "    print(\"Success\")\n",
    "timerStamper = pd.DataFrame(timerStamper)\n",
    "timerStamper.to_csv('data/silver/timerStamper1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c9fd1",
   "metadata": {},
   "source": [
    "Creating Official ML Model Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49174a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "timerStamper = pd.read_csv(\"data/silver/timerStamper1.csv\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "badIndeces = []\n",
    "encoder = LabelEncoder()\n",
    "timerStamper['CategoryEncoded'] = encoder.fit_transform(timerStamper['Category'])\n",
    "from datetime import timedelta\n",
    "import ast\n",
    "dataSector = []\n",
    "for iter, line in timerStamper.iterrows():\n",
    "    buysIndex0 = ast.literal_eval(line['BuysIndex0'])\n",
    "    buysIndex1 = ast.literal_eval(line['BuysIndex1'])\n",
    "    try:\n",
    "        timeDict = ast.literal_eval(line['TimeStamps'])\n",
    "        allDeltas = []\n",
    "        for deltas in timeDict.values():\n",
    "            allDeltas.extend(deltas)\n",
    "        maxDelta = max(allDeltas)\n",
    "    except:\n",
    "        badIndeces.append(iter)\n",
    "        print(\"Error: 7\")\n",
    "        continue\n",
    "    # Total length of time of the market\n",
    "    if None in [line['CategoryEncoded'], maxDelta ,len(buysIndex0), sum(buysIndex0), len(buysIndex1), sum(buysIndex1), line['Whale0'], line['Whale1'], line['FinalRatio']] or (len(buysIndex0) + len(buysIndex1)) < 10:\n",
    "        badIndeces.append(iter)\n",
    "        continue\n",
    "    dataSector.append([line['CategoryEncoded'], maxDelta ,len(buysIndex0), sum(buysIndex0), len(buysIndex1), sum(buysIndex1), line['Whale0'], line['Whale1'], line['FinalRatio'], line['OutcomeIndex']])\n",
    "    print(line['OutcomeIndex'])\n",
    "    \n",
    "timerStamper.drop(badIndeces, inplace=True)\n",
    "timerStamper.reset_index(drop=True, inplace=True)\n",
    "timerStamper.to_csv('data/silver/timerStamper1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c3758",
   "metadata": {},
   "source": [
    "Splitting Data into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32) # Use float for binary labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "totalData = len(dataSector)\n",
    "\n",
    "trainCap = totalData*4 // 5\n",
    "\n",
    "trainDataInputs = dataSector[:trainCap]\n",
    "\n",
    "testDataInputs = dataSector[trainCap:]\n",
    "\n",
    "trainResults = [x[9] for x in trainDataInputs]\n",
    "\n",
    "testResults = [x[9] for x in testDataInputs]\n",
    "\n",
    "trainData = BinaryDataset(trainDataInputs, trainResults)\n",
    "testData = BinaryDataset(testDataInputs, testResults)\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "shuffle = True\n",
    "dataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=shuffle)\n",
    "dataLoaderTest = DataLoader(testData, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd821c6e",
   "metadata": {},
   "source": [
    "Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fff352",
   "metadata": {},
   "source": [
    "Model Architecture taken from here : https://medium.com/data-science/pytorch-tabular-binary-classification-a0368da5bb89\n",
    "\n",
    "Key Point: Relu serves to fid complex relationships between variables\n",
    "Key Point: Normalizing to keep everything within 0 to 1 scope and not put too much weight on anything\n",
    "Key Point: Linear Layers Attempt to make reason out of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10096d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class MarketPredictor(nn.Module):\n",
    "    \"Initializes multi-class classification model\"\n",
    "    def __init__(self, input_features=9, output_features=1, hidden_units=16):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_features, hidden_units) \n",
    "        self.layer_2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.layer_out = nn.Linear(hidden_units, output_features) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_units)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_units)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "MarketPredictorModel = MarketPredictor(input_features=10,\n",
    "                    output_features=1,\n",
    "                    hidden_units=16)\n",
    "\n",
    "lossFn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(MarketPredictorModel.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0bf5d",
   "metadata": {},
   "source": [
    "A way to Analyze Accuracy (Literally just a percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1c26015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryAccuracy(actualOutcomes, predProbs, threshold=0.5):\n",
    "    preds = (predProbs > threshold).float()\n",
    "    correct = (preds == actualOutcomes).float().sum()\n",
    "    acc = correct / actualOutcomes.shape[0]\n",
    "    return acc * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da43031",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7bce3",
   "metadata": {},
   "source": [
    "Both training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6fd84c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "'''\n",
    "def train(epochs: int):\n",
    "  rates = []\n",
    "  maxAcc = 0\n",
    "  for epoch in range(epochs):\n",
    "    MarketPredictorModel.train()\n",
    "    trainIndex = random.randint(736)\n",
    "    binaryPredictions = MarketPredictorModel(trainData[trainIndex]).squeeze()\n",
    "    loss = lossFn(binaryPredictions, trainResults)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    probs1 = torch.sigmoid(binaryPredictions)\n",
    "    accuracy = binaryAccuracy(trainResults, probs1)\n",
    "    MarketPredictorModel.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "      indexTest = random.randint(100)\n",
    "      testPredictions = MarketPredictorModel(testData[indexTest]).squeeze()\n",
    "      lossTest = lossFn(testPredictions, testResults[indexTest])\n",
    "      probs = torch.sigmoid(testPredictions)\n",
    "      accTest = binaryAccuracy(testResults, probs)\n",
    "      \n",
    "      \n",
    "      print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Acc: {accuracy:.2f}% | Test Loss: {lossTest:.5f} | Test Acc: {accTest:.4f}\")\n",
    "      maxAcc = max(maxAcc, accTest)\n",
    "      rates.append(accTest)\n",
    "      \n",
    "  import statistics\n",
    "  return maxAcc, statistics.mean([r.item() for r in rates])\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "\n",
    "    for i, data in enumerate(dataLoader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = MarketPredictorModel(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = lossFn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        outputsSig = torch.sigmoid(outputs)\n",
    "\n",
    "        \n",
    "def test_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Iterate over the test data and generate predictions\n",
    "    for i, data in enumerate(dataLoaderTest):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = MarketPredictorModel(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = lossFn(outputs, labels)\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        outputsSig = torch.sigmoid(outputs)\n",
    "        accuracy = binaryAccuracy(labels, outputsSig)\n",
    "        \n",
    "        print(f\"Epoch: {epoch_index} | Test Loss: {loss:.5f} | Test Acc: {accuracy:.4f}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351432d2",
   "metadata": {},
   "source": [
    "Training Data to be 85% accuracte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ee40ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Test Loss: 0.60456 | Test Acc: 70.8333\n",
      "Epoch: 0 | Test Loss: 0.69069 | Test Acc: 79.1667\n",
      "Epoch: 0 | Test Loss: 0.66730 | Test Acc: 58.3333\n",
      "Epoch: 0 | Test Loss: 0.62188 | Test Acc: 79.1667\n",
      "Epoch: 0 | Test Loss: 0.64207 | Test Acc: 75.0000\n",
      "Epoch: 0 | Test Loss: 0.64822 | Test Acc: 58.3333\n",
      "Epoch: 0 | Test Loss: 0.53936 | Test Acc: 83.3333\n",
      "Epoch: 0 | Test Loss: 0.59176 | Test Acc: 81.2500\n",
      "Epoch 1 of 200\n",
      "Epoch: 1 | Test Loss: 0.68309 | Test Acc: 79.1667\n",
      "Epoch: 1 | Test Loss: 0.60463 | Test Acc: 75.0000\n",
      "Epoch: 1 | Test Loss: 0.58545 | Test Acc: 79.1667\n",
      "Epoch: 1 | Test Loss: 0.65504 | Test Acc: 66.6667\n",
      "Epoch: 1 | Test Loss: 0.55668 | Test Acc: 75.0000\n",
      "Epoch: 1 | Test Loss: 0.50289 | Test Acc: 87.5000\n",
      "Epoch: 1 | Test Loss: 0.62327 | Test Acc: 79.1667\n",
      "Epoch: 1 | Test Loss: 0.59279 | Test Acc: 81.2500\n",
      "Epoch 2 of 200\n",
      "Epoch: 2 | Test Loss: 0.60584 | Test Acc: 83.3333\n",
      "Epoch: 2 | Test Loss: 0.55445 | Test Acc: 83.3333\n",
      "Epoch: 2 | Test Loss: 0.56385 | Test Acc: 79.1667\n",
      "Epoch: 2 | Test Loss: 0.52767 | Test Acc: 83.3333\n",
      "Epoch: 2 | Test Loss: 0.51954 | Test Acc: 91.6667\n",
      "Epoch: 2 | Test Loss: 0.61906 | Test Acc: 66.6667\n",
      "Epoch: 2 | Test Loss: 0.61572 | Test Acc: 62.5000\n",
      "Epoch: 2 | Test Loss: 0.67542 | Test Acc: 81.2500\n",
      "Epoch 3 of 200\n",
      "Epoch: 3 | Test Loss: 0.58145 | Test Acc: 79.1667\n",
      "Epoch: 3 | Test Loss: 0.57764 | Test Acc: 75.0000\n",
      "Epoch: 3 | Test Loss: 0.42940 | Test Acc: 83.3333\n",
      "Epoch: 3 | Test Loss: 0.56313 | Test Acc: 79.1667\n",
      "Epoch: 3 | Test Loss: 0.52311 | Test Acc: 75.0000\n",
      "Epoch: 3 | Test Loss: 0.55607 | Test Acc: 75.0000\n",
      "Epoch: 3 | Test Loss: 0.56170 | Test Acc: 75.0000\n",
      "Epoch: 3 | Test Loss: 0.44309 | Test Acc: 93.7500\n",
      "Epoch 4 of 200\n",
      "Epoch: 4 | Test Loss: 0.50979 | Test Acc: 83.3333\n",
      "Epoch: 4 | Test Loss: 0.50459 | Test Acc: 83.3333\n",
      "Epoch: 4 | Test Loss: 0.55975 | Test Acc: 70.8333\n",
      "Epoch: 4 | Test Loss: 0.45210 | Test Acc: 83.3333\n",
      "Epoch: 4 | Test Loss: 0.53308 | Test Acc: 79.1667\n",
      "Epoch: 4 | Test Loss: 0.47188 | Test Acc: 87.5000\n",
      "Epoch: 4 | Test Loss: 0.53121 | Test Acc: 83.3333\n",
      "Epoch: 4 | Test Loss: 0.52468 | Test Acc: 75.0000\n",
      "Epoch 5 of 200\n",
      "Epoch: 5 | Test Loss: 0.56383 | Test Acc: 66.6667\n",
      "Epoch: 5 | Test Loss: 0.47027 | Test Acc: 87.5000\n",
      "Epoch: 5 | Test Loss: 0.59068 | Test Acc: 79.1667\n",
      "Epoch: 5 | Test Loss: 0.42132 | Test Acc: 95.8333\n",
      "Epoch: 5 | Test Loss: 0.46413 | Test Acc: 83.3333\n",
      "Epoch: 5 | Test Loss: 0.50215 | Test Acc: 87.5000\n",
      "Epoch: 5 | Test Loss: 0.45076 | Test Acc: 79.1667\n",
      "Epoch: 5 | Test Loss: 0.67889 | Test Acc: 68.7500\n",
      "Epoch 6 of 200\n",
      "Epoch: 6 | Test Loss: 0.50081 | Test Acc: 66.6667\n",
      "Epoch: 6 | Test Loss: 0.38140 | Test Acc: 95.8333\n",
      "Epoch: 6 | Test Loss: 0.50936 | Test Acc: 66.6667\n",
      "Epoch: 6 | Test Loss: 0.43855 | Test Acc: 83.3333\n",
      "Epoch: 6 | Test Loss: 0.60853 | Test Acc: 79.1667\n",
      "Epoch: 6 | Test Loss: 0.46133 | Test Acc: 79.1667\n",
      "Epoch: 6 | Test Loss: 0.49190 | Test Acc: 79.1667\n",
      "Epoch: 6 | Test Loss: 0.44776 | Test Acc: 81.2500\n",
      "Epoch 7 of 200\n",
      "Epoch: 7 | Test Loss: 0.58122 | Test Acc: 79.1667\n",
      "Epoch: 7 | Test Loss: 0.42676 | Test Acc: 91.6667\n",
      "Epoch: 7 | Test Loss: 0.44573 | Test Acc: 83.3333\n",
      "Epoch: 7 | Test Loss: 0.51382 | Test Acc: 79.1667\n",
      "Epoch: 7 | Test Loss: 0.57255 | Test Acc: 75.0000\n",
      "Epoch: 7 | Test Loss: 0.44216 | Test Acc: 83.3333\n",
      "Epoch: 7 | Test Loss: 0.44354 | Test Acc: 95.8333\n",
      "Epoch: 7 | Test Loss: 0.57421 | Test Acc: 56.2500\n",
      "Epoch 8 of 200\n",
      "Epoch: 8 | Test Loss: 0.39598 | Test Acc: 87.5000\n",
      "Epoch: 8 | Test Loss: 0.46640 | Test Acc: 75.0000\n",
      "Epoch: 8 | Test Loss: 0.41963 | Test Acc: 91.6667\n",
      "Epoch: 8 | Test Loss: 0.59205 | Test Acc: 50.0000\n",
      "Epoch: 8 | Test Loss: 0.50312 | Test Acc: 79.1667\n",
      "Epoch: 8 | Test Loss: 0.45219 | Test Acc: 91.6667\n",
      "Epoch: 8 | Test Loss: 0.60195 | Test Acc: 75.0000\n",
      "Epoch: 8 | Test Loss: 0.43726 | Test Acc: 81.2500\n",
      "Epoch 9 of 200\n",
      "Epoch: 9 | Test Loss: 0.57524 | Test Acc: 75.0000\n",
      "Epoch: 9 | Test Loss: 0.51830 | Test Acc: 79.1667\n",
      "Epoch: 9 | Test Loss: 0.45000 | Test Acc: 83.3333\n",
      "Epoch: 9 | Test Loss: 0.60635 | Test Acc: 62.5000\n",
      "Epoch: 9 | Test Loss: 0.56889 | Test Acc: 62.5000\n",
      "Epoch: 9 | Test Loss: 0.40152 | Test Acc: 91.6667\n",
      "Epoch: 9 | Test Loss: 0.48339 | Test Acc: 79.1667\n",
      "Epoch: 9 | Test Loss: 0.48580 | Test Acc: 81.2500\n",
      "Epoch 10 of 200\n",
      "Epoch: 10 | Test Loss: 0.51554 | Test Acc: 79.1667\n",
      "Epoch: 10 | Test Loss: 0.43528 | Test Acc: 83.3333\n",
      "Epoch: 10 | Test Loss: 0.43651 | Test Acc: 83.3333\n",
      "Epoch: 10 | Test Loss: 0.49451 | Test Acc: 70.8333\n",
      "Epoch: 10 | Test Loss: 0.39084 | Test Acc: 87.5000\n",
      "Epoch: 10 | Test Loss: 0.44670 | Test Acc: 87.5000\n",
      "Epoch: 10 | Test Loss: 0.49668 | Test Acc: 75.0000\n",
      "Epoch: 10 | Test Loss: 0.52469 | Test Acc: 75.0000\n",
      "Epoch 11 of 200\n",
      "Epoch: 11 | Test Loss: 0.63691 | Test Acc: 70.8333\n",
      "Epoch: 11 | Test Loss: 0.54406 | Test Acc: 79.1667\n",
      "Epoch: 11 | Test Loss: 0.46469 | Test Acc: 75.0000\n",
      "Epoch: 11 | Test Loss: 0.57380 | Test Acc: 75.0000\n",
      "Epoch: 11 | Test Loss: 0.46204 | Test Acc: 79.1667\n",
      "Epoch: 11 | Test Loss: 0.41019 | Test Acc: 83.3333\n",
      "Epoch: 11 | Test Loss: 0.46045 | Test Acc: 70.8333\n",
      "Epoch: 11 | Test Loss: 0.50943 | Test Acc: 75.0000\n",
      "Epoch 12 of 200\n",
      "Epoch: 12 | Test Loss: 0.57026 | Test Acc: 62.5000\n",
      "Epoch: 12 | Test Loss: 0.50632 | Test Acc: 79.1667\n",
      "Epoch: 12 | Test Loss: 0.53051 | Test Acc: 75.0000\n",
      "Epoch: 12 | Test Loss: 0.48317 | Test Acc: 79.1667\n",
      "Epoch: 12 | Test Loss: 0.43554 | Test Acc: 75.0000\n",
      "Epoch: 12 | Test Loss: 0.45032 | Test Acc: 83.3333\n",
      "Epoch: 12 | Test Loss: 0.42661 | Test Acc: 83.3333\n",
      "Epoch: 12 | Test Loss: 0.45584 | Test Acc: 87.5000\n",
      "Epoch 13 of 200\n",
      "Epoch: 13 | Test Loss: 0.37433 | Test Acc: 95.8333\n",
      "Epoch: 13 | Test Loss: 0.61222 | Test Acc: 66.6667\n",
      "Epoch: 13 | Test Loss: 0.37330 | Test Acc: 95.8333\n",
      "Epoch: 13 | Test Loss: 0.39772 | Test Acc: 79.1667\n",
      "Epoch: 13 | Test Loss: 0.56571 | Test Acc: 70.8333\n",
      "Epoch: 13 | Test Loss: 0.46424 | Test Acc: 79.1667\n",
      "Epoch: 13 | Test Loss: 0.53135 | Test Acc: 79.1667\n",
      "Epoch: 13 | Test Loss: 0.47194 | Test Acc: 81.2500\n",
      "Epoch 14 of 200\n",
      "Epoch: 14 | Test Loss: 0.44869 | Test Acc: 79.1667\n",
      "Epoch: 14 | Test Loss: 0.47908 | Test Acc: 87.5000\n",
      "Epoch: 14 | Test Loss: 0.48539 | Test Acc: 66.6667\n",
      "Epoch: 14 | Test Loss: 0.53795 | Test Acc: 79.1667\n",
      "Epoch: 14 | Test Loss: 0.59824 | Test Acc: 62.5000\n",
      "Epoch: 14 | Test Loss: 0.39727 | Test Acc: 87.5000\n",
      "Epoch: 14 | Test Loss: 0.44958 | Test Acc: 83.3333\n",
      "Epoch: 14 | Test Loss: 0.46909 | Test Acc: 87.5000\n",
      "Epoch 15 of 200\n",
      "Epoch: 15 | Test Loss: 0.44870 | Test Acc: 83.3333\n",
      "Epoch: 15 | Test Loss: 0.51665 | Test Acc: 75.0000\n",
      "Epoch: 15 | Test Loss: 0.61234 | Test Acc: 62.5000\n",
      "Epoch: 15 | Test Loss: 0.48464 | Test Acc: 87.5000\n",
      "Epoch: 15 | Test Loss: 0.46367 | Test Acc: 75.0000\n",
      "Epoch: 15 | Test Loss: 0.51082 | Test Acc: 79.1667\n",
      "Epoch: 15 | Test Loss: 0.46947 | Test Acc: 75.0000\n",
      "Epoch: 15 | Test Loss: 0.36557 | Test Acc: 93.7500\n",
      "Epoch 16 of 200\n",
      "Epoch: 16 | Test Loss: 0.57007 | Test Acc: 62.5000\n",
      "Epoch: 16 | Test Loss: 0.55798 | Test Acc: 62.5000\n",
      "Epoch: 16 | Test Loss: 0.40978 | Test Acc: 95.8333\n",
      "Epoch: 16 | Test Loss: 0.40793 | Test Acc: 83.3333\n",
      "Epoch: 16 | Test Loss: 0.47646 | Test Acc: 79.1667\n",
      "Epoch: 16 | Test Loss: 0.40968 | Test Acc: 87.5000\n",
      "Epoch: 16 | Test Loss: 0.41508 | Test Acc: 87.5000\n",
      "Epoch: 16 | Test Loss: 0.45326 | Test Acc: 93.7500\n",
      "Epoch 17 of 200\n",
      "Epoch: 17 | Test Loss: 0.48134 | Test Acc: 79.1667\n",
      "Epoch: 17 | Test Loss: 0.54844 | Test Acc: 79.1667\n",
      "Epoch: 17 | Test Loss: 0.45827 | Test Acc: 70.8333\n",
      "Epoch: 17 | Test Loss: 0.46140 | Test Acc: 79.1667\n",
      "Epoch: 17 | Test Loss: 0.47819 | Test Acc: 83.3333\n",
      "Epoch: 17 | Test Loss: 0.33844 | Test Acc: 91.6667\n",
      "Epoch: 17 | Test Loss: 0.41642 | Test Acc: 83.3333\n",
      "Epoch: 17 | Test Loss: 0.67222 | Test Acc: 68.7500\n",
      "Epoch 18 of 200\n",
      "Epoch: 18 | Test Loss: 0.46030 | Test Acc: 75.0000\n",
      "Epoch: 18 | Test Loss: 0.51153 | Test Acc: 79.1667\n",
      "Epoch: 18 | Test Loss: 0.51496 | Test Acc: 83.3333\n",
      "Epoch: 18 | Test Loss: 0.45228 | Test Acc: 83.3333\n",
      "Epoch: 18 | Test Loss: 0.42863 | Test Acc: 83.3333\n",
      "Epoch: 18 | Test Loss: 0.50080 | Test Acc: 87.5000\n",
      "Epoch: 18 | Test Loss: 0.52234 | Test Acc: 70.8333\n",
      "Epoch: 18 | Test Loss: 0.48071 | Test Acc: 75.0000\n",
      "Epoch 19 of 200\n",
      "Epoch: 19 | Test Loss: 0.50212 | Test Acc: 79.1667\n",
      "Epoch: 19 | Test Loss: 0.35854 | Test Acc: 87.5000\n",
      "Epoch: 19 | Test Loss: 0.51587 | Test Acc: 70.8333\n",
      "Epoch: 19 | Test Loss: 0.35504 | Test Acc: 87.5000\n",
      "Epoch: 19 | Test Loss: 0.39367 | Test Acc: 75.0000\n",
      "Epoch: 19 | Test Loss: 0.45710 | Test Acc: 79.1667\n",
      "Epoch: 19 | Test Loss: 0.57915 | Test Acc: 70.8333\n",
      "Epoch: 19 | Test Loss: 0.60306 | Test Acc: 68.7500\n",
      "Epoch 20 of 200\n",
      "Epoch: 20 | Test Loss: 0.39102 | Test Acc: 87.5000\n",
      "Epoch: 20 | Test Loss: 0.56908 | Test Acc: 58.3333\n",
      "Epoch: 20 | Test Loss: 0.46145 | Test Acc: 79.1667\n",
      "Epoch: 20 | Test Loss: 0.48191 | Test Acc: 79.1667\n",
      "Epoch: 20 | Test Loss: 0.55168 | Test Acc: 66.6667\n",
      "Epoch: 20 | Test Loss: 0.49449 | Test Acc: 79.1667\n",
      "Epoch: 20 | Test Loss: 0.48948 | Test Acc: 70.8333\n",
      "Epoch: 20 | Test Loss: 0.61988 | Test Acc: 62.5000\n",
      "Epoch 21 of 200\n",
      "Epoch: 21 | Test Loss: 0.51373 | Test Acc: 75.0000\n",
      "Epoch: 21 | Test Loss: 0.56587 | Test Acc: 58.3333\n",
      "Epoch: 21 | Test Loss: 0.53728 | Test Acc: 75.0000\n",
      "Epoch: 21 | Test Loss: 0.43283 | Test Acc: 79.1667\n",
      "Epoch: 21 | Test Loss: 0.41990 | Test Acc: 83.3333\n",
      "Epoch: 21 | Test Loss: 0.58534 | Test Acc: 75.0000\n",
      "Epoch: 21 | Test Loss: 0.59799 | Test Acc: 75.0000\n",
      "Epoch: 21 | Test Loss: 0.44279 | Test Acc: 87.5000\n",
      "Epoch 22 of 200\n",
      "Epoch: 22 | Test Loss: 0.43460 | Test Acc: 83.3333\n",
      "Epoch: 22 | Test Loss: 0.44671 | Test Acc: 79.1667\n",
      "Epoch: 22 | Test Loss: 0.52227 | Test Acc: 66.6667\n",
      "Epoch: 22 | Test Loss: 0.63126 | Test Acc: 70.8333\n",
      "Epoch: 22 | Test Loss: 0.43414 | Test Acc: 91.6667\n",
      "Epoch: 22 | Test Loss: 0.44231 | Test Acc: 79.1667\n",
      "Epoch: 22 | Test Loss: 0.57748 | Test Acc: 66.6667\n",
      "Epoch: 22 | Test Loss: 0.50495 | Test Acc: 75.0000\n",
      "Epoch 23 of 200\n",
      "Epoch: 23 | Test Loss: 0.45534 | Test Acc: 83.3333\n",
      "Epoch: 23 | Test Loss: 0.51637 | Test Acc: 75.0000\n",
      "Epoch: 23 | Test Loss: 0.37462 | Test Acc: 87.5000\n",
      "Epoch: 23 | Test Loss: 0.63238 | Test Acc: 79.1667\n",
      "Epoch: 23 | Test Loss: 0.56463 | Test Acc: 66.6667\n",
      "Epoch: 23 | Test Loss: 0.45636 | Test Acc: 83.3333\n",
      "Epoch: 23 | Test Loss: 0.47166 | Test Acc: 83.3333\n",
      "Epoch: 23 | Test Loss: 0.44567 | Test Acc: 81.2500\n",
      "Epoch 24 of 200\n",
      "Epoch: 24 | Test Loss: 0.45370 | Test Acc: 75.0000\n",
      "Epoch: 24 | Test Loss: 0.35103 | Test Acc: 95.8333\n",
      "Epoch: 24 | Test Loss: 0.39716 | Test Acc: 91.6667\n",
      "Epoch: 24 | Test Loss: 0.44315 | Test Acc: 83.3333\n",
      "Epoch: 24 | Test Loss: 0.61332 | Test Acc: 83.3333\n",
      "Epoch: 24 | Test Loss: 0.38312 | Test Acc: 83.3333\n",
      "Epoch: 24 | Test Loss: 0.51543 | Test Acc: 83.3333\n",
      "Epoch: 24 | Test Loss: 0.32924 | Test Acc: 87.5000\n",
      "Epoch 25 of 200\n",
      "Epoch: 25 | Test Loss: 0.46500 | Test Acc: 75.0000\n",
      "Epoch: 25 | Test Loss: 0.65257 | Test Acc: 54.1667\n",
      "Epoch: 25 | Test Loss: 0.40649 | Test Acc: 87.5000\n",
      "Epoch: 25 | Test Loss: 0.47707 | Test Acc: 79.1667\n",
      "Epoch: 25 | Test Loss: 0.40036 | Test Acc: 75.0000\n",
      "Epoch: 25 | Test Loss: 0.47697 | Test Acc: 83.3333\n",
      "Epoch: 25 | Test Loss: 0.42440 | Test Acc: 79.1667\n",
      "Epoch: 25 | Test Loss: 0.52507 | Test Acc: 62.5000\n",
      "Epoch 26 of 200\n",
      "Epoch: 26 | Test Loss: 0.50307 | Test Acc: 70.8333\n",
      "Epoch: 26 | Test Loss: 0.67186 | Test Acc: 62.5000\n",
      "Epoch: 26 | Test Loss: 0.54450 | Test Acc: 70.8333\n",
      "Epoch: 26 | Test Loss: 0.32200 | Test Acc: 91.6667\n",
      "Epoch: 26 | Test Loss: 0.51742 | Test Acc: 66.6667\n",
      "Epoch: 26 | Test Loss: 0.48603 | Test Acc: 62.5000\n",
      "Epoch: 26 | Test Loss: 0.48041 | Test Acc: 70.8333\n",
      "Epoch: 26 | Test Loss: 0.57200 | Test Acc: 75.0000\n",
      "Epoch 27 of 200\n",
      "Epoch: 27 | Test Loss: 0.39738 | Test Acc: 83.3333\n",
      "Epoch: 27 | Test Loss: 0.35998 | Test Acc: 95.8333\n",
      "Epoch: 27 | Test Loss: 0.35852 | Test Acc: 87.5000\n",
      "Epoch: 27 | Test Loss: 0.57742 | Test Acc: 66.6667\n",
      "Epoch: 27 | Test Loss: 0.51545 | Test Acc: 75.0000\n",
      "Epoch: 27 | Test Loss: 0.44071 | Test Acc: 79.1667\n",
      "Epoch: 27 | Test Loss: 0.44965 | Test Acc: 79.1667\n",
      "Epoch: 27 | Test Loss: 0.52423 | Test Acc: 81.2500\n",
      "Epoch 28 of 200\n",
      "Epoch: 28 | Test Loss: 0.48611 | Test Acc: 70.8333\n",
      "Epoch: 28 | Test Loss: 0.47113 | Test Acc: 79.1667\n",
      "Epoch: 28 | Test Loss: 0.52768 | Test Acc: 62.5000\n",
      "Epoch: 28 | Test Loss: 0.51064 | Test Acc: 70.8333\n",
      "Epoch: 28 | Test Loss: 0.49321 | Test Acc: 75.0000\n",
      "Epoch: 28 | Test Loss: 0.56539 | Test Acc: 58.3333\n",
      "Epoch: 28 | Test Loss: 0.56955 | Test Acc: 62.5000\n",
      "Epoch: 28 | Test Loss: 0.30768 | Test Acc: 93.7500\n",
      "Epoch 29 of 200\n",
      "Epoch: 29 | Test Loss: 0.45031 | Test Acc: 75.0000\n",
      "Epoch: 29 | Test Loss: 0.52019 | Test Acc: 75.0000\n",
      "Epoch: 29 | Test Loss: 0.37667 | Test Acc: 87.5000\n",
      "Epoch: 29 | Test Loss: 0.59604 | Test Acc: 70.8333\n",
      "Epoch: 29 | Test Loss: 0.52372 | Test Acc: 75.0000\n",
      "Epoch: 29 | Test Loss: 0.32043 | Test Acc: 91.6667\n",
      "Epoch: 29 | Test Loss: 0.40817 | Test Acc: 87.5000\n",
      "Epoch: 29 | Test Loss: 0.51677 | Test Acc: 62.5000\n",
      "Epoch 30 of 200\n",
      "Epoch: 30 | Test Loss: 0.59163 | Test Acc: 62.5000\n",
      "Epoch: 30 | Test Loss: 0.39974 | Test Acc: 83.3333\n",
      "Epoch: 30 | Test Loss: 0.59423 | Test Acc: 75.0000\n",
      "Epoch: 30 | Test Loss: 0.50898 | Test Acc: 70.8333\n",
      "Epoch: 30 | Test Loss: 0.49942 | Test Acc: 70.8333\n",
      "Epoch: 30 | Test Loss: 0.53331 | Test Acc: 79.1667\n",
      "Epoch: 30 | Test Loss: 0.35863 | Test Acc: 91.6667\n",
      "Epoch: 30 | Test Loss: 0.45754 | Test Acc: 75.0000\n",
      "Epoch 31 of 200\n",
      "Epoch: 31 | Test Loss: 0.31365 | Test Acc: 91.6667\n",
      "Epoch: 31 | Test Loss: 0.49102 | Test Acc: 70.8333\n",
      "Epoch: 31 | Test Loss: 0.62579 | Test Acc: 75.0000\n",
      "Epoch: 31 | Test Loss: 0.43876 | Test Acc: 75.0000\n",
      "Epoch: 31 | Test Loss: 0.30860 | Test Acc: 87.5000\n",
      "Epoch: 31 | Test Loss: 0.45683 | Test Acc: 87.5000\n",
      "Epoch: 31 | Test Loss: 0.65632 | Test Acc: 66.6667\n",
      "Epoch: 31 | Test Loss: 0.37705 | Test Acc: 81.2500\n",
      "Epoch 32 of 200\n",
      "Epoch: 32 | Test Loss: 0.39381 | Test Acc: 83.3333\n",
      "Epoch: 32 | Test Loss: 0.37668 | Test Acc: 87.5000\n",
      "Epoch: 32 | Test Loss: 0.44424 | Test Acc: 79.1667\n",
      "Epoch: 32 | Test Loss: 0.48301 | Test Acc: 75.0000\n",
      "Epoch: 32 | Test Loss: 0.73817 | Test Acc: 62.5000\n",
      "Epoch: 32 | Test Loss: 0.50419 | Test Acc: 70.8333\n",
      "Epoch: 32 | Test Loss: 0.57420 | Test Acc: 62.5000\n",
      "Epoch: 32 | Test Loss: 0.45087 | Test Acc: 81.2500\n",
      "Epoch 33 of 200\n",
      "Epoch: 33 | Test Loss: 0.44398 | Test Acc: 75.0000\n",
      "Epoch: 33 | Test Loss: 0.71801 | Test Acc: 66.6667\n",
      "Epoch: 33 | Test Loss: 0.39328 | Test Acc: 87.5000\n",
      "Epoch: 33 | Test Loss: 0.44533 | Test Acc: 83.3333\n",
      "Epoch: 33 | Test Loss: 0.54280 | Test Acc: 70.8333\n",
      "Epoch: 33 | Test Loss: 0.41128 | Test Acc: 91.6667\n",
      "Epoch: 33 | Test Loss: 0.44421 | Test Acc: 83.3333\n",
      "Epoch: 33 | Test Loss: 0.54026 | Test Acc: 68.7500\n",
      "Epoch 34 of 200\n",
      "Epoch: 34 | Test Loss: 0.47297 | Test Acc: 83.3333\n",
      "Epoch: 34 | Test Loss: 0.56248 | Test Acc: 79.1667\n",
      "Epoch: 34 | Test Loss: 0.48250 | Test Acc: 75.0000\n",
      "Epoch: 34 | Test Loss: 0.54372 | Test Acc: 79.1667\n",
      "Epoch: 34 | Test Loss: 0.40767 | Test Acc: 91.6667\n",
      "Epoch: 34 | Test Loss: 0.50423 | Test Acc: 58.3333\n",
      "Epoch: 34 | Test Loss: 0.47637 | Test Acc: 79.1667\n",
      "Epoch: 34 | Test Loss: 0.36806 | Test Acc: 81.2500\n",
      "Epoch 35 of 200\n",
      "Epoch: 35 | Test Loss: 0.38170 | Test Acc: 91.6667\n",
      "Epoch: 35 | Test Loss: 0.55662 | Test Acc: 58.3333\n",
      "Epoch: 35 | Test Loss: 0.57522 | Test Acc: 75.0000\n",
      "Epoch: 35 | Test Loss: 0.53156 | Test Acc: 83.3333\n",
      "Epoch: 35 | Test Loss: 0.44749 | Test Acc: 83.3333\n",
      "Epoch: 35 | Test Loss: 0.41208 | Test Acc: 87.5000\n",
      "Epoch: 35 | Test Loss: 0.38298 | Test Acc: 83.3333\n",
      "Epoch: 35 | Test Loss: 0.30363 | Test Acc: 100.0000\n",
      "Epoch 36 of 200\n",
      "Epoch: 36 | Test Loss: 0.46481 | Test Acc: 70.8333\n",
      "Epoch: 36 | Test Loss: 0.48765 | Test Acc: 75.0000\n",
      "Epoch: 36 | Test Loss: 0.49391 | Test Acc: 79.1667\n",
      "Epoch: 36 | Test Loss: 0.47560 | Test Acc: 70.8333\n",
      "Epoch: 36 | Test Loss: 0.54370 | Test Acc: 83.3333\n",
      "Epoch: 36 | Test Loss: 0.49213 | Test Acc: 83.3333\n",
      "Epoch: 36 | Test Loss: 0.40808 | Test Acc: 75.0000\n",
      "Epoch: 36 | Test Loss: 0.44829 | Test Acc: 75.0000\n",
      "Epoch 37 of 200\n",
      "Epoch: 37 | Test Loss: 0.62496 | Test Acc: 54.1667\n",
      "Epoch: 37 | Test Loss: 0.50516 | Test Acc: 66.6667\n",
      "Epoch: 37 | Test Loss: 0.52437 | Test Acc: 66.6667\n",
      "Epoch: 37 | Test Loss: 0.41428 | Test Acc: 87.5000\n",
      "Epoch: 37 | Test Loss: 0.51991 | Test Acc: 70.8333\n",
      "Epoch: 37 | Test Loss: 0.34716 | Test Acc: 91.6667\n",
      "Epoch: 37 | Test Loss: 0.49923 | Test Acc: 75.0000\n",
      "Epoch: 37 | Test Loss: 0.52812 | Test Acc: 81.2500\n",
      "Epoch 38 of 200\n",
      "Epoch: 38 | Test Loss: 0.46962 | Test Acc: 70.8333\n",
      "Epoch: 38 | Test Loss: 0.41788 | Test Acc: 87.5000\n",
      "Epoch: 38 | Test Loss: 0.50521 | Test Acc: 70.8333\n",
      "Epoch: 38 | Test Loss: 0.43195 | Test Acc: 87.5000\n",
      "Epoch: 38 | Test Loss: 0.49524 | Test Acc: 70.8333\n",
      "Epoch: 38 | Test Loss: 0.31169 | Test Acc: 95.8333\n",
      "Epoch: 38 | Test Loss: 0.49766 | Test Acc: 87.5000\n",
      "Epoch: 38 | Test Loss: 0.51222 | Test Acc: 62.5000\n",
      "Epoch 39 of 200\n",
      "Epoch: 39 | Test Loss: 0.53091 | Test Acc: 87.5000\n",
      "Epoch: 39 | Test Loss: 0.52880 | Test Acc: 83.3333\n",
      "Epoch: 39 | Test Loss: 0.44598 | Test Acc: 83.3333\n",
      "Epoch: 39 | Test Loss: 0.61476 | Test Acc: 66.6667\n",
      "Epoch: 39 | Test Loss: 0.49199 | Test Acc: 75.0000\n",
      "Epoch: 39 | Test Loss: 0.37371 | Test Acc: 87.5000\n",
      "Epoch: 39 | Test Loss: 0.60578 | Test Acc: 62.5000\n",
      "Epoch: 39 | Test Loss: 0.41987 | Test Acc: 81.2500\n",
      "Epoch 40 of 200\n",
      "Epoch: 40 | Test Loss: 0.56312 | Test Acc: 75.0000\n",
      "Epoch: 40 | Test Loss: 0.39617 | Test Acc: 83.3333\n",
      "Epoch: 40 | Test Loss: 0.56286 | Test Acc: 79.1667\n",
      "Epoch: 40 | Test Loss: 0.35844 | Test Acc: 83.3333\n",
      "Epoch: 40 | Test Loss: 0.47531 | Test Acc: 75.0000\n",
      "Epoch: 40 | Test Loss: 0.36969 | Test Acc: 91.6667\n",
      "Epoch: 40 | Test Loss: 0.53120 | Test Acc: 75.0000\n",
      "Epoch: 40 | Test Loss: 0.70639 | Test Acc: 68.7500\n",
      "Epoch 41 of 200\n",
      "Epoch: 41 | Test Loss: 0.56933 | Test Acc: 62.5000\n",
      "Epoch: 41 | Test Loss: 0.54467 | Test Acc: 70.8333\n",
      "Epoch: 41 | Test Loss: 0.39569 | Test Acc: 83.3333\n",
      "Epoch: 41 | Test Loss: 0.45122 | Test Acc: 75.0000\n",
      "Epoch: 41 | Test Loss: 0.52839 | Test Acc: 58.3333\n",
      "Epoch: 41 | Test Loss: 0.56013 | Test Acc: 58.3333\n",
      "Epoch: 41 | Test Loss: 0.51473 | Test Acc: 66.6667\n",
      "Epoch: 41 | Test Loss: 0.53781 | Test Acc: 56.2500\n",
      "Epoch 42 of 200\n",
      "Epoch: 42 | Test Loss: 0.43587 | Test Acc: 87.5000\n",
      "Epoch: 42 | Test Loss: 0.36970 | Test Acc: 79.1667\n",
      "Epoch: 42 | Test Loss: 0.53489 | Test Acc: 70.8333\n",
      "Epoch: 42 | Test Loss: 0.50781 | Test Acc: 66.6667\n",
      "Epoch: 42 | Test Loss: 0.56344 | Test Acc: 62.5000\n",
      "Epoch: 42 | Test Loss: 0.57470 | Test Acc: 75.0000\n",
      "Epoch: 42 | Test Loss: 0.41178 | Test Acc: 79.1667\n",
      "Epoch: 42 | Test Loss: 0.41163 | Test Acc: 81.2500\n",
      "Epoch 43 of 200\n",
      "Epoch: 43 | Test Loss: 0.47725 | Test Acc: 87.5000\n",
      "Epoch: 43 | Test Loss: 0.44176 | Test Acc: 87.5000\n",
      "Epoch: 43 | Test Loss: 0.55484 | Test Acc: 75.0000\n",
      "Epoch: 43 | Test Loss: 0.45416 | Test Acc: 79.1667\n",
      "Epoch: 43 | Test Loss: 0.33303 | Test Acc: 87.5000\n",
      "Epoch: 43 | Test Loss: 0.60549 | Test Acc: 58.3333\n",
      "Epoch: 43 | Test Loss: 0.60694 | Test Acc: 66.6667\n",
      "Epoch: 43 | Test Loss: 0.36654 | Test Acc: 87.5000\n",
      "Epoch 44 of 200\n",
      "Epoch: 44 | Test Loss: 0.47094 | Test Acc: 79.1667\n",
      "Epoch: 44 | Test Loss: 0.53045 | Test Acc: 79.1667\n",
      "Epoch: 44 | Test Loss: 0.51318 | Test Acc: 79.1667\n",
      "Epoch: 44 | Test Loss: 0.36813 | Test Acc: 75.0000\n",
      "Epoch: 44 | Test Loss: 0.46772 | Test Acc: 75.0000\n",
      "Epoch: 44 | Test Loss: 0.48434 | Test Acc: 70.8333\n",
      "Epoch: 44 | Test Loss: 0.36942 | Test Acc: 91.6667\n",
      "Epoch: 44 | Test Loss: 0.38629 | Test Acc: 75.0000\n",
      "Epoch 45 of 200\n",
      "Epoch: 45 | Test Loss: 0.48408 | Test Acc: 70.8333\n",
      "Epoch: 45 | Test Loss: 0.57084 | Test Acc: 62.5000\n",
      "Epoch: 45 | Test Loss: 0.40633 | Test Acc: 83.3333\n",
      "Epoch: 45 | Test Loss: 0.41942 | Test Acc: 79.1667\n",
      "Epoch: 45 | Test Loss: 0.54635 | Test Acc: 75.0000\n",
      "Epoch: 45 | Test Loss: 0.30263 | Test Acc: 91.6667\n",
      "Epoch: 45 | Test Loss: 0.52726 | Test Acc: 70.8333\n",
      "Epoch: 45 | Test Loss: 0.52947 | Test Acc: 81.2500\n",
      "Epoch 46 of 200\n",
      "Epoch: 46 | Test Loss: 0.36577 | Test Acc: 91.6667\n",
      "Epoch: 46 | Test Loss: 0.48406 | Test Acc: 79.1667\n",
      "Epoch: 46 | Test Loss: 0.41333 | Test Acc: 87.5000\n",
      "Epoch: 46 | Test Loss: 0.84206 | Test Acc: 50.0000\n",
      "Epoch: 46 | Test Loss: 0.41205 | Test Acc: 83.3333\n",
      "Epoch: 46 | Test Loss: 0.29254 | Test Acc: 91.6667\n",
      "Epoch: 46 | Test Loss: 0.39401 | Test Acc: 79.1667\n",
      "Epoch: 46 | Test Loss: 0.44938 | Test Acc: 81.2500\n",
      "Epoch 47 of 200\n",
      "Epoch: 47 | Test Loss: 0.47139 | Test Acc: 75.0000\n",
      "Epoch: 47 | Test Loss: 0.48741 | Test Acc: 83.3333\n",
      "Epoch: 47 | Test Loss: 0.46898 | Test Acc: 75.0000\n",
      "Epoch: 47 | Test Loss: 0.50074 | Test Acc: 70.8333\n",
      "Epoch: 47 | Test Loss: 0.50173 | Test Acc: 83.3333\n",
      "Epoch: 47 | Test Loss: 0.52080 | Test Acc: 62.5000\n",
      "Epoch: 47 | Test Loss: 0.55230 | Test Acc: 70.8333\n",
      "Epoch: 47 | Test Loss: 0.41537 | Test Acc: 75.0000\n",
      "Epoch 48 of 200\n",
      "Epoch: 48 | Test Loss: 0.55095 | Test Acc: 79.1667\n",
      "Epoch: 48 | Test Loss: 0.55601 | Test Acc: 66.6667\n",
      "Epoch: 48 | Test Loss: 0.55853 | Test Acc: 62.5000\n",
      "Epoch: 48 | Test Loss: 0.54495 | Test Acc: 83.3333\n",
      "Epoch: 48 | Test Loss: 0.41145 | Test Acc: 83.3333\n",
      "Epoch: 48 | Test Loss: 0.41079 | Test Acc: 83.3333\n",
      "Epoch: 48 | Test Loss: 0.29106 | Test Acc: 87.5000\n",
      "Epoch: 48 | Test Loss: 0.41529 | Test Acc: 81.2500\n",
      "Epoch 49 of 200\n",
      "Epoch: 49 | Test Loss: 0.54747 | Test Acc: 70.8333\n",
      "Epoch: 49 | Test Loss: 0.52896 | Test Acc: 70.8333\n",
      "Epoch: 49 | Test Loss: 0.39692 | Test Acc: 83.3333\n",
      "Epoch: 49 | Test Loss: 0.51828 | Test Acc: 75.0000\n",
      "Epoch: 49 | Test Loss: 0.27080 | Test Acc: 87.5000\n",
      "Epoch: 49 | Test Loss: 0.45220 | Test Acc: 79.1667\n",
      "Epoch: 49 | Test Loss: 0.52669 | Test Acc: 79.1667\n",
      "Epoch: 49 | Test Loss: 0.38584 | Test Acc: 87.5000\n",
      "Epoch 50 of 200\n",
      "Epoch: 50 | Test Loss: 0.34842 | Test Acc: 87.5000\n",
      "Epoch: 50 | Test Loss: 0.42336 | Test Acc: 75.0000\n",
      "Epoch: 50 | Test Loss: 0.56123 | Test Acc: 66.6667\n",
      "Epoch: 50 | Test Loss: 0.50730 | Test Acc: 79.1667\n",
      "Epoch: 50 | Test Loss: 0.46726 | Test Acc: 79.1667\n",
      "Epoch: 50 | Test Loss: 0.55480 | Test Acc: 66.6667\n",
      "Epoch: 50 | Test Loss: 0.45535 | Test Acc: 83.3333\n",
      "Epoch: 50 | Test Loss: 0.41746 | Test Acc: 81.2500\n",
      "Epoch 51 of 200\n",
      "Epoch: 51 | Test Loss: 0.62362 | Test Acc: 62.5000\n",
      "Epoch: 51 | Test Loss: 0.40500 | Test Acc: 83.3333\n",
      "Epoch: 51 | Test Loss: 0.53343 | Test Acc: 75.0000\n",
      "Epoch: 51 | Test Loss: 0.42086 | Test Acc: 79.1667\n",
      "Epoch: 51 | Test Loss: 0.47022 | Test Acc: 75.0000\n",
      "Epoch: 51 | Test Loss: 0.50977 | Test Acc: 62.5000\n",
      "Epoch: 51 | Test Loss: 0.51834 | Test Acc: 66.6667\n",
      "Epoch: 51 | Test Loss: 0.53058 | Test Acc: 81.2500\n",
      "Epoch 52 of 200\n",
      "Epoch: 52 | Test Loss: 0.45586 | Test Acc: 91.6667\n",
      "Epoch: 52 | Test Loss: 0.37616 | Test Acc: 87.5000\n",
      "Epoch: 52 | Test Loss: 0.43814 | Test Acc: 79.1667\n",
      "Epoch: 52 | Test Loss: 0.50930 | Test Acc: 75.0000\n",
      "Epoch: 52 | Test Loss: 0.62304 | Test Acc: 62.5000\n",
      "Epoch: 52 | Test Loss: 0.55409 | Test Acc: 75.0000\n",
      "Epoch: 52 | Test Loss: 0.37324 | Test Acc: 87.5000\n",
      "Epoch: 52 | Test Loss: 0.56489 | Test Acc: 68.7500\n",
      "Epoch 53 of 200\n",
      "Epoch: 53 | Test Loss: 0.42074 | Test Acc: 83.3333\n",
      "Epoch: 53 | Test Loss: 0.57478 | Test Acc: 75.0000\n",
      "Epoch: 53 | Test Loss: 0.44142 | Test Acc: 70.8333\n",
      "Epoch: 53 | Test Loss: 0.52329 | Test Acc: 70.8333\n",
      "Epoch: 53 | Test Loss: 0.47983 | Test Acc: 70.8333\n",
      "Epoch: 53 | Test Loss: 0.35166 | Test Acc: 87.5000\n",
      "Epoch: 53 | Test Loss: 0.45915 | Test Acc: 75.0000\n",
      "Epoch: 53 | Test Loss: 0.40279 | Test Acc: 81.2500\n",
      "Epoch 54 of 200\n",
      "Epoch: 54 | Test Loss: 0.51532 | Test Acc: 75.0000\n",
      "Epoch: 54 | Test Loss: 0.51491 | Test Acc: 79.1667\n",
      "Epoch: 54 | Test Loss: 0.47764 | Test Acc: 70.8333\n",
      "Epoch: 54 | Test Loss: 0.43648 | Test Acc: 91.6667\n",
      "Epoch: 54 | Test Loss: 0.36284 | Test Acc: 79.1667\n",
      "Epoch: 54 | Test Loss: 0.44804 | Test Acc: 83.3333\n",
      "Epoch: 54 | Test Loss: 0.47191 | Test Acc: 79.1667\n",
      "Epoch: 54 | Test Loss: 0.34573 | Test Acc: 93.7500\n",
      "Epoch 55 of 200\n",
      "Epoch: 55 | Test Loss: 0.54545 | Test Acc: 83.3333\n",
      "Epoch: 55 | Test Loss: 0.54596 | Test Acc: 70.8333\n",
      "Epoch: 55 | Test Loss: 0.44935 | Test Acc: 70.8333\n",
      "Epoch: 55 | Test Loss: 0.51931 | Test Acc: 70.8333\n",
      "Epoch: 55 | Test Loss: 0.44903 | Test Acc: 79.1667\n",
      "Epoch: 55 | Test Loss: 0.44593 | Test Acc: 79.1667\n",
      "Epoch: 55 | Test Loss: 0.50143 | Test Acc: 66.6667\n",
      "Epoch: 55 | Test Loss: 0.47747 | Test Acc: 75.0000\n",
      "Epoch 56 of 200\n",
      "Epoch: 56 | Test Loss: 0.48876 | Test Acc: 83.3333\n",
      "Epoch: 56 | Test Loss: 0.53699 | Test Acc: 70.8333\n",
      "Epoch: 56 | Test Loss: 0.46867 | Test Acc: 87.5000\n",
      "Epoch: 56 | Test Loss: 0.48106 | Test Acc: 83.3333\n",
      "Epoch: 56 | Test Loss: 0.38929 | Test Acc: 87.5000\n",
      "Epoch: 56 | Test Loss: 0.51501 | Test Acc: 75.0000\n",
      "Epoch: 56 | Test Loss: 0.34598 | Test Acc: 83.3333\n",
      "Epoch: 56 | Test Loss: 0.34395 | Test Acc: 93.7500\n",
      "Epoch 57 of 200\n",
      "Epoch: 57 | Test Loss: 0.44225 | Test Acc: 70.8333\n",
      "Epoch: 57 | Test Loss: 0.38353 | Test Acc: 91.6667\n",
      "Epoch: 57 | Test Loss: 0.44422 | Test Acc: 83.3333\n",
      "Epoch: 57 | Test Loss: 0.32528 | Test Acc: 87.5000\n",
      "Epoch: 57 | Test Loss: 0.52062 | Test Acc: 79.1667\n",
      "Epoch: 57 | Test Loss: 0.52850 | Test Acc: 83.3333\n",
      "Epoch: 57 | Test Loss: 0.58312 | Test Acc: 54.1667\n",
      "Epoch: 57 | Test Loss: 0.47408 | Test Acc: 75.0000\n",
      "Epoch 58 of 200\n",
      "Epoch: 58 | Test Loss: 0.41704 | Test Acc: 79.1667\n",
      "Epoch: 58 | Test Loss: 0.43663 | Test Acc: 87.5000\n",
      "Epoch: 58 | Test Loss: 0.46388 | Test Acc: 83.3333\n",
      "Epoch: 58 | Test Loss: 0.58559 | Test Acc: 70.8333\n",
      "Epoch: 58 | Test Loss: 0.37996 | Test Acc: 87.5000\n",
      "Epoch: 58 | Test Loss: 0.60224 | Test Acc: 62.5000\n",
      "Epoch: 58 | Test Loss: 0.51853 | Test Acc: 70.8333\n",
      "Epoch: 58 | Test Loss: 0.39593 | Test Acc: 81.2500\n",
      "Epoch 59 of 200\n",
      "Epoch: 59 | Test Loss: 0.52697 | Test Acc: 66.6667\n",
      "Epoch: 59 | Test Loss: 0.46488 | Test Acc: 75.0000\n",
      "Epoch: 59 | Test Loss: 0.54160 | Test Acc: 66.6667\n",
      "Epoch: 59 | Test Loss: 0.50741 | Test Acc: 70.8333\n",
      "Epoch: 59 | Test Loss: 0.42928 | Test Acc: 75.0000\n",
      "Epoch: 59 | Test Loss: 0.54276 | Test Acc: 66.6667\n",
      "Epoch: 59 | Test Loss: 0.56671 | Test Acc: 75.0000\n",
      "Epoch: 59 | Test Loss: 0.45322 | Test Acc: 87.5000\n",
      "Epoch 60 of 200\n",
      "Epoch: 60 | Test Loss: 0.58970 | Test Acc: 62.5000\n",
      "Epoch: 60 | Test Loss: 0.43526 | Test Acc: 79.1667\n",
      "Epoch: 60 | Test Loss: 0.40456 | Test Acc: 83.3333\n",
      "Epoch: 60 | Test Loss: 0.33418 | Test Acc: 87.5000\n",
      "Epoch: 60 | Test Loss: 0.58722 | Test Acc: 66.6667\n",
      "Epoch: 60 | Test Loss: 0.53332 | Test Acc: 70.8333\n",
      "Epoch: 60 | Test Loss: 0.55033 | Test Acc: 75.0000\n",
      "Epoch: 60 | Test Loss: 0.38752 | Test Acc: 93.7500\n",
      "Epoch 61 of 200\n",
      "Epoch: 61 | Test Loss: 0.47920 | Test Acc: 75.0000\n",
      "Epoch: 61 | Test Loss: 0.38097 | Test Acc: 87.5000\n",
      "Epoch: 61 | Test Loss: 0.52217 | Test Acc: 79.1667\n",
      "Epoch: 61 | Test Loss: 0.41928 | Test Acc: 87.5000\n",
      "Epoch: 61 | Test Loss: 0.45198 | Test Acc: 79.1667\n",
      "Epoch: 61 | Test Loss: 0.54688 | Test Acc: 79.1667\n",
      "Epoch: 61 | Test Loss: 0.42383 | Test Acc: 91.6667\n",
      "Epoch: 61 | Test Loss: 0.40216 | Test Acc: 87.5000\n",
      "Epoch 62 of 200\n",
      "Epoch: 62 | Test Loss: 0.46042 | Test Acc: 70.8333\n",
      "Epoch: 62 | Test Loss: 0.49325 | Test Acc: 70.8333\n",
      "Epoch: 62 | Test Loss: 0.50174 | Test Acc: 79.1667\n",
      "Epoch: 62 | Test Loss: 0.36608 | Test Acc: 87.5000\n",
      "Epoch: 62 | Test Loss: 0.46104 | Test Acc: 79.1667\n",
      "Epoch: 62 | Test Loss: 0.50147 | Test Acc: 70.8333\n",
      "Epoch: 62 | Test Loss: 0.66768 | Test Acc: 66.6667\n",
      "Epoch: 62 | Test Loss: 0.36236 | Test Acc: 87.5000\n",
      "Epoch 63 of 200\n",
      "Epoch: 63 | Test Loss: 0.43739 | Test Acc: 70.8333\n",
      "Epoch: 63 | Test Loss: 0.47753 | Test Acc: 79.1667\n",
      "Epoch: 63 | Test Loss: 0.46477 | Test Acc: 70.8333\n",
      "Epoch: 63 | Test Loss: 0.40831 | Test Acc: 87.5000\n",
      "Epoch: 63 | Test Loss: 0.55334 | Test Acc: 75.0000\n",
      "Epoch: 63 | Test Loss: 0.44356 | Test Acc: 87.5000\n",
      "Epoch: 63 | Test Loss: 0.45591 | Test Acc: 87.5000\n",
      "Epoch: 63 | Test Loss: 0.37194 | Test Acc: 87.5000\n",
      "Epoch 64 of 200\n",
      "Epoch: 64 | Test Loss: 0.54179 | Test Acc: 70.8333\n",
      "Epoch: 64 | Test Loss: 0.32071 | Test Acc: 87.5000\n",
      "Epoch: 64 | Test Loss: 0.48489 | Test Acc: 75.0000\n",
      "Epoch: 64 | Test Loss: 0.52331 | Test Acc: 70.8333\n",
      "Epoch: 64 | Test Loss: 0.51191 | Test Acc: 79.1667\n",
      "Epoch: 64 | Test Loss: 0.45665 | Test Acc: 79.1667\n",
      "Epoch: 64 | Test Loss: 0.45184 | Test Acc: 91.6667\n",
      "Epoch: 64 | Test Loss: 0.54963 | Test Acc: 81.2500\n",
      "Epoch 65 of 200\n",
      "Epoch: 65 | Test Loss: 0.49962 | Test Acc: 79.1667\n",
      "Epoch: 65 | Test Loss: 0.52835 | Test Acc: 58.3333\n",
      "Epoch: 65 | Test Loss: 0.58108 | Test Acc: 70.8333\n",
      "Epoch: 65 | Test Loss: 0.27373 | Test Acc: 91.6667\n",
      "Epoch: 65 | Test Loss: 0.37247 | Test Acc: 83.3333\n",
      "Epoch: 65 | Test Loss: 0.44069 | Test Acc: 79.1667\n",
      "Epoch: 65 | Test Loss: 0.40937 | Test Acc: 79.1667\n",
      "Epoch: 65 | Test Loss: 0.45666 | Test Acc: 75.0000\n",
      "Epoch 66 of 200\n",
      "Epoch: 66 | Test Loss: 0.52625 | Test Acc: 66.6667\n",
      "Epoch: 66 | Test Loss: 0.50397 | Test Acc: 70.8333\n",
      "Epoch: 66 | Test Loss: 0.59866 | Test Acc: 58.3333\n",
      "Epoch: 66 | Test Loss: 0.44796 | Test Acc: 75.0000\n",
      "Epoch: 66 | Test Loss: 0.43643 | Test Acc: 75.0000\n",
      "Epoch: 66 | Test Loss: 0.32710 | Test Acc: 91.6667\n",
      "Epoch: 66 | Test Loss: 0.53983 | Test Acc: 70.8333\n",
      "Epoch: 66 | Test Loss: 0.69803 | Test Acc: 56.2500\n",
      "Epoch 67 of 200\n",
      "Epoch: 67 | Test Loss: 0.45878 | Test Acc: 83.3333\n",
      "Epoch: 67 | Test Loss: 0.42532 | Test Acc: 83.3333\n",
      "Epoch: 67 | Test Loss: 0.39854 | Test Acc: 87.5000\n",
      "Epoch: 67 | Test Loss: 0.46420 | Test Acc: 83.3333\n",
      "Epoch: 67 | Test Loss: 0.43501 | Test Acc: 87.5000\n",
      "Epoch: 67 | Test Loss: 0.42491 | Test Acc: 87.5000\n",
      "Epoch: 67 | Test Loss: 0.48399 | Test Acc: 75.0000\n",
      "Epoch: 67 | Test Loss: 0.62953 | Test Acc: 62.5000\n",
      "Epoch 68 of 200\n",
      "Epoch: 68 | Test Loss: 0.39153 | Test Acc: 75.0000\n",
      "Epoch: 68 | Test Loss: 0.69007 | Test Acc: 62.5000\n",
      "Epoch: 68 | Test Loss: 0.40065 | Test Acc: 91.6667\n",
      "Epoch: 68 | Test Loss: 0.48326 | Test Acc: 83.3333\n",
      "Epoch: 68 | Test Loss: 0.27731 | Test Acc: 100.0000\n",
      "Epoch: 68 | Test Loss: 0.43748 | Test Acc: 79.1667\n",
      "Epoch: 68 | Test Loss: 0.47163 | Test Acc: 75.0000\n",
      "Epoch: 68 | Test Loss: 0.43779 | Test Acc: 81.2500\n",
      "Epoch 69 of 200\n",
      "Epoch: 69 | Test Loss: 0.48062 | Test Acc: 83.3333\n",
      "Epoch: 69 | Test Loss: 0.56590 | Test Acc: 83.3333\n",
      "Epoch: 69 | Test Loss: 0.32072 | Test Acc: 87.5000\n",
      "Epoch: 69 | Test Loss: 0.49236 | Test Acc: 70.8333\n",
      "Epoch: 69 | Test Loss: 0.47591 | Test Acc: 83.3333\n",
      "Epoch: 69 | Test Loss: 0.48528 | Test Acc: 79.1667\n",
      "Epoch: 69 | Test Loss: 0.49444 | Test Acc: 79.1667\n",
      "Epoch: 69 | Test Loss: 0.49708 | Test Acc: 75.0000\n",
      "Epoch 70 of 200\n",
      "Epoch: 70 | Test Loss: 0.48343 | Test Acc: 79.1667\n",
      "Epoch: 70 | Test Loss: 0.41046 | Test Acc: 87.5000\n",
      "Epoch: 70 | Test Loss: 0.69352 | Test Acc: 66.6667\n",
      "Epoch: 70 | Test Loss: 0.49514 | Test Acc: 70.8333\n",
      "Epoch: 70 | Test Loss: 0.48387 | Test Acc: 83.3333\n",
      "Epoch: 70 | Test Loss: 0.52268 | Test Acc: 66.6667\n",
      "Epoch: 70 | Test Loss: 0.43092 | Test Acc: 87.5000\n",
      "Epoch: 70 | Test Loss: 0.70409 | Test Acc: 68.7500\n",
      "Epoch 71 of 200\n",
      "Epoch: 71 | Test Loss: 0.58502 | Test Acc: 70.8333\n",
      "Epoch: 71 | Test Loss: 0.48005 | Test Acc: 70.8333\n",
      "Epoch: 71 | Test Loss: 0.44686 | Test Acc: 83.3333\n",
      "Epoch: 71 | Test Loss: 0.41279 | Test Acc: 79.1667\n",
      "Epoch: 71 | Test Loss: 0.38756 | Test Acc: 95.8333\n",
      "Epoch: 71 | Test Loss: 0.65375 | Test Acc: 70.8333\n",
      "Epoch: 71 | Test Loss: 0.47472 | Test Acc: 87.5000\n",
      "Epoch: 71 | Test Loss: 0.21720 | Test Acc: 100.0000\n",
      "Epoch 72 of 200\n",
      "Epoch: 72 | Test Loss: 0.57549 | Test Acc: 75.0000\n",
      "Epoch: 72 | Test Loss: 0.43634 | Test Acc: 83.3333\n",
      "Epoch: 72 | Test Loss: 0.46131 | Test Acc: 79.1667\n",
      "Epoch: 72 | Test Loss: 0.37789 | Test Acc: 83.3333\n",
      "Epoch: 72 | Test Loss: 0.48057 | Test Acc: 75.0000\n",
      "Epoch: 72 | Test Loss: 0.49957 | Test Acc: 75.0000\n",
      "Epoch: 72 | Test Loss: 0.40054 | Test Acc: 87.5000\n",
      "Epoch: 72 | Test Loss: 0.41237 | Test Acc: 87.5000\n",
      "Epoch 73 of 200\n",
      "Epoch: 73 | Test Loss: 0.46355 | Test Acc: 70.8333\n",
      "Epoch: 73 | Test Loss: 0.39501 | Test Acc: 91.6667\n",
      "Epoch: 73 | Test Loss: 0.49202 | Test Acc: 58.3333\n",
      "Epoch: 73 | Test Loss: 0.42320 | Test Acc: 83.3333\n",
      "Epoch: 73 | Test Loss: 0.54658 | Test Acc: 70.8333\n",
      "Epoch: 73 | Test Loss: 0.39623 | Test Acc: 83.3333\n",
      "Epoch: 73 | Test Loss: 0.54461 | Test Acc: 62.5000\n",
      "Epoch: 73 | Test Loss: 0.60820 | Test Acc: 81.2500\n",
      "Epoch 74 of 200\n",
      "Epoch: 74 | Test Loss: 0.61964 | Test Acc: 58.3333\n",
      "Epoch: 74 | Test Loss: 0.41276 | Test Acc: 79.1667\n",
      "Epoch: 74 | Test Loss: 0.48744 | Test Acc: 75.0000\n",
      "Epoch: 74 | Test Loss: 0.51290 | Test Acc: 79.1667\n",
      "Epoch: 74 | Test Loss: 0.63762 | Test Acc: 58.3333\n",
      "Epoch: 74 | Test Loss: 0.45144 | Test Acc: 79.1667\n",
      "Epoch: 74 | Test Loss: 0.45694 | Test Acc: 79.1667\n",
      "Epoch: 74 | Test Loss: 0.57217 | Test Acc: 75.0000\n",
      "Epoch 75 of 200\n",
      "Epoch: 75 | Test Loss: 0.51600 | Test Acc: 75.0000\n",
      "Epoch: 75 | Test Loss: 0.32683 | Test Acc: 95.8333\n",
      "Epoch: 75 | Test Loss: 0.45828 | Test Acc: 83.3333\n",
      "Epoch: 75 | Test Loss: 0.56762 | Test Acc: 75.0000\n",
      "Epoch: 75 | Test Loss: 0.41477 | Test Acc: 75.0000\n",
      "Epoch: 75 | Test Loss: 0.37707 | Test Acc: 91.6667\n",
      "Epoch: 75 | Test Loss: 0.46317 | Test Acc: 87.5000\n",
      "Epoch: 75 | Test Loss: 0.45883 | Test Acc: 81.2500\n",
      "Epoch 76 of 200\n",
      "Epoch: 76 | Test Loss: 0.45240 | Test Acc: 83.3333\n",
      "Epoch: 76 | Test Loss: 0.39077 | Test Acc: 87.5000\n",
      "Epoch: 76 | Test Loss: 0.48027 | Test Acc: 79.1667\n",
      "Epoch: 76 | Test Loss: 0.47239 | Test Acc: 79.1667\n",
      "Epoch: 76 | Test Loss: 0.46887 | Test Acc: 75.0000\n",
      "Epoch: 76 | Test Loss: 0.53899 | Test Acc: 62.5000\n",
      "Epoch: 76 | Test Loss: 0.45530 | Test Acc: 79.1667\n",
      "Epoch: 76 | Test Loss: 0.45541 | Test Acc: 75.0000\n",
      "Epoch 77 of 200\n",
      "Epoch: 77 | Test Loss: 0.45474 | Test Acc: 87.5000\n",
      "Epoch: 77 | Test Loss: 0.40155 | Test Acc: 79.1667\n",
      "Epoch: 77 | Test Loss: 0.47638 | Test Acc: 70.8333\n",
      "Epoch: 77 | Test Loss: 0.49574 | Test Acc: 79.1667\n",
      "Epoch: 77 | Test Loss: 0.52881 | Test Acc: 70.8333\n",
      "Epoch: 77 | Test Loss: 0.53527 | Test Acc: 83.3333\n",
      "Epoch: 77 | Test Loss: 0.41392 | Test Acc: 83.3333\n",
      "Epoch: 77 | Test Loss: 0.39965 | Test Acc: 87.5000\n",
      "Epoch 78 of 200\n",
      "Epoch: 78 | Test Loss: 0.44814 | Test Acc: 75.0000\n",
      "Epoch: 78 | Test Loss: 0.50794 | Test Acc: 70.8333\n",
      "Epoch: 78 | Test Loss: 0.53804 | Test Acc: 79.1667\n",
      "Epoch: 78 | Test Loss: 0.44456 | Test Acc: 79.1667\n",
      "Epoch: 78 | Test Loss: 0.44994 | Test Acc: 91.6667\n",
      "Epoch: 78 | Test Loss: 0.33887 | Test Acc: 91.6667\n",
      "Epoch: 78 | Test Loss: 0.52077 | Test Acc: 75.0000\n",
      "Epoch: 78 | Test Loss: 0.29053 | Test Acc: 93.7500\n",
      "Epoch 79 of 200\n",
      "Epoch: 79 | Test Loss: 0.51102 | Test Acc: 83.3333\n",
      "Epoch: 79 | Test Loss: 0.34141 | Test Acc: 91.6667\n",
      "Epoch: 79 | Test Loss: 0.50866 | Test Acc: 87.5000\n",
      "Epoch: 79 | Test Loss: 0.46362 | Test Acc: 79.1667\n",
      "Epoch: 79 | Test Loss: 0.38505 | Test Acc: 91.6667\n",
      "Epoch: 79 | Test Loss: 0.53647 | Test Acc: 66.6667\n",
      "Epoch: 79 | Test Loss: 0.48792 | Test Acc: 79.1667\n",
      "Epoch: 79 | Test Loss: 0.67665 | Test Acc: 56.2500\n",
      "Epoch 80 of 200\n",
      "Epoch: 80 | Test Loss: 0.49524 | Test Acc: 87.5000\n",
      "Epoch: 80 | Test Loss: 0.45105 | Test Acc: 75.0000\n",
      "Epoch: 80 | Test Loss: 0.57785 | Test Acc: 70.8333\n",
      "Epoch: 80 | Test Loss: 0.41432 | Test Acc: 79.1667\n",
      "Epoch: 80 | Test Loss: 0.41799 | Test Acc: 83.3333\n",
      "Epoch: 80 | Test Loss: 0.38288 | Test Acc: 87.5000\n",
      "Epoch: 80 | Test Loss: 0.48161 | Test Acc: 75.0000\n",
      "Epoch: 80 | Test Loss: 0.33029 | Test Acc: 75.0000\n",
      "Epoch 81 of 200\n",
      "Epoch: 81 | Test Loss: 0.48543 | Test Acc: 83.3333\n",
      "Epoch: 81 | Test Loss: 0.38344 | Test Acc: 100.0000\n",
      "Epoch: 81 | Test Loss: 0.43181 | Test Acc: 87.5000\n",
      "Epoch: 81 | Test Loss: 0.52784 | Test Acc: 66.6667\n",
      "Epoch: 81 | Test Loss: 0.41431 | Test Acc: 79.1667\n",
      "Epoch: 81 | Test Loss: 0.49017 | Test Acc: 79.1667\n",
      "Epoch: 81 | Test Loss: 0.52878 | Test Acc: 66.6667\n",
      "Epoch: 81 | Test Loss: 0.49895 | Test Acc: 68.7500\n",
      "Epoch 82 of 200\n",
      "Epoch: 82 | Test Loss: 0.59861 | Test Acc: 58.3333\n",
      "Epoch: 82 | Test Loss: 0.49671 | Test Acc: 75.0000\n",
      "Epoch: 82 | Test Loss: 0.33693 | Test Acc: 95.8333\n",
      "Epoch: 82 | Test Loss: 0.63405 | Test Acc: 75.0000\n",
      "Epoch: 82 | Test Loss: 0.43363 | Test Acc: 87.5000\n",
      "Epoch: 82 | Test Loss: 0.42234 | Test Acc: 83.3333\n",
      "Epoch: 82 | Test Loss: 0.44770 | Test Acc: 83.3333\n",
      "Epoch: 82 | Test Loss: 0.24937 | Test Acc: 93.7500\n",
      "Epoch 83 of 200\n",
      "Epoch: 83 | Test Loss: 0.42690 | Test Acc: 79.1667\n",
      "Epoch: 83 | Test Loss: 0.38947 | Test Acc: 87.5000\n",
      "Epoch: 83 | Test Loss: 0.60820 | Test Acc: 62.5000\n",
      "Epoch: 83 | Test Loss: 0.47404 | Test Acc: 75.0000\n",
      "Epoch: 83 | Test Loss: 0.32352 | Test Acc: 91.6667\n",
      "Epoch: 83 | Test Loss: 0.50757 | Test Acc: 75.0000\n",
      "Epoch: 83 | Test Loss: 0.43342 | Test Acc: 87.5000\n",
      "Epoch: 83 | Test Loss: 0.58241 | Test Acc: 87.5000\n",
      "Epoch 84 of 200\n",
      "Epoch: 84 | Test Loss: 0.41070 | Test Acc: 79.1667\n",
      "Epoch: 84 | Test Loss: 0.41358 | Test Acc: 87.5000\n",
      "Epoch: 84 | Test Loss: 0.46457 | Test Acc: 79.1667\n",
      "Epoch: 84 | Test Loss: 0.45022 | Test Acc: 75.0000\n",
      "Epoch: 84 | Test Loss: 0.35211 | Test Acc: 91.6667\n",
      "Epoch: 84 | Test Loss: 0.43606 | Test Acc: 87.5000\n",
      "Epoch: 84 | Test Loss: 0.44801 | Test Acc: 83.3333\n",
      "Epoch: 84 | Test Loss: 0.67051 | Test Acc: 62.5000\n",
      "Epoch 85 of 200\n",
      "Epoch: 85 | Test Loss: 0.53279 | Test Acc: 58.3333\n",
      "Epoch: 85 | Test Loss: 0.48886 | Test Acc: 79.1667\n",
      "Epoch: 85 | Test Loss: 0.40857 | Test Acc: 91.6667\n",
      "Epoch: 85 | Test Loss: 0.34341 | Test Acc: 91.6667\n",
      "Epoch: 85 | Test Loss: 0.39430 | Test Acc: 91.6667\n",
      "Epoch: 85 | Test Loss: 0.43174 | Test Acc: 83.3333\n",
      "Epoch: 85 | Test Loss: 0.55446 | Test Acc: 58.3333\n",
      "Epoch: 85 | Test Loss: 0.70554 | Test Acc: 56.2500\n",
      "Epoch 86 of 200\n",
      "Epoch: 86 | Test Loss: 0.48078 | Test Acc: 79.1667\n",
      "Epoch: 86 | Test Loss: 0.40021 | Test Acc: 83.3333\n",
      "Epoch: 86 | Test Loss: 0.45424 | Test Acc: 75.0000\n",
      "Epoch: 86 | Test Loss: 0.67682 | Test Acc: 62.5000\n",
      "Epoch: 86 | Test Loss: 0.54123 | Test Acc: 70.8333\n",
      "Epoch: 86 | Test Loss: 0.39193 | Test Acc: 87.5000\n",
      "Epoch: 86 | Test Loss: 0.35579 | Test Acc: 91.6667\n",
      "Epoch: 86 | Test Loss: 0.43661 | Test Acc: 81.2500\n",
      "Epoch 87 of 200\n",
      "Epoch: 87 | Test Loss: 0.50551 | Test Acc: 79.1667\n",
      "Epoch: 87 | Test Loss: 0.38311 | Test Acc: 83.3333\n",
      "Epoch: 87 | Test Loss: 0.44732 | Test Acc: 83.3333\n",
      "Epoch: 87 | Test Loss: 0.50539 | Test Acc: 83.3333\n",
      "Epoch: 87 | Test Loss: 0.44627 | Test Acc: 83.3333\n",
      "Epoch: 87 | Test Loss: 0.35590 | Test Acc: 83.3333\n",
      "Epoch: 87 | Test Loss: 0.45829 | Test Acc: 70.8333\n",
      "Epoch: 87 | Test Loss: 0.59877 | Test Acc: 68.7500\n",
      "Epoch 88 of 200\n",
      "Epoch: 88 | Test Loss: 0.59342 | Test Acc: 70.8333\n",
      "Epoch: 88 | Test Loss: 0.48316 | Test Acc: 79.1667\n",
      "Epoch: 88 | Test Loss: 0.46919 | Test Acc: 75.0000\n",
      "Epoch: 88 | Test Loss: 0.39980 | Test Acc: 79.1667\n",
      "Epoch: 88 | Test Loss: 0.42976 | Test Acc: 87.5000\n",
      "Epoch: 88 | Test Loss: 0.40300 | Test Acc: 83.3333\n",
      "Epoch: 88 | Test Loss: 0.42007 | Test Acc: 83.3333\n",
      "Epoch: 88 | Test Loss: 0.48841 | Test Acc: 68.7500\n",
      "Epoch 89 of 200\n",
      "Epoch: 89 | Test Loss: 0.55210 | Test Acc: 83.3333\n",
      "Epoch: 89 | Test Loss: 0.54095 | Test Acc: 70.8333\n",
      "Epoch: 89 | Test Loss: 0.49006 | Test Acc: 66.6667\n",
      "Epoch: 89 | Test Loss: 0.35591 | Test Acc: 91.6667\n",
      "Epoch: 89 | Test Loss: 0.44505 | Test Acc: 79.1667\n",
      "Epoch: 89 | Test Loss: 0.61234 | Test Acc: 66.6667\n",
      "Epoch: 89 | Test Loss: 0.51078 | Test Acc: 75.0000\n",
      "Epoch: 89 | Test Loss: 0.40855 | Test Acc: 81.2500\n",
      "Epoch 90 of 200\n",
      "Epoch: 90 | Test Loss: 0.49746 | Test Acc: 75.0000\n",
      "Epoch: 90 | Test Loss: 0.45456 | Test Acc: 79.1667\n",
      "Epoch: 90 | Test Loss: 0.35849 | Test Acc: 91.6667\n",
      "Epoch: 90 | Test Loss: 0.28757 | Test Acc: 95.8333\n",
      "Epoch: 90 | Test Loss: 0.53090 | Test Acc: 66.6667\n",
      "Epoch: 90 | Test Loss: 0.42346 | Test Acc: 83.3333\n",
      "Epoch: 90 | Test Loss: 0.68152 | Test Acc: 75.0000\n",
      "Epoch: 90 | Test Loss: 0.44942 | Test Acc: 87.5000\n",
      "Epoch 91 of 200\n",
      "Epoch: 91 | Test Loss: 0.57358 | Test Acc: 79.1667\n",
      "Epoch: 91 | Test Loss: 0.27695 | Test Acc: 95.8333\n",
      "Epoch: 91 | Test Loss: 0.36975 | Test Acc: 83.3333\n",
      "Epoch: 91 | Test Loss: 0.39805 | Test Acc: 91.6667\n",
      "Epoch: 91 | Test Loss: 0.42901 | Test Acc: 75.0000\n",
      "Epoch: 91 | Test Loss: 0.50857 | Test Acc: 75.0000\n",
      "Epoch: 91 | Test Loss: 0.52699 | Test Acc: 83.3333\n",
      "Epoch: 91 | Test Loss: 0.35152 | Test Acc: 87.5000\n",
      "Epoch 92 of 200\n",
      "Epoch: 92 | Test Loss: 0.41257 | Test Acc: 87.5000\n",
      "Epoch: 92 | Test Loss: 0.31477 | Test Acc: 95.8333\n",
      "Epoch: 92 | Test Loss: 0.48857 | Test Acc: 75.0000\n",
      "Epoch: 92 | Test Loss: 0.56207 | Test Acc: 83.3333\n",
      "Epoch: 92 | Test Loss: 0.42891 | Test Acc: 87.5000\n",
      "Epoch: 92 | Test Loss: 0.44253 | Test Acc: 79.1667\n",
      "Epoch: 92 | Test Loss: 0.36382 | Test Acc: 87.5000\n",
      "Epoch: 92 | Test Loss: 0.45726 | Test Acc: 81.2500\n",
      "Epoch 93 of 200\n",
      "Epoch: 93 | Test Loss: 0.45521 | Test Acc: 79.1667\n",
      "Epoch: 93 | Test Loss: 0.43361 | Test Acc: 75.0000\n",
      "Epoch: 93 | Test Loss: 0.38230 | Test Acc: 87.5000\n",
      "Epoch: 93 | Test Loss: 0.45574 | Test Acc: 79.1667\n",
      "Epoch: 93 | Test Loss: 0.49561 | Test Acc: 79.1667\n",
      "Epoch: 93 | Test Loss: 0.46558 | Test Acc: 75.0000\n",
      "Epoch: 93 | Test Loss: 0.49310 | Test Acc: 70.8333\n",
      "Epoch: 93 | Test Loss: 0.55882 | Test Acc: 62.5000\n",
      "Epoch 94 of 200\n",
      "Epoch: 94 | Test Loss: 0.54799 | Test Acc: 70.8333\n",
      "Epoch: 94 | Test Loss: 0.39631 | Test Acc: 75.0000\n",
      "Epoch: 94 | Test Loss: 0.47284 | Test Acc: 75.0000\n",
      "Epoch: 94 | Test Loss: 0.54975 | Test Acc: 75.0000\n",
      "Epoch: 94 | Test Loss: 0.38775 | Test Acc: 91.6667\n",
      "Epoch: 94 | Test Loss: 0.46855 | Test Acc: 91.6667\n",
      "Epoch: 94 | Test Loss: 0.40695 | Test Acc: 79.1667\n",
      "Epoch: 94 | Test Loss: 0.47487 | Test Acc: 68.7500\n",
      "Epoch 95 of 200\n",
      "Epoch: 95 | Test Loss: 0.38230 | Test Acc: 87.5000\n",
      "Epoch: 95 | Test Loss: 0.41246 | Test Acc: 79.1667\n",
      "Epoch: 95 | Test Loss: 0.45691 | Test Acc: 79.1667\n",
      "Epoch: 95 | Test Loss: 0.47370 | Test Acc: 83.3333\n",
      "Epoch: 95 | Test Loss: 0.63743 | Test Acc: 62.5000\n",
      "Epoch: 95 | Test Loss: 0.55897 | Test Acc: 70.8333\n",
      "Epoch: 95 | Test Loss: 0.42764 | Test Acc: 79.1667\n",
      "Epoch: 95 | Test Loss: 0.47557 | Test Acc: 75.0000\n",
      "Epoch 96 of 200\n",
      "Epoch: 96 | Test Loss: 0.40155 | Test Acc: 87.5000\n",
      "Epoch: 96 | Test Loss: 0.48583 | Test Acc: 87.5000\n",
      "Epoch: 96 | Test Loss: 0.47490 | Test Acc: 91.6667\n",
      "Epoch: 96 | Test Loss: 0.31372 | Test Acc: 91.6667\n",
      "Epoch: 96 | Test Loss: 0.53127 | Test Acc: 70.8333\n",
      "Epoch: 96 | Test Loss: 0.38336 | Test Acc: 83.3333\n",
      "Epoch: 96 | Test Loss: 0.44587 | Test Acc: 79.1667\n",
      "Epoch: 96 | Test Loss: 0.55205 | Test Acc: 75.0000\n",
      "Epoch 97 of 200\n",
      "Epoch: 97 | Test Loss: 0.54675 | Test Acc: 70.8333\n",
      "Epoch: 97 | Test Loss: 0.53410 | Test Acc: 58.3333\n",
      "Epoch: 97 | Test Loss: 0.42843 | Test Acc: 83.3333\n",
      "Epoch: 97 | Test Loss: 0.42686 | Test Acc: 87.5000\n",
      "Epoch: 97 | Test Loss: 0.46295 | Test Acc: 62.5000\n",
      "Epoch: 97 | Test Loss: 0.64336 | Test Acc: 54.1667\n",
      "Epoch: 97 | Test Loss: 0.42948 | Test Acc: 83.3333\n",
      "Epoch: 97 | Test Loss: 0.49648 | Test Acc: 75.0000\n",
      "Epoch 98 of 200\n",
      "Epoch: 98 | Test Loss: 0.40199 | Test Acc: 87.5000\n",
      "Epoch: 98 | Test Loss: 0.42297 | Test Acc: 83.3333\n",
      "Epoch: 98 | Test Loss: 0.27257 | Test Acc: 95.8333\n",
      "Epoch: 98 | Test Loss: 0.47050 | Test Acc: 87.5000\n",
      "Epoch: 98 | Test Loss: 0.50739 | Test Acc: 58.3333\n",
      "Epoch: 98 | Test Loss: 0.60025 | Test Acc: 54.1667\n",
      "Epoch: 98 | Test Loss: 0.35980 | Test Acc: 87.5000\n",
      "Epoch: 98 | Test Loss: 0.38360 | Test Acc: 87.5000\n",
      "Epoch 99 of 200\n",
      "Epoch: 99 | Test Loss: 0.50130 | Test Acc: 70.8333\n",
      "Epoch: 99 | Test Loss: 0.53072 | Test Acc: 75.0000\n",
      "Epoch: 99 | Test Loss: 0.55082 | Test Acc: 58.3333\n",
      "Epoch: 99 | Test Loss: 0.48662 | Test Acc: 75.0000\n",
      "Epoch: 99 | Test Loss: 0.48822 | Test Acc: 79.1667\n",
      "Epoch: 99 | Test Loss: 0.51737 | Test Acc: 70.8333\n",
      "Epoch: 99 | Test Loss: 0.44498 | Test Acc: 87.5000\n",
      "Epoch: 99 | Test Loss: 0.50300 | Test Acc: 68.7500\n",
      "Epoch 100 of 200\n",
      "Epoch: 100 | Test Loss: 0.42972 | Test Acc: 87.5000\n",
      "Epoch: 100 | Test Loss: 0.48306 | Test Acc: 70.8333\n",
      "Epoch: 100 | Test Loss: 0.42708 | Test Acc: 83.3333\n",
      "Epoch: 100 | Test Loss: 0.56207 | Test Acc: 79.1667\n",
      "Epoch: 100 | Test Loss: 0.51939 | Test Acc: 66.6667\n",
      "Epoch: 100 | Test Loss: 0.48919 | Test Acc: 70.8333\n",
      "Epoch: 100 | Test Loss: 0.45054 | Test Acc: 79.1667\n",
      "Epoch: 100 | Test Loss: 0.32945 | Test Acc: 87.5000\n",
      "Epoch 101 of 200\n",
      "Epoch: 101 | Test Loss: 0.47108 | Test Acc: 75.0000\n",
      "Epoch: 101 | Test Loss: 0.39885 | Test Acc: 87.5000\n",
      "Epoch: 101 | Test Loss: 0.43924 | Test Acc: 95.8333\n",
      "Epoch: 101 | Test Loss: 0.42693 | Test Acc: 95.8333\n",
      "Epoch: 101 | Test Loss: 0.50598 | Test Acc: 75.0000\n",
      "Epoch: 101 | Test Loss: 0.36494 | Test Acc: 87.5000\n",
      "Epoch: 101 | Test Loss: 0.48195 | Test Acc: 79.1667\n",
      "Epoch: 101 | Test Loss: 0.49694 | Test Acc: 75.0000\n",
      "Epoch 102 of 200\n",
      "Epoch: 102 | Test Loss: 0.41925 | Test Acc: 83.3333\n",
      "Epoch: 102 | Test Loss: 0.43317 | Test Acc: 91.6667\n",
      "Epoch: 102 | Test Loss: 0.63406 | Test Acc: 70.8333\n",
      "Epoch: 102 | Test Loss: 0.53521 | Test Acc: 54.1667\n",
      "Epoch: 102 | Test Loss: 0.39995 | Test Acc: 87.5000\n",
      "Epoch: 102 | Test Loss: 0.43591 | Test Acc: 91.6667\n",
      "Epoch: 102 | Test Loss: 0.50845 | Test Acc: 70.8333\n",
      "Epoch: 102 | Test Loss: 0.46921 | Test Acc: 81.2500\n",
      "Epoch 103 of 200\n",
      "Epoch: 103 | Test Loss: 0.62989 | Test Acc: 66.6667\n",
      "Epoch: 103 | Test Loss: 0.56685 | Test Acc: 83.3333\n",
      "Epoch: 103 | Test Loss: 0.58743 | Test Acc: 58.3333\n",
      "Epoch: 103 | Test Loss: 0.33255 | Test Acc: 91.6667\n",
      "Epoch: 103 | Test Loss: 0.38938 | Test Acc: 83.3333\n",
      "Epoch: 103 | Test Loss: 0.58354 | Test Acc: 62.5000\n",
      "Epoch: 103 | Test Loss: 0.44863 | Test Acc: 83.3333\n",
      "Epoch: 103 | Test Loss: 0.48206 | Test Acc: 68.7500\n",
      "Epoch 104 of 200\n",
      "Epoch: 104 | Test Loss: 0.43902 | Test Acc: 70.8333\n",
      "Epoch: 104 | Test Loss: 0.56191 | Test Acc: 66.6667\n",
      "Epoch: 104 | Test Loss: 0.38732 | Test Acc: 87.5000\n",
      "Epoch: 104 | Test Loss: 0.52453 | Test Acc: 79.1667\n",
      "Epoch: 104 | Test Loss: 0.40805 | Test Acc: 87.5000\n",
      "Epoch: 104 | Test Loss: 0.38514 | Test Acc: 87.5000\n",
      "Epoch: 104 | Test Loss: 0.44714 | Test Acc: 83.3333\n",
      "Epoch: 104 | Test Loss: 0.33965 | Test Acc: 87.5000\n",
      "Epoch 105 of 200\n",
      "Epoch: 105 | Test Loss: 0.60749 | Test Acc: 66.6667\n",
      "Epoch: 105 | Test Loss: 0.52107 | Test Acc: 58.3333\n",
      "Epoch: 105 | Test Loss: 0.55079 | Test Acc: 66.6667\n",
      "Epoch: 105 | Test Loss: 0.58774 | Test Acc: 70.8333\n",
      "Epoch: 105 | Test Loss: 0.56057 | Test Acc: 62.5000\n",
      "Epoch: 105 | Test Loss: 0.51103 | Test Acc: 83.3333\n",
      "Epoch: 105 | Test Loss: 0.41125 | Test Acc: 79.1667\n",
      "Epoch: 105 | Test Loss: 0.27579 | Test Acc: 93.7500\n",
      "Epoch 106 of 200\n",
      "Epoch: 106 | Test Loss: 0.49731 | Test Acc: 66.6667\n",
      "Epoch: 106 | Test Loss: 0.49340 | Test Acc: 79.1667\n",
      "Epoch: 106 | Test Loss: 0.38357 | Test Acc: 91.6667\n",
      "Epoch: 106 | Test Loss: 0.45449 | Test Acc: 79.1667\n",
      "Epoch: 106 | Test Loss: 0.35497 | Test Acc: 87.5000\n",
      "Epoch: 106 | Test Loss: 0.49192 | Test Acc: 75.0000\n",
      "Epoch: 106 | Test Loss: 0.38894 | Test Acc: 87.5000\n",
      "Epoch: 106 | Test Loss: 0.54058 | Test Acc: 75.0000\n",
      "Epoch 107 of 200\n",
      "Epoch: 107 | Test Loss: 0.35226 | Test Acc: 83.3333\n",
      "Epoch: 107 | Test Loss: 0.34025 | Test Acc: 91.6667\n",
      "Epoch: 107 | Test Loss: 0.50536 | Test Acc: 75.0000\n",
      "Epoch: 107 | Test Loss: 0.37595 | Test Acc: 91.6667\n",
      "Epoch: 107 | Test Loss: 0.49343 | Test Acc: 83.3333\n",
      "Epoch: 107 | Test Loss: 0.41711 | Test Acc: 83.3333\n",
      "Epoch: 107 | Test Loss: 0.62847 | Test Acc: 58.3333\n",
      "Epoch: 107 | Test Loss: 0.50712 | Test Acc: 75.0000\n",
      "Epoch 108 of 200\n",
      "Epoch: 108 | Test Loss: 0.40370 | Test Acc: 91.6667\n",
      "Epoch: 108 | Test Loss: 0.42262 | Test Acc: 79.1667\n",
      "Epoch: 108 | Test Loss: 0.52777 | Test Acc: 66.6667\n",
      "Epoch: 108 | Test Loss: 0.44109 | Test Acc: 87.5000\n",
      "Epoch: 108 | Test Loss: 0.51616 | Test Acc: 70.8333\n",
      "Epoch: 108 | Test Loss: 0.55454 | Test Acc: 58.3333\n",
      "Epoch: 108 | Test Loss: 0.54136 | Test Acc: 70.8333\n",
      "Epoch: 108 | Test Loss: 0.37480 | Test Acc: 87.5000\n",
      "Epoch 109 of 200\n",
      "Epoch: 109 | Test Loss: 0.47243 | Test Acc: 83.3333\n",
      "Epoch: 109 | Test Loss: 0.57871 | Test Acc: 62.5000\n",
      "Epoch: 109 | Test Loss: 0.52279 | Test Acc: 83.3333\n",
      "Epoch: 109 | Test Loss: 0.37315 | Test Acc: 87.5000\n",
      "Epoch: 109 | Test Loss: 0.31982 | Test Acc: 87.5000\n",
      "Epoch: 109 | Test Loss: 0.40331 | Test Acc: 83.3333\n",
      "Epoch: 109 | Test Loss: 0.45318 | Test Acc: 83.3333\n",
      "Epoch: 109 | Test Loss: 0.41366 | Test Acc: 81.2500\n",
      "Epoch 110 of 200\n",
      "Epoch: 110 | Test Loss: 0.51543 | Test Acc: 79.1667\n",
      "Epoch: 110 | Test Loss: 0.32804 | Test Acc: 91.6667\n",
      "Epoch: 110 | Test Loss: 0.44527 | Test Acc: 79.1667\n",
      "Epoch: 110 | Test Loss: 0.39122 | Test Acc: 79.1667\n",
      "Epoch: 110 | Test Loss: 0.35285 | Test Acc: 87.5000\n",
      "Epoch: 110 | Test Loss: 0.57859 | Test Acc: 66.6667\n",
      "Epoch: 110 | Test Loss: 0.44191 | Test Acc: 91.6667\n",
      "Epoch: 110 | Test Loss: 0.64994 | Test Acc: 50.0000\n",
      "Epoch 111 of 200\n",
      "Epoch: 111 | Test Loss: 0.46028 | Test Acc: 75.0000\n",
      "Epoch: 111 | Test Loss: 0.51281 | Test Acc: 87.5000\n",
      "Epoch: 111 | Test Loss: 0.46389 | Test Acc: 83.3333\n",
      "Epoch: 111 | Test Loss: 0.43128 | Test Acc: 75.0000\n",
      "Epoch: 111 | Test Loss: 0.53921 | Test Acc: 62.5000\n",
      "Epoch: 111 | Test Loss: 0.50325 | Test Acc: 62.5000\n",
      "Epoch: 111 | Test Loss: 0.46594 | Test Acc: 75.0000\n",
      "Epoch: 111 | Test Loss: 0.54640 | Test Acc: 68.7500\n",
      "Epoch 112 of 200\n",
      "Epoch: 112 | Test Loss: 0.49255 | Test Acc: 62.5000\n",
      "Epoch: 112 | Test Loss: 0.40694 | Test Acc: 91.6667\n",
      "Epoch: 112 | Test Loss: 0.49708 | Test Acc: 75.0000\n",
      "Epoch: 112 | Test Loss: 0.56326 | Test Acc: 50.0000\n",
      "Epoch: 112 | Test Loss: 0.47241 | Test Acc: 75.0000\n",
      "Epoch: 112 | Test Loss: 0.47086 | Test Acc: 79.1667\n",
      "Epoch: 112 | Test Loss: 0.36189 | Test Acc: 87.5000\n",
      "Epoch: 112 | Test Loss: 0.48940 | Test Acc: 68.7500\n",
      "Epoch 113 of 200\n",
      "Epoch: 113 | Test Loss: 0.40050 | Test Acc: 83.3333\n",
      "Epoch: 113 | Test Loss: 0.42525 | Test Acc: 87.5000\n",
      "Epoch: 113 | Test Loss: 0.48552 | Test Acc: 70.8333\n",
      "Epoch: 113 | Test Loss: 0.46415 | Test Acc: 79.1667\n",
      "Epoch: 113 | Test Loss: 0.46684 | Test Acc: 75.0000\n",
      "Epoch: 113 | Test Loss: 0.62772 | Test Acc: 62.5000\n",
      "Epoch: 113 | Test Loss: 0.48607 | Test Acc: 66.6667\n",
      "Epoch: 113 | Test Loss: 0.57151 | Test Acc: 62.5000\n",
      "Epoch 114 of 200\n",
      "Epoch: 114 | Test Loss: 0.39235 | Test Acc: 87.5000\n",
      "Epoch: 114 | Test Loss: 0.43214 | Test Acc: 79.1667\n",
      "Epoch: 114 | Test Loss: 0.38974 | Test Acc: 87.5000\n",
      "Epoch: 114 | Test Loss: 0.49846 | Test Acc: 79.1667\n",
      "Epoch: 114 | Test Loss: 0.41825 | Test Acc: 83.3333\n",
      "Epoch: 114 | Test Loss: 0.37155 | Test Acc: 87.5000\n",
      "Epoch: 114 | Test Loss: 0.51728 | Test Acc: 79.1667\n",
      "Epoch: 114 | Test Loss: 0.44746 | Test Acc: 87.5000\n",
      "Epoch 115 of 200\n",
      "Epoch: 115 | Test Loss: 0.38282 | Test Acc: 87.5000\n",
      "Epoch: 115 | Test Loss: 0.44155 | Test Acc: 87.5000\n",
      "Epoch: 115 | Test Loss: 0.40637 | Test Acc: 83.3333\n",
      "Epoch: 115 | Test Loss: 0.55134 | Test Acc: 62.5000\n",
      "Epoch: 115 | Test Loss: 0.47510 | Test Acc: 75.0000\n",
      "Epoch: 115 | Test Loss: 0.40053 | Test Acc: 83.3333\n",
      "Epoch: 115 | Test Loss: 0.60292 | Test Acc: 66.6667\n",
      "Epoch: 115 | Test Loss: 0.45339 | Test Acc: 75.0000\n",
      "Epoch 116 of 200\n",
      "Epoch: 116 | Test Loss: 0.48000 | Test Acc: 75.0000\n",
      "Epoch: 116 | Test Loss: 0.34040 | Test Acc: 91.6667\n",
      "Epoch: 116 | Test Loss: 0.49033 | Test Acc: 75.0000\n",
      "Epoch: 116 | Test Loss: 0.52094 | Test Acc: 70.8333\n",
      "Epoch: 116 | Test Loss: 0.48155 | Test Acc: 79.1667\n",
      "Epoch: 116 | Test Loss: 0.52247 | Test Acc: 79.1667\n",
      "Epoch: 116 | Test Loss: 0.47521 | Test Acc: 83.3333\n",
      "Epoch: 116 | Test Loss: 0.57723 | Test Acc: 68.7500\n",
      "Epoch 117 of 200\n",
      "Epoch: 117 | Test Loss: 0.34781 | Test Acc: 91.6667\n",
      "Epoch: 117 | Test Loss: 0.58508 | Test Acc: 83.3333\n",
      "Epoch: 117 | Test Loss: 0.55238 | Test Acc: 79.1667\n",
      "Epoch: 117 | Test Loss: 0.48406 | Test Acc: 66.6667\n",
      "Epoch: 117 | Test Loss: 0.40943 | Test Acc: 83.3333\n",
      "Epoch: 117 | Test Loss: 0.64582 | Test Acc: 54.1667\n",
      "Epoch: 117 | Test Loss: 0.42136 | Test Acc: 79.1667\n",
      "Epoch: 117 | Test Loss: 0.46085 | Test Acc: 93.7500\n",
      "Epoch 118 of 200\n",
      "Epoch: 118 | Test Loss: 0.57651 | Test Acc: 66.6667\n",
      "Epoch: 118 | Test Loss: 0.48206 | Test Acc: 79.1667\n",
      "Epoch: 118 | Test Loss: 0.51423 | Test Acc: 70.8333\n",
      "Epoch: 118 | Test Loss: 0.59569 | Test Acc: 70.8333\n",
      "Epoch: 118 | Test Loss: 0.53340 | Test Acc: 58.3333\n",
      "Epoch: 118 | Test Loss: 0.35313 | Test Acc: 87.5000\n",
      "Epoch: 118 | Test Loss: 0.58373 | Test Acc: 62.5000\n",
      "Epoch: 118 | Test Loss: 0.41012 | Test Acc: 81.2500\n",
      "Epoch 119 of 200\n",
      "Epoch: 119 | Test Loss: 0.47171 | Test Acc: 75.0000\n",
      "Epoch: 119 | Test Loss: 0.51943 | Test Acc: 70.8333\n",
      "Epoch: 119 | Test Loss: 0.59755 | Test Acc: 66.6667\n",
      "Epoch: 119 | Test Loss: 0.60776 | Test Acc: 62.5000\n",
      "Epoch: 119 | Test Loss: 0.42682 | Test Acc: 79.1667\n",
      "Epoch: 119 | Test Loss: 0.39062 | Test Acc: 87.5000\n",
      "Epoch: 119 | Test Loss: 0.55777 | Test Acc: 75.0000\n",
      "Epoch: 119 | Test Loss: 0.40731 | Test Acc: 75.0000\n",
      "Epoch 120 of 200\n",
      "Epoch: 120 | Test Loss: 0.36874 | Test Acc: 91.6667\n",
      "Epoch: 120 | Test Loss: 0.56454 | Test Acc: 79.1667\n",
      "Epoch: 120 | Test Loss: 0.52076 | Test Acc: 75.0000\n",
      "Epoch: 120 | Test Loss: 0.46557 | Test Acc: 79.1667\n",
      "Epoch: 120 | Test Loss: 0.43184 | Test Acc: 87.5000\n",
      "Epoch: 120 | Test Loss: 0.35077 | Test Acc: 91.6667\n",
      "Epoch: 120 | Test Loss: 0.19517 | Test Acc: 100.0000\n",
      "Epoch: 120 | Test Loss: 0.53710 | Test Acc: 68.7500\n",
      "Epoch 121 of 200\n",
      "Epoch: 121 | Test Loss: 0.37352 | Test Acc: 91.6667\n",
      "Epoch: 121 | Test Loss: 0.49484 | Test Acc: 79.1667\n",
      "Epoch: 121 | Test Loss: 0.52558 | Test Acc: 62.5000\n",
      "Epoch: 121 | Test Loss: 0.26985 | Test Acc: 95.8333\n",
      "Epoch: 121 | Test Loss: 0.57258 | Test Acc: 66.6667\n",
      "Epoch: 121 | Test Loss: 0.42067 | Test Acc: 83.3333\n",
      "Epoch: 121 | Test Loss: 0.48431 | Test Acc: 75.0000\n",
      "Epoch: 121 | Test Loss: 0.64759 | Test Acc: 68.7500\n",
      "Epoch 122 of 200\n",
      "Epoch: 122 | Test Loss: 0.57237 | Test Acc: 66.6667\n",
      "Epoch: 122 | Test Loss: 0.40260 | Test Acc: 79.1667\n",
      "Epoch: 122 | Test Loss: 0.34767 | Test Acc: 87.5000\n",
      "Epoch: 122 | Test Loss: 0.57780 | Test Acc: 62.5000\n",
      "Epoch: 122 | Test Loss: 0.43316 | Test Acc: 75.0000\n",
      "Epoch: 122 | Test Loss: 0.39143 | Test Acc: 79.1667\n",
      "Epoch: 122 | Test Loss: 0.57398 | Test Acc: 79.1667\n",
      "Epoch: 122 | Test Loss: 0.51711 | Test Acc: 81.2500\n",
      "Epoch 123 of 200\n",
      "Epoch: 123 | Test Loss: 0.66221 | Test Acc: 54.1667\n",
      "Epoch: 123 | Test Loss: 0.50549 | Test Acc: 83.3333\n",
      "Epoch: 123 | Test Loss: 0.56463 | Test Acc: 66.6667\n",
      "Epoch: 123 | Test Loss: 0.32763 | Test Acc: 91.6667\n",
      "Epoch: 123 | Test Loss: 0.41944 | Test Acc: 83.3333\n",
      "Epoch: 123 | Test Loss: 0.36581 | Test Acc: 91.6667\n",
      "Epoch: 123 | Test Loss: 0.38572 | Test Acc: 79.1667\n",
      "Epoch: 123 | Test Loss: 0.47674 | Test Acc: 87.5000\n",
      "Epoch 124 of 200\n",
      "Epoch: 124 | Test Loss: 0.50645 | Test Acc: 79.1667\n",
      "Epoch: 124 | Test Loss: 0.37540 | Test Acc: 83.3333\n",
      "Epoch: 124 | Test Loss: 0.53162 | Test Acc: 79.1667\n",
      "Epoch: 124 | Test Loss: 0.32976 | Test Acc: 83.3333\n",
      "Epoch: 124 | Test Loss: 0.56088 | Test Acc: 70.8333\n",
      "Epoch: 124 | Test Loss: 0.44002 | Test Acc: 91.6667\n",
      "Epoch: 124 | Test Loss: 0.49002 | Test Acc: 75.0000\n",
      "Epoch: 124 | Test Loss: 0.65695 | Test Acc: 68.7500\n",
      "Epoch 125 of 200\n",
      "Epoch: 125 | Test Loss: 0.38312 | Test Acc: 83.3333\n",
      "Epoch: 125 | Test Loss: 0.43969 | Test Acc: 75.0000\n",
      "Epoch: 125 | Test Loss: 0.52388 | Test Acc: 83.3333\n",
      "Epoch: 125 | Test Loss: 0.35592 | Test Acc: 83.3333\n",
      "Epoch: 125 | Test Loss: 0.48368 | Test Acc: 75.0000\n",
      "Epoch: 125 | Test Loss: 0.47685 | Test Acc: 75.0000\n",
      "Epoch: 125 | Test Loss: 0.45908 | Test Acc: 79.1667\n",
      "Epoch: 125 | Test Loss: 0.57742 | Test Acc: 62.5000\n",
      "Epoch 126 of 200\n",
      "Epoch: 126 | Test Loss: 0.58966 | Test Acc: 66.6667\n",
      "Epoch: 126 | Test Loss: 0.62314 | Test Acc: 58.3333\n",
      "Epoch: 126 | Test Loss: 0.50770 | Test Acc: 70.8333\n",
      "Epoch: 126 | Test Loss: 0.47283 | Test Acc: 75.0000\n",
      "Epoch: 126 | Test Loss: 0.41194 | Test Acc: 87.5000\n",
      "Epoch: 126 | Test Loss: 0.36966 | Test Acc: 83.3333\n",
      "Epoch: 126 | Test Loss: 0.44341 | Test Acc: 79.1667\n",
      "Epoch: 126 | Test Loss: 0.37430 | Test Acc: 87.5000\n",
      "Epoch 127 of 200\n",
      "Epoch: 127 | Test Loss: 0.47821 | Test Acc: 75.0000\n",
      "Epoch: 127 | Test Loss: 0.63299 | Test Acc: 66.6667\n",
      "Epoch: 127 | Test Loss: 0.45091 | Test Acc: 70.8333\n",
      "Epoch: 127 | Test Loss: 0.52733 | Test Acc: 83.3333\n",
      "Epoch: 127 | Test Loss: 0.41823 | Test Acc: 87.5000\n",
      "Epoch: 127 | Test Loss: 0.39828 | Test Acc: 87.5000\n",
      "Epoch: 127 | Test Loss: 0.52048 | Test Acc: 66.6667\n",
      "Epoch: 127 | Test Loss: 0.38123 | Test Acc: 87.5000\n",
      "Epoch 128 of 200\n",
      "Epoch: 128 | Test Loss: 0.57219 | Test Acc: 75.0000\n",
      "Epoch: 128 | Test Loss: 0.47325 | Test Acc: 70.8333\n",
      "Epoch: 128 | Test Loss: 0.32292 | Test Acc: 95.8333\n",
      "Epoch: 128 | Test Loss: 0.47249 | Test Acc: 83.3333\n",
      "Epoch: 128 | Test Loss: 0.45628 | Test Acc: 75.0000\n",
      "Epoch: 128 | Test Loss: 0.48997 | Test Acc: 83.3333\n",
      "Epoch: 128 | Test Loss: 0.44307 | Test Acc: 70.8333\n",
      "Epoch: 128 | Test Loss: 0.54052 | Test Acc: 68.7500\n",
      "Epoch 129 of 200\n",
      "Epoch: 129 | Test Loss: 0.45257 | Test Acc: 79.1667\n",
      "Epoch: 129 | Test Loss: 0.53669 | Test Acc: 79.1667\n",
      "Epoch: 129 | Test Loss: 0.49639 | Test Acc: 83.3333\n",
      "Epoch: 129 | Test Loss: 0.44052 | Test Acc: 70.8333\n",
      "Epoch: 129 | Test Loss: 0.45042 | Test Acc: 79.1667\n",
      "Epoch: 129 | Test Loss: 0.45262 | Test Acc: 75.0000\n",
      "Epoch: 129 | Test Loss: 0.46261 | Test Acc: 79.1667\n",
      "Epoch: 129 | Test Loss: 0.37001 | Test Acc: 75.0000\n",
      "Epoch 130 of 200\n",
      "Epoch: 130 | Test Loss: 0.40712 | Test Acc: 83.3333\n",
      "Epoch: 130 | Test Loss: 0.53863 | Test Acc: 70.8333\n",
      "Epoch: 130 | Test Loss: 0.52958 | Test Acc: 75.0000\n",
      "Epoch: 130 | Test Loss: 0.52849 | Test Acc: 66.6667\n",
      "Epoch: 130 | Test Loss: 0.42395 | Test Acc: 75.0000\n",
      "Epoch: 130 | Test Loss: 0.39489 | Test Acc: 87.5000\n",
      "Epoch: 130 | Test Loss: 0.47520 | Test Acc: 79.1667\n",
      "Epoch: 130 | Test Loss: 0.55460 | Test Acc: 75.0000\n",
      "Epoch 131 of 200\n",
      "Epoch: 131 | Test Loss: 0.58197 | Test Acc: 70.8333\n",
      "Epoch: 131 | Test Loss: 0.44723 | Test Acc: 70.8333\n",
      "Epoch: 131 | Test Loss: 0.65210 | Test Acc: 70.8333\n",
      "Epoch: 131 | Test Loss: 0.47765 | Test Acc: 83.3333\n",
      "Epoch: 131 | Test Loss: 0.48080 | Test Acc: 79.1667\n",
      "Epoch: 131 | Test Loss: 0.34453 | Test Acc: 83.3333\n",
      "Epoch: 131 | Test Loss: 0.63409 | Test Acc: 50.0000\n",
      "Epoch: 131 | Test Loss: 0.35110 | Test Acc: 81.2500\n",
      "Epoch 132 of 200\n",
      "Epoch: 132 | Test Loss: 0.44609 | Test Acc: 75.0000\n",
      "Epoch: 132 | Test Loss: 0.42025 | Test Acc: 75.0000\n",
      "Epoch: 132 | Test Loss: 0.48846 | Test Acc: 66.6667\n",
      "Epoch: 132 | Test Loss: 0.43070 | Test Acc: 83.3333\n",
      "Epoch: 132 | Test Loss: 0.38558 | Test Acc: 91.6667\n",
      "Epoch: 132 | Test Loss: 0.53286 | Test Acc: 70.8333\n",
      "Epoch: 132 | Test Loss: 0.42679 | Test Acc: 75.0000\n",
      "Epoch: 132 | Test Loss: 0.63009 | Test Acc: 50.0000\n",
      "Epoch 133 of 200\n",
      "Epoch: 133 | Test Loss: 0.61773 | Test Acc: 54.1667\n",
      "Epoch: 133 | Test Loss: 0.49724 | Test Acc: 75.0000\n",
      "Epoch: 133 | Test Loss: 0.34981 | Test Acc: 79.1667\n",
      "Epoch: 133 | Test Loss: 0.44534 | Test Acc: 79.1667\n",
      "Epoch: 133 | Test Loss: 0.48238 | Test Acc: 75.0000\n",
      "Epoch: 133 | Test Loss: 0.36185 | Test Acc: 83.3333\n",
      "Epoch: 133 | Test Loss: 0.57869 | Test Acc: 91.6667\n",
      "Epoch: 133 | Test Loss: 0.56509 | Test Acc: 68.7500\n",
      "Epoch 134 of 200\n",
      "Epoch: 134 | Test Loss: 0.46690 | Test Acc: 87.5000\n",
      "Epoch: 134 | Test Loss: 0.35087 | Test Acc: 91.6667\n",
      "Epoch: 134 | Test Loss: 0.36565 | Test Acc: 87.5000\n",
      "Epoch: 134 | Test Loss: 0.36275 | Test Acc: 91.6667\n",
      "Epoch: 134 | Test Loss: 0.50120 | Test Acc: 83.3333\n",
      "Epoch: 134 | Test Loss: 0.63547 | Test Acc: 66.6667\n",
      "Epoch: 134 | Test Loss: 0.40030 | Test Acc: 83.3333\n",
      "Epoch: 134 | Test Loss: 0.41756 | Test Acc: 87.5000\n",
      "Epoch 135 of 200\n",
      "Epoch: 135 | Test Loss: 0.39367 | Test Acc: 83.3333\n",
      "Epoch: 135 | Test Loss: 0.47135 | Test Acc: 87.5000\n",
      "Epoch: 135 | Test Loss: 0.45626 | Test Acc: 75.0000\n",
      "Epoch: 135 | Test Loss: 0.36591 | Test Acc: 79.1667\n",
      "Epoch: 135 | Test Loss: 0.31826 | Test Acc: 95.8333\n",
      "Epoch: 135 | Test Loss: 0.55871 | Test Acc: 62.5000\n",
      "Epoch: 135 | Test Loss: 0.46587 | Test Acc: 83.3333\n",
      "Epoch: 135 | Test Loss: 0.53067 | Test Acc: 68.7500\n",
      "Epoch 136 of 200\n",
      "Epoch: 136 | Test Loss: 0.59369 | Test Acc: 62.5000\n",
      "Epoch: 136 | Test Loss: 0.46550 | Test Acc: 79.1667\n",
      "Epoch: 136 | Test Loss: 0.44509 | Test Acc: 66.6667\n",
      "Epoch: 136 | Test Loss: 0.44022 | Test Acc: 83.3333\n",
      "Epoch: 136 | Test Loss: 0.43788 | Test Acc: 83.3333\n",
      "Epoch: 136 | Test Loss: 0.40605 | Test Acc: 83.3333\n",
      "Epoch: 136 | Test Loss: 0.42461 | Test Acc: 79.1667\n",
      "Epoch: 136 | Test Loss: 0.47524 | Test Acc: 68.7500\n",
      "Epoch 137 of 200\n",
      "Epoch: 137 | Test Loss: 0.53125 | Test Acc: 79.1667\n",
      "Epoch: 137 | Test Loss: 0.39554 | Test Acc: 79.1667\n",
      "Epoch: 137 | Test Loss: 0.37066 | Test Acc: 79.1667\n",
      "Epoch: 137 | Test Loss: 0.52062 | Test Acc: 79.1667\n",
      "Epoch: 137 | Test Loss: 0.51070 | Test Acc: 79.1667\n",
      "Epoch: 137 | Test Loss: 0.38063 | Test Acc: 83.3333\n",
      "Epoch: 137 | Test Loss: 0.38833 | Test Acc: 87.5000\n",
      "Epoch: 137 | Test Loss: 0.55605 | Test Acc: 75.0000\n",
      "Epoch 138 of 200\n",
      "Epoch: 138 | Test Loss: 0.40793 | Test Acc: 83.3333\n",
      "Epoch: 138 | Test Loss: 0.37294 | Test Acc: 79.1667\n",
      "Epoch: 138 | Test Loss: 0.58113 | Test Acc: 70.8333\n",
      "Epoch: 138 | Test Loss: 0.44645 | Test Acc: 83.3333\n",
      "Epoch: 138 | Test Loss: 0.54879 | Test Acc: 62.5000\n",
      "Epoch: 138 | Test Loss: 0.50150 | Test Acc: 79.1667\n",
      "Epoch: 138 | Test Loss: 0.45184 | Test Acc: 75.0000\n",
      "Epoch: 138 | Test Loss: 0.53941 | Test Acc: 75.0000\n",
      "Epoch 139 of 200\n",
      "Epoch: 139 | Test Loss: 0.44011 | Test Acc: 75.0000\n",
      "Epoch: 139 | Test Loss: 0.50544 | Test Acc: 75.0000\n",
      "Epoch: 139 | Test Loss: 0.43767 | Test Acc: 83.3333\n",
      "Epoch: 139 | Test Loss: 0.36415 | Test Acc: 87.5000\n",
      "Epoch: 139 | Test Loss: 0.38206 | Test Acc: 83.3333\n",
      "Epoch: 139 | Test Loss: 0.43904 | Test Acc: 75.0000\n",
      "Epoch: 139 | Test Loss: 0.55286 | Test Acc: 83.3333\n",
      "Epoch: 139 | Test Loss: 0.45939 | Test Acc: 62.5000\n",
      "Epoch 140 of 200\n",
      "Epoch: 140 | Test Loss: 0.40982 | Test Acc: 75.0000\n",
      "Epoch: 140 | Test Loss: 0.55048 | Test Acc: 70.8333\n",
      "Epoch: 140 | Test Loss: 0.41829 | Test Acc: 66.6667\n",
      "Epoch: 140 | Test Loss: 0.47822 | Test Acc: 79.1667\n",
      "Epoch: 140 | Test Loss: 0.49063 | Test Acc: 79.1667\n",
      "Epoch: 140 | Test Loss: 0.36782 | Test Acc: 87.5000\n",
      "Epoch: 140 | Test Loss: 0.49412 | Test Acc: 66.6667\n",
      "Epoch: 140 | Test Loss: 0.45608 | Test Acc: 81.2500\n",
      "Epoch 141 of 200\n",
      "Epoch: 141 | Test Loss: 0.60814 | Test Acc: 75.0000\n",
      "Epoch: 141 | Test Loss: 0.53585 | Test Acc: 79.1667\n",
      "Epoch: 141 | Test Loss: 0.49095 | Test Acc: 70.8333\n",
      "Epoch: 141 | Test Loss: 0.47231 | Test Acc: 83.3333\n",
      "Epoch: 141 | Test Loss: 0.43819 | Test Acc: 70.8333\n",
      "Epoch: 141 | Test Loss: 0.45947 | Test Acc: 62.5000\n",
      "Epoch: 141 | Test Loss: 0.51323 | Test Acc: 66.6667\n",
      "Epoch: 141 | Test Loss: 0.40281 | Test Acc: 81.2500\n",
      "Epoch 142 of 200\n",
      "Epoch: 142 | Test Loss: 0.38752 | Test Acc: 91.6667\n",
      "Epoch: 142 | Test Loss: 0.39828 | Test Acc: 79.1667\n",
      "Epoch: 142 | Test Loss: 0.48869 | Test Acc: 66.6667\n",
      "Epoch: 142 | Test Loss: 0.71436 | Test Acc: 54.1667\n",
      "Epoch: 142 | Test Loss: 0.48615 | Test Acc: 79.1667\n",
      "Epoch: 142 | Test Loss: 0.44637 | Test Acc: 79.1667\n",
      "Epoch: 142 | Test Loss: 0.46712 | Test Acc: 79.1667\n",
      "Epoch: 142 | Test Loss: 0.22894 | Test Acc: 100.0000\n",
      "Epoch 143 of 200\n",
      "Epoch: 143 | Test Loss: 0.40822 | Test Acc: 87.5000\n",
      "Epoch: 143 | Test Loss: 0.55201 | Test Acc: 75.0000\n",
      "Epoch: 143 | Test Loss: 0.33896 | Test Acc: 83.3333\n",
      "Epoch: 143 | Test Loss: 0.46248 | Test Acc: 75.0000\n",
      "Epoch: 143 | Test Loss: 0.50108 | Test Acc: 79.1667\n",
      "Epoch: 143 | Test Loss: 0.47442 | Test Acc: 75.0000\n",
      "Epoch: 143 | Test Loss: 0.43650 | Test Acc: 83.3333\n",
      "Epoch: 143 | Test Loss: 0.43175 | Test Acc: 75.0000\n",
      "Epoch 144 of 200\n",
      "Epoch: 144 | Test Loss: 0.55998 | Test Acc: 79.1667\n",
      "Epoch: 144 | Test Loss: 0.26424 | Test Acc: 95.8333\n",
      "Epoch: 144 | Test Loss: 0.45066 | Test Acc: 87.5000\n",
      "Epoch: 144 | Test Loss: 0.38572 | Test Acc: 83.3333\n",
      "Epoch: 144 | Test Loss: 0.57533 | Test Acc: 75.0000\n",
      "Epoch: 144 | Test Loss: 0.44463 | Test Acc: 70.8333\n",
      "Epoch: 144 | Test Loss: 0.41581 | Test Acc: 79.1667\n",
      "Epoch: 144 | Test Loss: 0.36738 | Test Acc: 75.0000\n",
      "Epoch 145 of 200\n",
      "Epoch: 145 | Test Loss: 0.46714 | Test Acc: 75.0000\n",
      "Epoch: 145 | Test Loss: 0.37334 | Test Acc: 83.3333\n",
      "Epoch: 145 | Test Loss: 0.56422 | Test Acc: 66.6667\n",
      "Epoch: 145 | Test Loss: 0.50241 | Test Acc: 58.3333\n",
      "Epoch: 145 | Test Loss: 0.49309 | Test Acc: 75.0000\n",
      "Epoch: 145 | Test Loss: 0.33042 | Test Acc: 79.1667\n",
      "Epoch: 145 | Test Loss: 0.43585 | Test Acc: 79.1667\n",
      "Epoch: 145 | Test Loss: 0.44625 | Test Acc: 75.0000\n",
      "Epoch 146 of 200\n",
      "Epoch: 146 | Test Loss: 0.44143 | Test Acc: 83.3333\n",
      "Epoch: 146 | Test Loss: 0.33520 | Test Acc: 100.0000\n",
      "Epoch: 146 | Test Loss: 0.33546 | Test Acc: 87.5000\n",
      "Epoch: 146 | Test Loss: 0.46044 | Test Acc: 79.1667\n",
      "Epoch: 146 | Test Loss: 0.47264 | Test Acc: 70.8333\n",
      "Epoch: 146 | Test Loss: 0.39989 | Test Acc: 87.5000\n",
      "Epoch: 146 | Test Loss: 0.37813 | Test Acc: 83.3333\n",
      "Epoch: 146 | Test Loss: 0.61344 | Test Acc: 81.2500\n",
      "Epoch 147 of 200\n",
      "Epoch: 147 | Test Loss: 0.48407 | Test Acc: 79.1667\n",
      "Epoch: 147 | Test Loss: 0.44034 | Test Acc: 83.3333\n",
      "Epoch: 147 | Test Loss: 0.45615 | Test Acc: 79.1667\n",
      "Epoch: 147 | Test Loss: 0.51810 | Test Acc: 83.3333\n",
      "Epoch: 147 | Test Loss: 0.62222 | Test Acc: 70.8333\n",
      "Epoch: 147 | Test Loss: 0.41473 | Test Acc: 75.0000\n",
      "Epoch: 147 | Test Loss: 0.35530 | Test Acc: 87.5000\n",
      "Epoch: 147 | Test Loss: 0.43179 | Test Acc: 75.0000\n",
      "Epoch 148 of 200\n",
      "Epoch: 148 | Test Loss: 0.62401 | Test Acc: 70.8333\n",
      "Epoch: 148 | Test Loss: 0.50455 | Test Acc: 62.5000\n",
      "Epoch: 148 | Test Loss: 0.41022 | Test Acc: 83.3333\n",
      "Epoch: 148 | Test Loss: 0.44436 | Test Acc: 83.3333\n",
      "Epoch: 148 | Test Loss: 0.40981 | Test Acc: 83.3333\n",
      "Epoch: 148 | Test Loss: 0.52081 | Test Acc: 83.3333\n",
      "Epoch: 148 | Test Loss: 0.39808 | Test Acc: 75.0000\n",
      "Epoch: 148 | Test Loss: 0.45215 | Test Acc: 87.5000\n",
      "Epoch 149 of 200\n",
      "Epoch: 149 | Test Loss: 0.50970 | Test Acc: 62.5000\n",
      "Epoch: 149 | Test Loss: 0.57139 | Test Acc: 83.3333\n",
      "Epoch: 149 | Test Loss: 0.43415 | Test Acc: 79.1667\n",
      "Epoch: 149 | Test Loss: 0.52839 | Test Acc: 75.0000\n",
      "Epoch: 149 | Test Loss: 0.39894 | Test Acc: 87.5000\n",
      "Epoch: 149 | Test Loss: 0.44772 | Test Acc: 75.0000\n",
      "Epoch: 149 | Test Loss: 0.37392 | Test Acc: 83.3333\n",
      "Epoch: 149 | Test Loss: 0.43625 | Test Acc: 81.2500\n",
      "Epoch 150 of 200\n",
      "Epoch: 150 | Test Loss: 0.42986 | Test Acc: 70.8333\n",
      "Epoch: 150 | Test Loss: 0.58548 | Test Acc: 70.8333\n",
      "Epoch: 150 | Test Loss: 0.35019 | Test Acc: 83.3333\n",
      "Epoch: 150 | Test Loss: 0.51614 | Test Acc: 66.6667\n",
      "Epoch: 150 | Test Loss: 0.39979 | Test Acc: 83.3333\n",
      "Epoch: 150 | Test Loss: 0.56314 | Test Acc: 75.0000\n",
      "Epoch: 150 | Test Loss: 0.60293 | Test Acc: 62.5000\n",
      "Epoch: 150 | Test Loss: 0.44945 | Test Acc: 75.0000\n",
      "Epoch 151 of 200\n",
      "Epoch: 151 | Test Loss: 0.42756 | Test Acc: 79.1667\n",
      "Epoch: 151 | Test Loss: 0.46050 | Test Acc: 79.1667\n",
      "Epoch: 151 | Test Loss: 0.61899 | Test Acc: 54.1667\n",
      "Epoch: 151 | Test Loss: 0.54582 | Test Acc: 70.8333\n",
      "Epoch: 151 | Test Loss: 0.46475 | Test Acc: 75.0000\n",
      "Epoch: 151 | Test Loss: 0.53288 | Test Acc: 75.0000\n",
      "Epoch: 151 | Test Loss: 0.42974 | Test Acc: 83.3333\n",
      "Epoch: 151 | Test Loss: 0.40620 | Test Acc: 87.5000\n",
      "Epoch 152 of 200\n",
      "Epoch: 152 | Test Loss: 0.50251 | Test Acc: 83.3333\n",
      "Epoch: 152 | Test Loss: 0.40845 | Test Acc: 79.1667\n",
      "Epoch: 152 | Test Loss: 0.64989 | Test Acc: 70.8333\n",
      "Epoch: 152 | Test Loss: 0.40650 | Test Acc: 87.5000\n",
      "Epoch: 152 | Test Loss: 0.42658 | Test Acc: 79.1667\n",
      "Epoch: 152 | Test Loss: 0.47714 | Test Acc: 79.1667\n",
      "Epoch: 152 | Test Loss: 0.44751 | Test Acc: 75.0000\n",
      "Epoch: 152 | Test Loss: 0.39043 | Test Acc: 87.5000\n",
      "Epoch 153 of 200\n",
      "Epoch: 153 | Test Loss: 0.61199 | Test Acc: 62.5000\n",
      "Epoch: 153 | Test Loss: 0.39814 | Test Acc: 83.3333\n",
      "Epoch: 153 | Test Loss: 0.34465 | Test Acc: 91.6667\n",
      "Epoch: 153 | Test Loss: 0.43709 | Test Acc: 79.1667\n",
      "Epoch: 153 | Test Loss: 0.43490 | Test Acc: 79.1667\n",
      "Epoch: 153 | Test Loss: 0.56675 | Test Acc: 66.6667\n",
      "Epoch: 153 | Test Loss: 0.42337 | Test Acc: 79.1667\n",
      "Epoch: 153 | Test Loss: 0.34986 | Test Acc: 81.2500\n",
      "Epoch 154 of 200\n",
      "Epoch: 154 | Test Loss: 0.47982 | Test Acc: 66.6667\n",
      "Epoch: 154 | Test Loss: 0.50517 | Test Acc: 79.1667\n",
      "Epoch: 154 | Test Loss: 0.35910 | Test Acc: 87.5000\n",
      "Epoch: 154 | Test Loss: 0.52490 | Test Acc: 70.8333\n",
      "Epoch: 154 | Test Loss: 0.47545 | Test Acc: 66.6667\n",
      "Epoch: 154 | Test Loss: 0.46390 | Test Acc: 83.3333\n",
      "Epoch: 154 | Test Loss: 0.46036 | Test Acc: 79.1667\n",
      "Epoch: 154 | Test Loss: 0.37847 | Test Acc: 81.2500\n",
      "Epoch 155 of 200\n",
      "Epoch: 155 | Test Loss: 0.37123 | Test Acc: 91.6667\n",
      "Epoch: 155 | Test Loss: 0.36781 | Test Acc: 83.3333\n",
      "Epoch: 155 | Test Loss: 0.45750 | Test Acc: 79.1667\n",
      "Epoch: 155 | Test Loss: 0.51999 | Test Acc: 70.8333\n",
      "Epoch: 155 | Test Loss: 0.54970 | Test Acc: 70.8333\n",
      "Epoch: 155 | Test Loss: 0.63419 | Test Acc: 75.0000\n",
      "Epoch: 155 | Test Loss: 0.42697 | Test Acc: 87.5000\n",
      "Epoch: 155 | Test Loss: 0.59267 | Test Acc: 62.5000\n",
      "Epoch 156 of 200\n",
      "Epoch: 156 | Test Loss: 0.61916 | Test Acc: 66.6667\n",
      "Epoch: 156 | Test Loss: 0.56069 | Test Acc: 58.3333\n",
      "Epoch: 156 | Test Loss: 0.30962 | Test Acc: 87.5000\n",
      "Epoch: 156 | Test Loss: 0.42324 | Test Acc: 75.0000\n",
      "Epoch: 156 | Test Loss: 0.38360 | Test Acc: 83.3333\n",
      "Epoch: 156 | Test Loss: 0.47705 | Test Acc: 79.1667\n",
      "Epoch: 156 | Test Loss: 0.55768 | Test Acc: 66.6667\n",
      "Epoch: 156 | Test Loss: 0.48774 | Test Acc: 81.2500\n",
      "Epoch 157 of 200\n",
      "Epoch: 157 | Test Loss: 0.52157 | Test Acc: 66.6667\n",
      "Epoch: 157 | Test Loss: 0.54746 | Test Acc: 70.8333\n",
      "Epoch: 157 | Test Loss: 0.40012 | Test Acc: 87.5000\n",
      "Epoch: 157 | Test Loss: 0.60021 | Test Acc: 66.6667\n",
      "Epoch: 157 | Test Loss: 0.41266 | Test Acc: 87.5000\n",
      "Epoch: 157 | Test Loss: 0.42381 | Test Acc: 75.0000\n",
      "Epoch: 157 | Test Loss: 0.33949 | Test Acc: 87.5000\n",
      "Epoch: 157 | Test Loss: 0.39524 | Test Acc: 93.7500\n",
      "Epoch 158 of 200\n",
      "Epoch: 158 | Test Loss: 0.44378 | Test Acc: 79.1667\n",
      "Epoch: 158 | Test Loss: 0.57794 | Test Acc: 75.0000\n",
      "Epoch: 158 | Test Loss: 0.52157 | Test Acc: 70.8333\n",
      "Epoch: 158 | Test Loss: 0.47406 | Test Acc: 79.1667\n",
      "Epoch: 158 | Test Loss: 0.38919 | Test Acc: 87.5000\n",
      "Epoch: 158 | Test Loss: 0.41198 | Test Acc: 75.0000\n",
      "Epoch: 158 | Test Loss: 0.56128 | Test Acc: 70.8333\n",
      "Epoch: 158 | Test Loss: 0.43283 | Test Acc: 75.0000\n",
      "Epoch 159 of 200\n",
      "Epoch: 159 | Test Loss: 0.51766 | Test Acc: 83.3333\n",
      "Epoch: 159 | Test Loss: 0.52835 | Test Acc: 66.6667\n",
      "Epoch: 159 | Test Loss: 0.48716 | Test Acc: 79.1667\n",
      "Epoch: 159 | Test Loss: 0.40229 | Test Acc: 91.6667\n",
      "Epoch: 159 | Test Loss: 0.39891 | Test Acc: 79.1667\n",
      "Epoch: 159 | Test Loss: 0.48696 | Test Acc: 70.8333\n",
      "Epoch: 159 | Test Loss: 0.37211 | Test Acc: 95.8333\n",
      "Epoch: 159 | Test Loss: 0.63398 | Test Acc: 56.2500\n",
      "Epoch 160 of 200\n",
      "Epoch: 160 | Test Loss: 0.49140 | Test Acc: 79.1667\n",
      "Epoch: 160 | Test Loss: 0.41785 | Test Acc: 70.8333\n",
      "Epoch: 160 | Test Loss: 0.50571 | Test Acc: 75.0000\n",
      "Epoch: 160 | Test Loss: 0.40703 | Test Acc: 87.5000\n",
      "Epoch: 160 | Test Loss: 0.58686 | Test Acc: 58.3333\n",
      "Epoch: 160 | Test Loss: 0.55780 | Test Acc: 62.5000\n",
      "Epoch: 160 | Test Loss: 0.47912 | Test Acc: 79.1667\n",
      "Epoch: 160 | Test Loss: 0.44582 | Test Acc: 87.5000\n",
      "Epoch 161 of 200\n",
      "Epoch: 161 | Test Loss: 0.36892 | Test Acc: 87.5000\n",
      "Epoch: 161 | Test Loss: 0.45747 | Test Acc: 79.1667\n",
      "Epoch: 161 | Test Loss: 0.51241 | Test Acc: 79.1667\n",
      "Epoch: 161 | Test Loss: 0.41185 | Test Acc: 83.3333\n",
      "Epoch: 161 | Test Loss: 0.47731 | Test Acc: 75.0000\n",
      "Epoch: 161 | Test Loss: 0.42226 | Test Acc: 87.5000\n",
      "Epoch: 161 | Test Loss: 0.75695 | Test Acc: 62.5000\n",
      "Epoch: 161 | Test Loss: 0.67893 | Test Acc: 68.7500\n",
      "Epoch 162 of 200\n",
      "Epoch: 162 | Test Loss: 0.42137 | Test Acc: 83.3333\n",
      "Epoch: 162 | Test Loss: 0.40859 | Test Acc: 87.5000\n",
      "Epoch: 162 | Test Loss: 0.50289 | Test Acc: 70.8333\n",
      "Epoch: 162 | Test Loss: 0.44498 | Test Acc: 75.0000\n",
      "Epoch: 162 | Test Loss: 0.43685 | Test Acc: 79.1667\n",
      "Epoch: 162 | Test Loss: 0.54326 | Test Acc: 70.8333\n",
      "Epoch: 162 | Test Loss: 0.44399 | Test Acc: 79.1667\n",
      "Epoch: 162 | Test Loss: 0.43526 | Test Acc: 75.0000\n",
      "Epoch 163 of 200\n",
      "Epoch: 163 | Test Loss: 0.46643 | Test Acc: 75.0000\n",
      "Epoch: 163 | Test Loss: 0.57296 | Test Acc: 50.0000\n",
      "Epoch: 163 | Test Loss: 0.56160 | Test Acc: 70.8333\n",
      "Epoch: 163 | Test Loss: 0.42848 | Test Acc: 79.1667\n",
      "Epoch: 163 | Test Loss: 0.43216 | Test Acc: 91.6667\n",
      "Epoch: 163 | Test Loss: 0.52437 | Test Acc: 66.6667\n",
      "Epoch: 163 | Test Loss: 0.44271 | Test Acc: 70.8333\n",
      "Epoch: 163 | Test Loss: 0.48015 | Test Acc: 81.2500\n",
      "Epoch 164 of 200\n",
      "Epoch: 164 | Test Loss: 0.52408 | Test Acc: 70.8333\n",
      "Epoch: 164 | Test Loss: 0.49755 | Test Acc: 66.6667\n",
      "Epoch: 164 | Test Loss: 0.51305 | Test Acc: 70.8333\n",
      "Epoch: 164 | Test Loss: 0.48067 | Test Acc: 87.5000\n",
      "Epoch: 164 | Test Loss: 0.33276 | Test Acc: 87.5000\n",
      "Epoch: 164 | Test Loss: 0.58216 | Test Acc: 62.5000\n",
      "Epoch: 164 | Test Loss: 0.60280 | Test Acc: 70.8333\n",
      "Epoch: 164 | Test Loss: 0.35960 | Test Acc: 87.5000\n",
      "Epoch 165 of 200\n",
      "Epoch: 165 | Test Loss: 0.46867 | Test Acc: 87.5000\n",
      "Epoch: 165 | Test Loss: 0.41716 | Test Acc: 91.6667\n",
      "Epoch: 165 | Test Loss: 0.51356 | Test Acc: 83.3333\n",
      "Epoch: 165 | Test Loss: 0.42134 | Test Acc: 75.0000\n",
      "Epoch: 165 | Test Loss: 0.62482 | Test Acc: 58.3333\n",
      "Epoch: 165 | Test Loss: 0.37728 | Test Acc: 83.3333\n",
      "Epoch: 165 | Test Loss: 0.53222 | Test Acc: 70.8333\n",
      "Epoch: 165 | Test Loss: 0.48884 | Test Acc: 75.0000\n",
      "Epoch 166 of 200\n",
      "Epoch: 166 | Test Loss: 0.55681 | Test Acc: 79.1667\n",
      "Epoch: 166 | Test Loss: 0.44034 | Test Acc: 75.0000\n",
      "Epoch: 166 | Test Loss: 0.46343 | Test Acc: 83.3333\n",
      "Epoch: 166 | Test Loss: 0.51988 | Test Acc: 79.1667\n",
      "Epoch: 166 | Test Loss: 0.38461 | Test Acc: 83.3333\n",
      "Epoch: 166 | Test Loss: 0.38355 | Test Acc: 83.3333\n",
      "Epoch: 166 | Test Loss: 0.40699 | Test Acc: 83.3333\n",
      "Epoch: 166 | Test Loss: 0.49151 | Test Acc: 93.7500\n",
      "Epoch 167 of 200\n",
      "Epoch: 167 | Test Loss: 0.52775 | Test Acc: 66.6667\n",
      "Epoch: 167 | Test Loss: 0.37726 | Test Acc: 87.5000\n",
      "Epoch: 167 | Test Loss: 0.56920 | Test Acc: 79.1667\n",
      "Epoch: 167 | Test Loss: 0.53961 | Test Acc: 70.8333\n",
      "Epoch: 167 | Test Loss: 0.40113 | Test Acc: 91.6667\n",
      "Epoch: 167 | Test Loss: 0.50428 | Test Acc: 66.6667\n",
      "Epoch: 167 | Test Loss: 0.40462 | Test Acc: 87.5000\n",
      "Epoch: 167 | Test Loss: 0.37540 | Test Acc: 87.5000\n",
      "Epoch 168 of 200\n",
      "Epoch: 168 | Test Loss: 0.46390 | Test Acc: 75.0000\n",
      "Epoch: 168 | Test Loss: 0.45757 | Test Acc: 75.0000\n",
      "Epoch: 168 | Test Loss: 0.44271 | Test Acc: 87.5000\n",
      "Epoch: 168 | Test Loss: 0.38451 | Test Acc: 87.5000\n",
      "Epoch: 168 | Test Loss: 0.36262 | Test Acc: 83.3333\n",
      "Epoch: 168 | Test Loss: 0.46355 | Test Acc: 79.1667\n",
      "Epoch: 168 | Test Loss: 0.39354 | Test Acc: 87.5000\n",
      "Epoch: 168 | Test Loss: 0.54953 | Test Acc: 68.7500\n",
      "Epoch 169 of 200\n",
      "Epoch: 169 | Test Loss: 0.49405 | Test Acc: 83.3333\n",
      "Epoch: 169 | Test Loss: 0.33632 | Test Acc: 87.5000\n",
      "Epoch: 169 | Test Loss: 0.49964 | Test Acc: 75.0000\n",
      "Epoch: 169 | Test Loss: 0.71769 | Test Acc: 58.3333\n",
      "Epoch: 169 | Test Loss: 0.40019 | Test Acc: 75.0000\n",
      "Epoch: 169 | Test Loss: 0.52574 | Test Acc: 66.6667\n",
      "Epoch: 169 | Test Loss: 0.41601 | Test Acc: 87.5000\n",
      "Epoch: 169 | Test Loss: 0.46400 | Test Acc: 81.2500\n",
      "Epoch 170 of 200\n",
      "Epoch: 170 | Test Loss: 0.36585 | Test Acc: 91.6667\n",
      "Epoch: 170 | Test Loss: 0.65229 | Test Acc: 58.3333\n",
      "Epoch: 170 | Test Loss: 0.44570 | Test Acc: 91.6667\n",
      "Epoch: 170 | Test Loss: 0.50018 | Test Acc: 75.0000\n",
      "Epoch: 170 | Test Loss: 0.39807 | Test Acc: 83.3333\n",
      "Epoch: 170 | Test Loss: 0.45813 | Test Acc: 70.8333\n",
      "Epoch: 170 | Test Loss: 0.42023 | Test Acc: 83.3333\n",
      "Epoch: 170 | Test Loss: 0.33873 | Test Acc: 87.5000\n",
      "Epoch 171 of 200\n",
      "Epoch: 171 | Test Loss: 0.42587 | Test Acc: 70.8333\n",
      "Epoch: 171 | Test Loss: 0.62064 | Test Acc: 70.8333\n",
      "Epoch: 171 | Test Loss: 0.54555 | Test Acc: 66.6667\n",
      "Epoch: 171 | Test Loss: 0.43387 | Test Acc: 75.0000\n",
      "Epoch: 171 | Test Loss: 0.55406 | Test Acc: 58.3333\n",
      "Epoch: 171 | Test Loss: 0.53643 | Test Acc: 70.8333\n",
      "Epoch: 171 | Test Loss: 0.54979 | Test Acc: 58.3333\n",
      "Epoch: 171 | Test Loss: 0.41457 | Test Acc: 81.2500\n",
      "Epoch 172 of 200\n",
      "Epoch: 172 | Test Loss: 0.41319 | Test Acc: 79.1667\n",
      "Epoch: 172 | Test Loss: 0.52357 | Test Acc: 83.3333\n",
      "Epoch: 172 | Test Loss: 0.45968 | Test Acc: 75.0000\n",
      "Epoch: 172 | Test Loss: 0.48120 | Test Acc: 91.6667\n",
      "Epoch: 172 | Test Loss: 0.47481 | Test Acc: 66.6667\n",
      "Epoch: 172 | Test Loss: 0.41887 | Test Acc: 79.1667\n",
      "Epoch: 172 | Test Loss: 0.55316 | Test Acc: 75.0000\n",
      "Epoch: 172 | Test Loss: 0.45114 | Test Acc: 81.2500\n",
      "Epoch 173 of 200\n",
      "Epoch: 173 | Test Loss: 0.33826 | Test Acc: 83.3333\n",
      "Epoch: 173 | Test Loss: 0.43062 | Test Acc: 75.0000\n",
      "Epoch: 173 | Test Loss: 0.36652 | Test Acc: 91.6667\n",
      "Epoch: 173 | Test Loss: 0.56741 | Test Acc: 70.8333\n",
      "Epoch: 173 | Test Loss: 0.53618 | Test Acc: 58.3333\n",
      "Epoch: 173 | Test Loss: 0.48329 | Test Acc: 87.5000\n",
      "Epoch: 173 | Test Loss: 0.44737 | Test Acc: 75.0000\n",
      "Epoch: 173 | Test Loss: 0.39592 | Test Acc: 93.7500\n",
      "Epoch 174 of 200\n",
      "Epoch: 174 | Test Loss: 0.45187 | Test Acc: 83.3333\n",
      "Epoch: 174 | Test Loss: 0.37397 | Test Acc: 91.6667\n",
      "Epoch: 174 | Test Loss: 0.52134 | Test Acc: 83.3333\n",
      "Epoch: 174 | Test Loss: 0.49086 | Test Acc: 75.0000\n",
      "Epoch: 174 | Test Loss: 0.50473 | Test Acc: 83.3333\n",
      "Epoch: 174 | Test Loss: 0.46195 | Test Acc: 79.1667\n",
      "Epoch: 174 | Test Loss: 0.43527 | Test Acc: 83.3333\n",
      "Epoch: 174 | Test Loss: 0.45335 | Test Acc: 81.2500\n",
      "Epoch 175 of 200\n",
      "Epoch: 175 | Test Loss: 0.42245 | Test Acc: 66.6667\n",
      "Epoch: 175 | Test Loss: 0.42229 | Test Acc: 91.6667\n",
      "Epoch: 175 | Test Loss: 0.34853 | Test Acc: 87.5000\n",
      "Epoch: 175 | Test Loss: 0.53707 | Test Acc: 75.0000\n",
      "Epoch: 175 | Test Loss: 0.70002 | Test Acc: 75.0000\n",
      "Epoch: 175 | Test Loss: 0.38813 | Test Acc: 87.5000\n",
      "Epoch: 175 | Test Loss: 0.44087 | Test Acc: 79.1667\n",
      "Epoch: 175 | Test Loss: 0.30075 | Test Acc: 87.5000\n",
      "Epoch 176 of 200\n",
      "Epoch: 176 | Test Loss: 0.45235 | Test Acc: 79.1667\n",
      "Epoch: 176 | Test Loss: 0.43837 | Test Acc: 79.1667\n",
      "Epoch: 176 | Test Loss: 0.50051 | Test Acc: 70.8333\n",
      "Epoch: 176 | Test Loss: 0.51735 | Test Acc: 75.0000\n",
      "Epoch: 176 | Test Loss: 0.33461 | Test Acc: 83.3333\n",
      "Epoch: 176 | Test Loss: 0.60300 | Test Acc: 62.5000\n",
      "Epoch: 176 | Test Loss: 0.49818 | Test Acc: 79.1667\n",
      "Epoch: 176 | Test Loss: 0.41828 | Test Acc: 87.5000\n",
      "Epoch 177 of 200\n",
      "Epoch: 177 | Test Loss: 0.59121 | Test Acc: 66.6667\n",
      "Epoch: 177 | Test Loss: 0.60962 | Test Acc: 66.6667\n",
      "Epoch: 177 | Test Loss: 0.42113 | Test Acc: 79.1667\n",
      "Epoch: 177 | Test Loss: 0.49177 | Test Acc: 62.5000\n",
      "Epoch: 177 | Test Loss: 0.37922 | Test Acc: 83.3333\n",
      "Epoch: 177 | Test Loss: 0.39556 | Test Acc: 79.1667\n",
      "Epoch: 177 | Test Loss: 0.47041 | Test Acc: 75.0000\n",
      "Epoch: 177 | Test Loss: 0.49811 | Test Acc: 68.7500\n",
      "Epoch 178 of 200\n",
      "Epoch: 178 | Test Loss: 0.52578 | Test Acc: 75.0000\n",
      "Epoch: 178 | Test Loss: 0.45244 | Test Acc: 83.3333\n",
      "Epoch: 178 | Test Loss: 0.64847 | Test Acc: 54.1667\n",
      "Epoch: 178 | Test Loss: 0.35283 | Test Acc: 79.1667\n",
      "Epoch: 178 | Test Loss: 0.47366 | Test Acc: 70.8333\n",
      "Epoch: 178 | Test Loss: 0.46042 | Test Acc: 79.1667\n",
      "Epoch: 178 | Test Loss: 0.30367 | Test Acc: 95.8333\n",
      "Epoch: 178 | Test Loss: 0.43102 | Test Acc: 87.5000\n",
      "Epoch 179 of 200\n",
      "Epoch: 179 | Test Loss: 0.46907 | Test Acc: 70.8333\n",
      "Epoch: 179 | Test Loss: 0.40071 | Test Acc: 87.5000\n",
      "Epoch: 179 | Test Loss: 0.55709 | Test Acc: 66.6667\n",
      "Epoch: 179 | Test Loss: 0.64955 | Test Acc: 62.5000\n",
      "Epoch: 179 | Test Loss: 0.42649 | Test Acc: 75.0000\n",
      "Epoch: 179 | Test Loss: 0.48583 | Test Acc: 83.3333\n",
      "Epoch: 179 | Test Loss: 0.46046 | Test Acc: 75.0000\n",
      "Epoch: 179 | Test Loss: 0.38636 | Test Acc: 93.7500\n",
      "Epoch 180 of 200\n",
      "Epoch: 180 | Test Loss: 0.30847 | Test Acc: 95.8333\n",
      "Epoch: 180 | Test Loss: 0.48370 | Test Acc: 91.6667\n",
      "Epoch: 180 | Test Loss: 0.58925 | Test Acc: 58.3333\n",
      "Epoch: 180 | Test Loss: 0.61025 | Test Acc: 62.5000\n",
      "Epoch: 180 | Test Loss: 0.59141 | Test Acc: 66.6667\n",
      "Epoch: 180 | Test Loss: 0.43768 | Test Acc: 75.0000\n",
      "Epoch: 180 | Test Loss: 0.60435 | Test Acc: 70.8333\n",
      "Epoch: 180 | Test Loss: 0.43342 | Test Acc: 93.7500\n",
      "Epoch 181 of 200\n",
      "Epoch: 181 | Test Loss: 0.58044 | Test Acc: 50.0000\n",
      "Epoch: 181 | Test Loss: 0.46354 | Test Acc: 87.5000\n",
      "Epoch: 181 | Test Loss: 0.36172 | Test Acc: 91.6667\n",
      "Epoch: 181 | Test Loss: 0.44699 | Test Acc: 83.3333\n",
      "Epoch: 181 | Test Loss: 0.58522 | Test Acc: 58.3333\n",
      "Epoch: 181 | Test Loss: 0.51078 | Test Acc: 79.1667\n",
      "Epoch: 181 | Test Loss: 0.45046 | Test Acc: 70.8333\n",
      "Epoch: 181 | Test Loss: 0.46136 | Test Acc: 68.7500\n",
      "Epoch 182 of 200\n",
      "Epoch: 182 | Test Loss: 0.56418 | Test Acc: 66.6667\n",
      "Epoch: 182 | Test Loss: 0.69941 | Test Acc: 58.3333\n",
      "Epoch: 182 | Test Loss: 0.50335 | Test Acc: 58.3333\n",
      "Epoch: 182 | Test Loss: 0.43322 | Test Acc: 70.8333\n",
      "Epoch: 182 | Test Loss: 0.52183 | Test Acc: 66.6667\n",
      "Epoch: 182 | Test Loss: 0.56329 | Test Acc: 75.0000\n",
      "Epoch: 182 | Test Loss: 0.31908 | Test Acc: 87.5000\n",
      "Epoch: 182 | Test Loss: 0.53038 | Test Acc: 75.0000\n",
      "Epoch 183 of 200\n",
      "Epoch: 183 | Test Loss: 0.40535 | Test Acc: 79.1667\n",
      "Epoch: 183 | Test Loss: 0.68858 | Test Acc: 54.1667\n",
      "Epoch: 183 | Test Loss: 0.48395 | Test Acc: 79.1667\n",
      "Epoch: 183 | Test Loss: 0.50787 | Test Acc: 66.6667\n",
      "Epoch: 183 | Test Loss: 0.51598 | Test Acc: 58.3333\n",
      "Epoch: 183 | Test Loss: 0.58383 | Test Acc: 75.0000\n",
      "Epoch: 183 | Test Loss: 0.50583 | Test Acc: 62.5000\n",
      "Epoch: 183 | Test Loss: 0.47280 | Test Acc: 81.2500\n",
      "Epoch 184 of 200\n",
      "Epoch: 184 | Test Loss: 0.47976 | Test Acc: 75.0000\n",
      "Epoch: 184 | Test Loss: 0.33239 | Test Acc: 87.5000\n",
      "Epoch: 184 | Test Loss: 0.45146 | Test Acc: 79.1667\n",
      "Epoch: 184 | Test Loss: 0.49136 | Test Acc: 62.5000\n",
      "Epoch: 184 | Test Loss: 0.51108 | Test Acc: 70.8333\n",
      "Epoch: 184 | Test Loss: 0.52835 | Test Acc: 70.8333\n",
      "Epoch: 184 | Test Loss: 0.64915 | Test Acc: 66.6667\n",
      "Epoch: 184 | Test Loss: 0.34022 | Test Acc: 100.0000\n",
      "Epoch 185 of 200\n",
      "Epoch: 185 | Test Loss: 0.45992 | Test Acc: 79.1667\n",
      "Epoch: 185 | Test Loss: 0.48010 | Test Acc: 75.0000\n",
      "Epoch: 185 | Test Loss: 0.46115 | Test Acc: 79.1667\n",
      "Epoch: 185 | Test Loss: 0.52701 | Test Acc: 62.5000\n",
      "Epoch: 185 | Test Loss: 0.44843 | Test Acc: 79.1667\n",
      "Epoch: 185 | Test Loss: 0.59283 | Test Acc: 66.6667\n",
      "Epoch: 185 | Test Loss: 0.34022 | Test Acc: 87.5000\n",
      "Epoch: 185 | Test Loss: 0.51021 | Test Acc: 68.7500\n",
      "Epoch 186 of 200\n",
      "Epoch: 186 | Test Loss: 0.46948 | Test Acc: 70.8333\n",
      "Epoch: 186 | Test Loss: 0.56821 | Test Acc: 70.8333\n",
      "Epoch: 186 | Test Loss: 0.47066 | Test Acc: 75.0000\n",
      "Epoch: 186 | Test Loss: 0.36200 | Test Acc: 87.5000\n",
      "Epoch: 186 | Test Loss: 0.45873 | Test Acc: 70.8333\n",
      "Epoch: 186 | Test Loss: 0.49241 | Test Acc: 75.0000\n",
      "Epoch: 186 | Test Loss: 0.46855 | Test Acc: 83.3333\n",
      "Epoch: 186 | Test Loss: 0.59327 | Test Acc: 81.2500\n",
      "Epoch 187 of 200\n",
      "Epoch: 187 | Test Loss: 0.54231 | Test Acc: 83.3333\n",
      "Epoch: 187 | Test Loss: 0.47837 | Test Acc: 66.6667\n",
      "Epoch: 187 | Test Loss: 0.41206 | Test Acc: 83.3333\n",
      "Epoch: 187 | Test Loss: 0.36997 | Test Acc: 87.5000\n",
      "Epoch: 187 | Test Loss: 0.36862 | Test Acc: 83.3333\n",
      "Epoch: 187 | Test Loss: 0.30062 | Test Acc: 95.8333\n",
      "Epoch: 187 | Test Loss: 0.49752 | Test Acc: 79.1667\n",
      "Epoch: 187 | Test Loss: 0.69797 | Test Acc: 68.7500\n",
      "Epoch 188 of 200\n",
      "Epoch: 188 | Test Loss: 0.51084 | Test Acc: 70.8333\n",
      "Epoch: 188 | Test Loss: 0.45524 | Test Acc: 75.0000\n",
      "Epoch: 188 | Test Loss: 0.41526 | Test Acc: 75.0000\n",
      "Epoch: 188 | Test Loss: 0.67865 | Test Acc: 62.5000\n",
      "Epoch: 188 | Test Loss: 0.36690 | Test Acc: 83.3333\n",
      "Epoch: 188 | Test Loss: 0.48294 | Test Acc: 70.8333\n",
      "Epoch: 188 | Test Loss: 0.43235 | Test Acc: 79.1667\n",
      "Epoch: 188 | Test Loss: 0.48965 | Test Acc: 81.2500\n",
      "Epoch 189 of 200\n",
      "Epoch: 189 | Test Loss: 0.44824 | Test Acc: 70.8333\n",
      "Epoch: 189 | Test Loss: 0.58416 | Test Acc: 66.6667\n",
      "Epoch: 189 | Test Loss: 0.45912 | Test Acc: 79.1667\n",
      "Epoch: 189 | Test Loss: 0.48792 | Test Acc: 70.8333\n",
      "Epoch: 189 | Test Loss: 0.51881 | Test Acc: 79.1667\n",
      "Epoch: 189 | Test Loss: 0.64627 | Test Acc: 58.3333\n",
      "Epoch: 189 | Test Loss: 0.39742 | Test Acc: 87.5000\n",
      "Epoch: 189 | Test Loss: 0.46420 | Test Acc: 68.7500\n",
      "Epoch 190 of 200\n",
      "Epoch: 190 | Test Loss: 0.44482 | Test Acc: 70.8333\n",
      "Epoch: 190 | Test Loss: 0.43482 | Test Acc: 70.8333\n",
      "Epoch: 190 | Test Loss: 0.57998 | Test Acc: 62.5000\n",
      "Epoch: 190 | Test Loss: 0.59814 | Test Acc: 66.6667\n",
      "Epoch: 190 | Test Loss: 0.45148 | Test Acc: 75.0000\n",
      "Epoch: 190 | Test Loss: 0.48966 | Test Acc: 75.0000\n",
      "Epoch: 190 | Test Loss: 0.45245 | Test Acc: 75.0000\n",
      "Epoch: 190 | Test Loss: 0.21540 | Test Acc: 100.0000\n",
      "Epoch 191 of 200\n",
      "Epoch: 191 | Test Loss: 0.40874 | Test Acc: 87.5000\n",
      "Epoch: 191 | Test Loss: 0.36007 | Test Acc: 83.3333\n",
      "Epoch: 191 | Test Loss: 0.49032 | Test Acc: 75.0000\n",
      "Epoch: 191 | Test Loss: 0.48001 | Test Acc: 79.1667\n",
      "Epoch: 191 | Test Loss: 0.38709 | Test Acc: 87.5000\n",
      "Epoch: 191 | Test Loss: 0.63411 | Test Acc: 75.0000\n",
      "Epoch: 191 | Test Loss: 0.47075 | Test Acc: 79.1667\n",
      "Epoch: 191 | Test Loss: 0.30846 | Test Acc: 87.5000\n",
      "Epoch 192 of 200\n",
      "Epoch: 192 | Test Loss: 0.61691 | Test Acc: 79.1667\n",
      "Epoch: 192 | Test Loss: 0.70952 | Test Acc: 58.3333\n",
      "Epoch: 192 | Test Loss: 0.42700 | Test Acc: 79.1667\n",
      "Epoch: 192 | Test Loss: 0.46761 | Test Acc: 75.0000\n",
      "Epoch: 192 | Test Loss: 0.48082 | Test Acc: 83.3333\n",
      "Epoch: 192 | Test Loss: 0.38439 | Test Acc: 87.5000\n",
      "Epoch: 192 | Test Loss: 0.36599 | Test Acc: 91.6667\n",
      "Epoch: 192 | Test Loss: 0.36886 | Test Acc: 87.5000\n",
      "Epoch 193 of 200\n",
      "Epoch: 193 | Test Loss: 0.62088 | Test Acc: 66.6667\n",
      "Epoch: 193 | Test Loss: 0.46062 | Test Acc: 79.1667\n",
      "Epoch: 193 | Test Loss: 0.49707 | Test Acc: 66.6667\n",
      "Epoch: 193 | Test Loss: 0.54572 | Test Acc: 70.8333\n",
      "Epoch: 193 | Test Loss: 0.56525 | Test Acc: 75.0000\n",
      "Epoch: 193 | Test Loss: 0.36679 | Test Acc: 95.8333\n",
      "Epoch: 193 | Test Loss: 0.52510 | Test Acc: 66.6667\n",
      "Epoch: 193 | Test Loss: 0.54097 | Test Acc: 68.7500\n",
      "Epoch 194 of 200\n",
      "Epoch: 194 | Test Loss: 0.54606 | Test Acc: 70.8333\n",
      "Epoch: 194 | Test Loss: 0.46544 | Test Acc: 79.1667\n",
      "Epoch: 194 | Test Loss: 0.44955 | Test Acc: 79.1667\n",
      "Epoch: 194 | Test Loss: 0.37629 | Test Acc: 91.6667\n",
      "Epoch: 194 | Test Loss: 0.56005 | Test Acc: 62.5000\n",
      "Epoch: 194 | Test Loss: 0.49376 | Test Acc: 66.6667\n",
      "Epoch: 194 | Test Loss: 0.38014 | Test Acc: 83.3333\n",
      "Epoch: 194 | Test Loss: 0.54190 | Test Acc: 62.5000\n",
      "Epoch 195 of 200\n",
      "Epoch: 195 | Test Loss: 0.35270 | Test Acc: 83.3333\n",
      "Epoch: 195 | Test Loss: 0.60606 | Test Acc: 58.3333\n",
      "Epoch: 195 | Test Loss: 0.49659 | Test Acc: 70.8333\n",
      "Epoch: 195 | Test Loss: 0.54107 | Test Acc: 58.3333\n",
      "Epoch: 195 | Test Loss: 0.44211 | Test Acc: 83.3333\n",
      "Epoch: 195 | Test Loss: 0.47985 | Test Acc: 75.0000\n",
      "Epoch: 195 | Test Loss: 0.37846 | Test Acc: 87.5000\n",
      "Epoch: 195 | Test Loss: 0.41492 | Test Acc: 87.5000\n",
      "Epoch 196 of 200\n",
      "Epoch: 196 | Test Loss: 0.51922 | Test Acc: 79.1667\n",
      "Epoch: 196 | Test Loss: 0.51992 | Test Acc: 70.8333\n",
      "Epoch: 196 | Test Loss: 0.52247 | Test Acc: 70.8333\n",
      "Epoch: 196 | Test Loss: 0.34118 | Test Acc: 95.8333\n",
      "Epoch: 196 | Test Loss: 0.46918 | Test Acc: 79.1667\n",
      "Epoch: 196 | Test Loss: 0.47901 | Test Acc: 75.0000\n",
      "Epoch: 196 | Test Loss: 0.57336 | Test Acc: 66.6667\n",
      "Epoch: 196 | Test Loss: 0.48722 | Test Acc: 81.2500\n",
      "Epoch 197 of 200\n",
      "Epoch: 197 | Test Loss: 0.44277 | Test Acc: 87.5000\n",
      "Epoch: 197 | Test Loss: 0.44029 | Test Acc: 75.0000\n",
      "Epoch: 197 | Test Loss: 0.54428 | Test Acc: 62.5000\n",
      "Epoch: 197 | Test Loss: 0.49557 | Test Acc: 70.8333\n",
      "Epoch: 197 | Test Loss: 0.43990 | Test Acc: 75.0000\n",
      "Epoch: 197 | Test Loss: 0.50639 | Test Acc: 79.1667\n",
      "Epoch: 197 | Test Loss: 0.49518 | Test Acc: 66.6667\n",
      "Epoch: 197 | Test Loss: 0.45872 | Test Acc: 87.5000\n",
      "Epoch 198 of 200\n",
      "Epoch: 198 | Test Loss: 0.41037 | Test Acc: 79.1667\n",
      "Epoch: 198 | Test Loss: 0.61178 | Test Acc: 62.5000\n",
      "Epoch: 198 | Test Loss: 0.46727 | Test Acc: 79.1667\n",
      "Epoch: 198 | Test Loss: 0.52266 | Test Acc: 79.1667\n",
      "Epoch: 198 | Test Loss: 0.28790 | Test Acc: 95.8333\n",
      "Epoch: 198 | Test Loss: 0.61385 | Test Acc: 58.3333\n",
      "Epoch: 198 | Test Loss: 0.49500 | Test Acc: 75.0000\n",
      "Epoch: 198 | Test Loss: 0.56876 | Test Acc: 75.0000\n",
      "Epoch 199 of 200\n",
      "Epoch: 199 | Test Loss: 0.48705 | Test Acc: 70.8333\n",
      "Epoch: 199 | Test Loss: 0.38627 | Test Acc: 79.1667\n",
      "Epoch: 199 | Test Loss: 0.52930 | Test Acc: 79.1667\n",
      "Epoch: 199 | Test Loss: 0.64258 | Test Acc: 70.8333\n",
      "Epoch: 199 | Test Loss: 0.46827 | Test Acc: 75.0000\n",
      "Epoch: 199 | Test Loss: 0.48355 | Test Acc: 70.8333\n",
      "Epoch: 199 | Test Loss: 0.37121 | Test Acc: 83.3333\n",
      "Epoch: 199 | Test Loss: 0.52027 | Test Acc: 75.0000\n"
     ]
    }
   ],
   "source": [
    "maxAcc = 0\n",
    "mean = 0\n",
    "epochs =200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch} of {epochs}\")\n",
    "    train_one_epoch(epoch, None)\n",
    "    test_one_epoch(epoch, None)\n",
    "    \n",
    "torch.save(MarketPredictorModel.state_dict(), \"MarketPredictor.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb88c8e",
   "metadata": {},
   "source": [
    "Analyzing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a59a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: layer_1.weight | Shape: torch.Size([16, 10])\n",
      "tensor([[ 0.2855, -0.1026, -0.3004, -0.1612,  0.3087, -0.0544, -0.2885,  0.2669,\n",
      "          0.2743,  0.2848],\n",
      "        [ 0.1103, -0.8060,  0.6610,  0.2554, -0.1130, -0.1737,  0.4275, -0.0267,\n",
      "         -0.1830,  0.1022],\n",
      "        [ 0.2512,  0.9193, -1.6732,  0.0966,  0.8955, -0.0755, -1.8308,  1.0481,\n",
      "         -0.2885, -0.3258],\n",
      "        [ 0.2332,  0.3016,  0.3132, -0.0043,  0.0264, -0.3117,  0.0831,  0.0926,\n",
      "         -0.0455, -0.2206],\n",
      "        [ 0.0571,  0.0719,  0.6415, -0.0618,  0.0895,  0.2832,  0.9054, -0.2846,\n",
      "          0.0477, -0.2134],\n",
      "        [ 0.2311,  1.6097, -0.3228,  0.0970,  0.7149, -0.1563, -0.1165,  0.7615,\n",
      "          0.0725,  0.1527],\n",
      "        [-0.1749,  0.0721, -0.2027, -0.0701, -0.0561, -0.0349,  0.2728,  0.0407,\n",
      "          0.0602, -0.3040],\n",
      "        [ 0.3539,  0.5547,  1.1252, -0.1985, -0.0796,  0.1080,  0.8224, -0.4665,\n",
      "          0.2864,  0.0451],\n",
      "        [-0.1708,  0.2028, -0.4525,  0.2756, -0.0510, -0.0689, -0.0834, -0.0380,\n",
      "         -0.2037, -0.2720],\n",
      "        [ 0.1460,  0.5599,  1.2523, -0.1522, -0.3055,  0.0571,  1.5750, -0.0975,\n",
      "          0.1402, -0.1115],\n",
      "        [-0.2859,  1.1174,  0.1249,  0.2551,  0.1933, -0.1015, -0.3159,  0.5026,\n",
      "         -0.0331, -0.3047],\n",
      "        [ 0.2260, -0.0308,  0.2724,  0.1955, -0.2614, -0.0633,  0.5454, -0.3331,\n",
      "         -0.0327,  0.0207],\n",
      "        [ 0.2967, -0.6683,  0.9114,  0.3006, -0.0318, -0.1359,  1.0967, -0.5669,\n",
      "          0.2768,  0.1238],\n",
      "        [-0.0422,  0.0136,  0.4099,  0.1806,  0.2959, -0.1795,  0.2100, -0.2404,\n",
      "         -0.0934,  0.1332],\n",
      "        [ 0.3232,  1.4721, -0.6358,  0.2646,  0.1170, -0.2008, -0.3253,  0.2034,\n",
      "          0.2095, -0.0464],\n",
      "        [-0.2957, -0.1578, -0.1595, -0.1241,  0.2499, -0.1105,  0.2960,  0.2712,\n",
      "          0.2301,  0.2004]])\n",
      "----------------------------------------\n",
      "Layer: layer_1.bias | Shape: torch.Size([16])\n",
      "tensor([-0.1510, -0.0798, -0.2327,  0.2773,  0.0700, -0.0118,  0.0446, -0.2711,\n",
      "         0.1412,  0.4199, -0.1566, -0.0960,  0.2994, -0.0810, -0.2351,  0.0401])\n",
      "----------------------------------------\n",
      "Layer: layer_2.weight | Shape: torch.Size([16, 16])\n",
      "tensor([[ 1.0240e-01, -3.6258e-02, -3.2263e-01, -5.5623e-02, -3.1759e-02,\n",
      "         -5.6328e-01,  1.2037e-01,  1.2692e-01, -2.0712e-01, -5.9217e-02,\n",
      "         -1.3167e-02, -2.1495e-01,  2.1998e-01, -2.1919e-04, -3.3266e-01,\n",
      "         -2.8121e-01],\n",
      "        [-5.8274e-02,  2.4562e-01, -1.6875e-01, -5.9708e-02,  4.6034e-02,\n",
      "         -3.1991e-03,  1.1114e-01, -2.2641e-01,  1.7476e-01,  3.0331e-02,\n",
      "          2.2104e-01,  1.6473e-01, -5.5429e-02,  1.5476e-01,  1.5550e-01,\n",
      "         -3.2053e-02],\n",
      "        [ 1.8627e-01,  2.8859e-01, -1.4577e-01, -1.6679e-01, -2.8045e-02,\n",
      "         -9.4999e-02,  2.5636e-01, -1.9970e-01, -2.8683e-01,  3.5630e-02,\n",
      "          9.9271e-02,  1.7854e-01, -1.2918e-01,  3.1004e-01, -2.2011e-02,\n",
      "         -1.0126e-01],\n",
      "        [ 2.4146e-01, -1.2504e-02, -2.6008e-01, -3.1216e-01, -1.0897e-01,\n",
      "         -4.0379e-01, -3.5477e-01, -2.2175e-02, -1.7105e-01,  7.2709e-03,\n",
      "         -2.4146e-01,  1.0362e-01, -1.5087e-01, -1.9248e-01, -3.7583e-01,\n",
      "          4.2505e-01],\n",
      "        [ 4.6483e-01, -4.0703e-01, -1.3466e-01, -2.4637e-01, -4.0051e-02,\n",
      "         -6.3347e-01, -2.4046e-02, -1.6569e-02,  5.5659e-02,  1.3231e-02,\n",
      "         -2.7002e-01,  2.5317e-02, -3.8636e-02, -1.7784e-01, -3.2529e-01,\n",
      "          3.2249e-01],\n",
      "        [ 6.4290e-02,  1.7184e-01,  1.7277e-01, -1.3471e-01, -8.9420e-02,\n",
      "         -3.1010e-01, -2.1670e-01,  7.3350e-02,  1.2320e-01, -1.8501e-02,\n",
      "         -2.7592e-01, -1.8474e-01,  9.8576e-02, -1.5681e-01, -2.5365e-01,\n",
      "          1.3728e-01],\n",
      "        [-2.1294e-01,  1.7828e-01,  1.7185e-01,  1.5966e-01,  1.0125e-02,\n",
      "         -1.5356e-02, -8.1655e-02, -3.0311e-01,  1.9002e-01, -1.0056e+00,\n",
      "         -2.1908e-01, -9.6383e-02, -1.1603e-01,  1.0318e-01, -2.1660e-01,\n",
      "         -3.6674e-01],\n",
      "        [ 2.2130e-01,  1.7414e-02, -4.3896e-02, -1.0805e-02,  2.1184e-01,\n",
      "          2.0318e-01, -3.6746e-02,  8.3415e-02, -8.9914e-02,  1.4183e-01,\n",
      "         -1.1891e-01,  1.2934e-01, -9.7934e-02, -1.6230e-01,  3.7553e-02,\n",
      "          2.7534e-01],\n",
      "        [ 4.1358e-02,  1.0578e-02,  1.6372e-01, -3.0984e-01, -4.0318e-01,\n",
      "          3.4056e-02, -1.3866e-01, -4.7991e-01, -2.5183e-01, -3.3067e-01,\n",
      "          8.0102e-02,  1.0840e-01, -1.5160e-01, -5.9885e-02, -8.7956e-02,\n",
      "          3.0672e-01],\n",
      "        [ 3.9051e-01, -4.3909e-02,  9.1092e-02, -7.5487e-02, -2.2132e-01,\n",
      "         -3.4844e-02, -3.1590e-01, -1.8259e-01, -1.2861e-01, -1.2676e-01,\n",
      "          2.0558e-01, -1.5026e-01,  1.3225e-01, -7.2686e-02, -1.1956e-01,\n",
      "          1.1650e-02],\n",
      "        [-1.3567e-01, -2.2763e-01,  1.7026e-01, -1.6529e-02, -1.7910e-01,\n",
      "          9.5310e-03,  4.8528e-02, -4.8733e-01,  1.3960e-01, -6.0955e-01,\n",
      "         -5.8540e-02, -4.6451e-02, -1.3807e-01, -8.9821e-02,  1.4853e-01,\n",
      "          1.2333e-01],\n",
      "        [-1.2555e-01,  2.2781e-01, -9.7867e-02, -7.2710e-02,  1.1609e-01,\n",
      "          9.6908e-02, -3.3088e-02, -1.4962e-01,  6.8384e-02, -1.0619e-01,\n",
      "          2.0978e-01, -1.4107e-01,  1.6027e-01,  1.3626e-01,  1.9111e-01,\n",
      "         -2.5229e-01],\n",
      "        [-7.5693e-02, -1.4453e-01,  3.0116e-03, -2.2844e-01,  2.3851e-01,\n",
      "         -2.1129e-01, -2.1768e-01, -5.5779e-02,  2.3414e-01, -1.0837e-01,\n",
      "         -6.6174e-02,  2.2820e-01, -5.5139e-02, -1.5192e-01, -4.2331e-02,\n",
      "          2.5737e-01],\n",
      "        [ 9.8341e-02, -1.6110e-01,  1.5047e-01,  6.2916e-02, -6.7785e-02,\n",
      "         -1.9462e-02, -3.1863e-01, -9.1105e-02,  2.1959e-01,  8.6356e-02,\n",
      "         -1.4316e-03,  2.1054e-01, -9.9497e-02, -4.2452e-02,  2.2418e-02,\n",
      "          3.0608e-01],\n",
      "        [ 2.9762e-01,  1.9063e-02,  1.8000e-01, -2.4914e-01, -5.0175e-02,\n",
      "         -2.4230e-01, -3.0087e-01, -1.1012e-01, -1.5020e-01,  1.1443e-01,\n",
      "         -1.1385e-01, -1.0953e-01,  2.2889e-01, -1.8879e-01, -9.1002e-02,\n",
      "          3.1819e-01],\n",
      "        [-1.5514e-01,  1.2855e-01, -1.1670e-01, -9.0041e-02, -6.7835e-01,\n",
      "          2.4153e-01,  1.5829e-01, -2.8316e-01, -8.8058e-02,  6.9084e-02,\n",
      "         -2.5728e-01, -4.9713e-02,  2.0580e-01,  9.1893e-02, -6.4307e-02,\n",
      "         -4.0225e-02]])\n",
      "----------------------------------------\n",
      "Layer: layer_2.bias | Shape: torch.Size([16])\n",
      "tensor([-0.1710,  0.1963,  0.0577, -0.1363,  0.0123, -0.1332, -0.0445,  0.0098,\n",
      "        -0.1433, -0.2420, -0.1996,  0.1732, -0.2426, -0.1906, -0.1676, -0.0857])\n",
      "----------------------------------------\n",
      "Layer: layer_out.weight | Shape: torch.Size([1, 16])\n",
      "tensor([[ 0.2415, -0.0359, -0.1792,  0.3087,  0.3433,  0.3111, -0.3761,  0.0686,\n",
      "         -0.2978, -0.2177, -0.3621, -0.0362,  0.0089, -0.0203,  0.3994, -0.1495]])\n",
      "----------------------------------------\n",
      "Layer: layer_out.bias | Shape: torch.Size([1])\n",
      "tensor([0.0168])\n",
      "----------------------------------------\n",
      "Layer: batchnorm1.weight | Shape: torch.Size([16])\n",
      "tensor([1.0000, 0.9582, 0.8787, 0.9557, 1.0291, 0.9441, 1.0000, 1.0442, 0.9783,\n",
      "        1.0974, 0.9902, 0.9710, 0.8886, 0.9386, 0.9922, 1.0000])\n",
      "----------------------------------------\n",
      "Layer: batchnorm1.bias | Shape: torch.Size([16])\n",
      "tensor([-0.1620,  0.0255, -0.0929,  0.2420,  0.2142,  0.1783,  0.1936,  0.1732,\n",
      "         0.1653,  0.0746,  0.0456,  0.1103, -0.0501,  0.2068,  0.1880, -0.1332])\n",
      "----------------------------------------\n",
      "Layer: batchnorm2.weight | Shape: torch.Size([16])\n",
      "tensor([1.2383, 0.8639, 0.9148, 1.1527, 1.1549, 1.1341, 1.1330, 0.8440, 1.1876,\n",
      "        1.1954, 1.1792, 0.8305, 0.8492, 1.0033, 1.1738, 1.1342])\n",
      "----------------------------------------\n",
      "Layer: batchnorm2.bias | Shape: torch.Size([16])\n",
      "tensor([ 0.2077,  0.1036, -0.1764,  0.1688,  0.1652,  0.1963, -0.1768,  0.1443,\n",
      "        -0.1861, -0.1757, -0.1854,  0.1205,  0.1158,  0.1683,  0.1413,  0.0482])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, param in MarketPredictorModel.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Shape: {param.shape}\")\n",
    "        print(param.data)\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "132010cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 731 | Total: 920 | Accuracy: 79.45652173913044%\n"
     ]
    }
   ],
   "source": [
    "timerStamper = pd.read_csv(\"data/silver/timerStamper1.csv\")\n",
    "\n",
    "correct = 0\n",
    "total  =0\n",
    "\n",
    "for market in timerStamper['Market'].tolist():\n",
    "    try:\n",
    "        buyScans = pd.read_csv(f'data/bronze/contract_official_buys_{market}.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 1\")\n",
    "        continue\n",
    "\n",
    "    buyScans75Index = max(buyScans['timeStampSinceFirst'].tolist())*.75\n",
    "    buyScans75 = buyScans[buyScans['timeStampSinceFirst'] < buyScans75Index]\n",
    "\n",
    "\n",
    "    buysIndex0 = buyScans75[buyScans75['outcomeIndex'] == 0]['investmentAmount']\n",
    "    buysIndex1 = buyScans75[buyScans75['outcomeIndex'] == 1]['investmentAmount']\n",
    "\n",
    "    \n",
    "    prediction = sum(buysIndex0)/(sum(buysIndex0) + sum(buysIndex1))\n",
    "    prediction = 0 if prediction >= 0.5 else 1\n",
    "    outcomeIndex = timerStamper[timerStamper['Market'] == market]['OutcomeIndex'].tolist()[0]\n",
    "    \n",
    "    if prediction == outcomeIndex:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    \n",
    "print(f\"Correct: {correct} | Total: {total} | Accuracy: {correct/total*100}%\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6479e622",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainResults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "trainResults.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainData : torch.Size([736, 10])\n",
      "trainResults : torch.Size([736])\n"
     ]
    }
   ],
   "source": [
    "print(\"trainData :\", trainData.shape)\n",
    "print(\"trainResults :\", trainResults.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PredMarkEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
